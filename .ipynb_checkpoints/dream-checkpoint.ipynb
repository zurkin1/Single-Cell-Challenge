{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ResNet Like Model For b to d Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydot\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Flatten, Dropout, Lambda, Activation, BatchNormalization, LocallyConnected1D, Reshape, AlphaDropout, Conv1D, MaxPooling1D\n",
    "from keras.activations import relu\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "num_situ = 20\n",
    "num_all = 8924\n",
    "glist_60 = [3,16,80,77,19,52,53,57,78,68,62,0,75,21,66,26,81,51,63,7,8,56,35,18,83,6,1,61,65,55,74,22,64,20,59,23,79,48,58,31,69,73,76,24,33,17,47,14,25,15,67,42,54,46,50,28,27,49,43,13]\n",
    "glist_60_tom = ['kni','Ance','brk','cad','eve','fkh','hb','hkb','ImpE2','oc','sna','srp','twi','zen','zfh1','Blimp-1','croc','D','Dfd','Doc3','dpn','fj','ftz','gt','h','ken','knrl','Kr','odd','peb','run','tkv','tll','tsh','zen2','Antp','apt','bowl','CG14427','CG17724','CG17786','CG8147','Cyp310a1','dan','disco','Doc2','E(spl)m5-HLH','Ilp4','ImpL2','Mes2','NetA','prd','rau','rho','toc','trn','aay','gk','ems','numb']\n",
    "glist_40_tom = ['kni','Ance','brk','cad','eve','fkh','hb','hkb','ImpE2','oc','sna','srp','twi','zen','zfh1','Blimp-1','croc','D','Dfd','Doc3','dpn','fj','ftz','gt','h','ken','knrl','Kr','odd','peb','run','tkv','tll','tsh','zen2','trn','E(spl)m5-HLH','CG17724','disco','dan']\n",
    "glist_20_tom = ['kni','Ance','brk','cad','eve','fkh','hb','hkb','ImpE2','oc','sna','srp','twi','zen','zfh1','gt','Kr','ftz','tkv','croc']\n",
    "glist = ['danr','CG14427','dan','CG43394','ImpL2','Nek2','CG8147','Ama','Btk29A','trn','numb','prd','brk','tsh','pxb','dpn','ftz','Kr','h','eve','Traf4','run','Blimp-1','lok','kni','tkv','MESR3','odd','noc','nub','Ilp4','aay','twi','bmm','hb','toc','rho','CG10479','gt','gk']\n",
    "glist_20 = ['danr','CG14427','dan','CG43394','ImpL2','Nek2','CG8147','Ama','Btk29A','trn','numb','prd','brk','tsh','pxb','dpn','ftz','Kr','h','eve']\n",
    "glist_84 = ['aay','Ama','Ance','Antp','apt','Blimp-1','bmm','bowl','brk','Btk29A','bun','cad','CenG1A','CG10479','CG11208','CG14427','CG17724','CG17786','CG43394','CG8147','cnc','croc','Cyp310a1','D','dan','danr','Dfd','disco','Doc2','Doc3','dpn','edl','ems','erm','Esp','E(spl)m5-HLH','eve','exex','fj','fkh','ftz','gk','gt','h','hb','hkb','htl','Ilp4','ImpE2','ImpL2','ken','kni','knrl','Kr','lok','Mdr49','Mes2','MESR3','mfas','Nek2','NetA','noc','nub','numb','oc','odd','peb','prd','pxb','rau','rho','run','sna','srp','tkv','tll','toc','Traf4','trn','tsh','twi','zen','zen2','zfh1']\n",
    "glist_40 = ['danr','CG14427','dan','CG43394','ImpL2','Nek2','CG8147','Ama','Btk29A','trn','numb','prd','brk','tsh','pxb','dpn','ftz','Kr','h','eve','Traf4','run','Blimp-1','lok','kni','tkv','MESR3','odd','noc','nub','Ilp4','aay','twi','bmm','hb','toc','rho','CG10479','gt','gk']\n",
    "glist_mcc_20 = ['run','h','noc','Traf4','pxb','aay','Btk29A','trn','odd','CG43394','bun','dpn','nub','CG10479','CG8147','Antp','ImpL2','kni','eve','CG14427']\n",
    "glist_20_mod = ['danr', 'CG14427', 'dan', 'CG43394', 'ImpL2', 'Nek2', 'CG8147', 'Ama', 'Btk29A', 'trn', 'numb', 'prd', 'brk', 'tsh', 'pxb', 'dpn', 'h', 'Traf4', 'run', 'toc']\n",
    "\"\"\"\n",
    "#If running on Amazon\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAJIQ7JIXG4KRICKRA\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "\n",
    "print(time.ctime(), 'Read files...')\n",
    "b = pd.read_csv('s3://daniglassbox/b.csv')\n",
    "d = pd.read_csv('s3://daniglassbox/d.csv', index_col=0, header=None, encoding='ISO-8859-1').T\n",
    "labels = pd.read_csv('s3://daniglassbox/labels.csv', index_col=0, header=None).T\n",
    "\"\"\"\n",
    "\n",
    "def diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]\n",
    "\n",
    "#bdtnp = pd.read_csv('data/bdtnp.csv')\n",
    "bdtnp_bin = pd.read_csv('data/binarized_bdtnp.csv')[glist_20_mod]\n",
    "#cols = list(bdtnp)\n",
    "#cols = glist\n",
    "\n",
    "#meds = d.median()\n",
    "#for col in d:\n",
    "#    d[col] = d[col].apply(lambda x: 0 if x<= meds[col] else 1)\n",
    "#d.to_csv('data/magic_dge_bin.csv')\n",
    "\n",
    "#Changes 'na' to 'naa' and 'nan' to 'nana'\n",
    "#d = pd.read_csv('data/dge_raw.csv', index_col=0, header=None, encoding='ISO-8859-1').T\n",
    "#d = pd.read_csv('data/magic_dge.csv')\n",
    "d1_bin = pd.read_csv('data/dge_binarized_distMap_T.csv')\n",
    "#d2_bin = pd.read_csv('data/magic_dge_bin.csv')[glist_84]\n",
    "labels = pd.read_csv('data/labels.csv')\n",
    "\n",
    "#Move in-situ 84 genes to the begining\n",
    "#cols = list(bdtnp) + list(set(list(d)) - set(list(bdtnp)))\n",
    "#d = d.loc[:,cols]\n",
    "\n",
    "#Scaling\n",
    "#d = d.div(d.sum(axis=1), axis=0)\n",
    "#d.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 13 22:31:34 2018 Create true list of tuples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1693"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the true label pairs. Left: d-array, right: b-array \n",
    "#Labels start from 0 in the original file. They indicate a specific row in b table.\n",
    "print(time.ctime(),'Create true list of tuples')\n",
    "#labels.pkl contains a dictionary mapping of all 1270 cells to (possibly few) locations in [0,3038].\n",
    "pkl_file = open('data/labels.pkl', 'rb')\n",
    "ind_load = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "data_ind = pd.DataFrame(list(ind_load.items()))\n",
    "data_ind.drop([0], axis=1, inplace=True)\n",
    "data_ind[1] = [np.ndarray.flatten(data_ind[1][i]) for i in range(len(data_ind))]\n",
    "#data_ind.head()\n",
    "\n",
    "d_true = []\n",
    "for i in range(len(data_ind)):\n",
    "    for j in range(len(data_ind.iloc[i].iloc[0])):\n",
    "        d_true.append((i,data_ind.iloc[i].iloc[0][j],1))\n",
    "len(d_true) #1693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 13 22:31:44 2018 Create false list of tuples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3939890"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the false label pairs\n",
    "print(time.ctime(), 'Create false list of tuples')\n",
    "\n",
    "\"\"\"\n",
    "a_list = [i for i in range(0,1297)] # d-array\n",
    "b_list = [j for j in range(0,3039)] # b-array\n",
    "d_prod = list(itertools.product(a_list, b_list))\n",
    "d_false = [x+(0,) for x in d_prod if x not in d_true] #Need to remove the ,1 in d_true before running this code.\n",
    "with open('d_false.pkl', 'wb') as f:\n",
    "    pickle.dump(d_false, f)\n",
    "\"\"\"\n",
    "with open('data/d_false.pkl', 'rb') as f:\n",
    "    d_false = pickle.load(f)\n",
    "len(d_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 13 23:20:45 2018 Merging lists\n",
      "Tue Nov 13 23:20:45 2018 len(d_list): 17700\n"
     ]
    }
   ],
   "source": [
    "#Merge the two lists. Select 16003 samples from d_false and 1693 (not 1297 due to multiple max(mcc) values) from d_true for training.\n",
    "print(time.ctime(), 'Merging lists')\n",
    "indicies = random.sample(range(len(d_false)), 16007) #2307\n",
    "d_false1 = [d_false[i] for i in indicies]\n",
    "d_list = d_true + d_false1\n",
    "random.shuffle(d_list)\n",
    "len_list = len(d_list)\n",
    "print(time.ctime(), f'len(d_list): {len_list}') #11300, 17700, 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.ctime(), 'Create train input arrays')\n",
    "X1_train = np.empty((len_list, num_situ)) #Can create a test array using X1_test = np.empty((1300, 84))\n",
    "X2_train = np.empty((len_list, num_situ))\n",
    "X_train = np.empty((len_list, 2, num_situ)) #Trying a convolutional model.\n",
    "X3_train = np.empty((len_list, num_all)) #8864\n",
    "y_train = np.empty((len_list), dtype=int)\n",
    "batch=0\n",
    "\n",
    "\n",
    "for i in d_list[0:len_list]:\n",
    "    if (batch % 100 == 0):\n",
    "        print(batch, ' ', end=\"\")\n",
    "    X1_train[batch] = bdtnp.iloc[i[1]][cols]\n",
    "    X2_train[batch] = d.iloc[i[0]][cols]\n",
    "    X_train[batch] = np.vstack([X1_train[batch],X2_train[batch]])\n",
    "    X3_train[batch] = d.iloc[i[0]]\n",
    "    y_train[batch] = i[2]\n",
    "    batch = batch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 18 20:37:04 2018 Create train input array for dge to bdtnp model\n",
      "0  100  200  300  400  500  600  700  800  900  1000  1100  1200  "
     ]
    }
   ],
   "source": [
    "print(time.ctime(), 'Create train input array for dge to bdtnp model')\n",
    "\n",
    "len_ = len(labels)\n",
    "#X_ = np.empty((len_, num_situ))\n",
    "Z_ = np.empty((len_, 84))\n",
    "#W_ = np.empty((len_, num_all))\n",
    "y_ = np.empty((len_, num_situ))\n",
    "\n",
    "for index, row in labels.iterrows():\n",
    "    if (index % 100 == 0):\n",
    "        print(index, ' ', end=\"\")\n",
    "    #X_[index] = d_bin.iloc[index][glist_20]\n",
    "    Z_[index] = d1_bin.iloc[index] #[diff(glist_84, glist_20)]\n",
    "    #W_[index] = d2_bin.iloc[index]\n",
    "    y_[index] = bdtnp_bin.iloc[int(row[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model build\n",
    "print(time.strftime(\"%H:%M:%S\"), ' Model build')\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weigts = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "def my_accuracy(y_true, y_pred):\n",
    "    cnt=K.sum(class_weigts)    \n",
    "    err=K.sum(K.not_equal(K.argmax(y_pred,axis=-1)*class_weigts,K.argmax(y_true,axis=-1)*class_weigts))\n",
    "    acc=1.0-(err/cnt)\n",
    "    return acc\n",
    "\"\"\"\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "#    with tf.get_default_graph().gradient_override_map({\"Round\": \"Identity\"}):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def matthews_correlation_loss2(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos =  y_pred\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = y_true\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = K.square(tp * tn - fp * fn)\n",
    "    denominator = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "\n",
    "    return 50 - 100 * numerator/(denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def correlation_coefficient_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return 1 - K.square(r)\n",
    "\n",
    "\"\"\"\n",
    "def blockBuild(a_in, b_in):\n",
    "    #First input model\n",
    "    a_in = Dense(num_situ)(a_in)\n",
    "    a_in = AlphaDropout(0.2)(a_in)\n",
    "    a_in = BatchNormalization()(a_in)\n",
    "    a_in = Activation('softplus')(a_in)\n",
    "    \n",
    "    #Second input model\n",
    "    b_in = Dense(num_situ)(b_in)\n",
    "    b_in = AlphaDropout(0.2)(b_in)\n",
    "    b_in = BatchNormalization()(b_in)\n",
    "    b_in = Activation('softplus')(b_in)\n",
    "    #b = LeakyReLU()(b)\n",
    "    \n",
    "    y = concatenate([a_in, b_in], axis=0)\n",
    "    y = Dense(num_situ)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('softplus')(y)\n",
    "    return(y)\n",
    "\n",
    "a = Input(shape=(num_situ,))\n",
    "b = Input(shape=(num_situ,))\n",
    "\n",
    "first_block = blockBuild(a,b)\n",
    "second_block = blockBuild(a,first_block)\n",
    "third_block = blockBuild(second_block,b)\n",
    "\"\"\"\n",
    "\n",
    "a1 = Input(shape=(2,num_situ,))\n",
    "a2 = Conv1D (kernel_size = 2, filters = 8, activation='softplus')(a1)\n",
    "a3 = Flatten()(a2)\n",
    "a4 = Dense(2*num_situ)(a3)\n",
    "a5 = AlphaDropout(0.2)(a4)\n",
    "a6 = BatchNormalization()(a5)\n",
    "a7 = Activation('softplus')(a6)\n",
    "a8 = Dense(num_situ, activation='softplus')(a7)\n",
    "\n",
    "#Third input model\n",
    "c1 = Input(shape=(num_all,))\n",
    "c2 = Dense(num_situ)(c1)\n",
    "c3 = AlphaDropout(0.2)(c2)\n",
    "c4 = BatchNormalization()(c3)\n",
    "c5 = Activation('softplus')(c4)\n",
    "c6 = concatenate([a8,c1]) #third_block\n",
    "c7 = Dense(num_situ)(c6)\n",
    "c8 = BatchNormalization()(c7)\n",
    "c9 = Activation('softplus')(c8)\n",
    "c10 = Dense(10)(c9)\n",
    "c11 = BatchNormalization()(c10)\n",
    "c12 = Activation('softplus')(c11)\n",
    "output = Dense(1, activation='sigmoid')(c12)\n",
    "model = Model(inputs=[a1, c1], outputs=[output])\n",
    "#model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "model.compile(optimizer='adam', loss=matthews_correlation_loss2, metrics=[matthews_correlation]) # metrics=['binary_accuracy'])\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='my_res_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 18 20:56:25 2018 Model build\n",
      "20:56:25  Fit\n",
      "Train on 1037 samples, validate on 260 samples\n",
      "Epoch 1/10000\n",
      " - 1s - loss: 0.6506 - binary_accuracy: 0.6302 - val_loss: 0.5064 - val_binary_accuracy: 0.7667\n",
      "Epoch 2/10000\n",
      " - 0s - loss: 0.5370 - binary_accuracy: 0.7279 - val_loss: 0.4568 - val_binary_accuracy: 0.8017\n",
      "Epoch 3/10000\n",
      " - 0s - loss: 0.5032 - binary_accuracy: 0.7521 - val_loss: 0.4309 - val_binary_accuracy: 0.8104\n",
      "Epoch 4/10000\n",
      " - 0s - loss: 0.4826 - binary_accuracy: 0.7662 - val_loss: 0.4095 - val_binary_accuracy: 0.8248\n",
      "Epoch 5/10000\n",
      " - 0s - loss: 0.4578 - binary_accuracy: 0.7854 - val_loss: 0.3913 - val_binary_accuracy: 0.8310\n",
      "Epoch 6/10000\n",
      " - 0s - loss: 0.4475 - binary_accuracy: 0.7888 - val_loss: 0.3802 - val_binary_accuracy: 0.8385\n",
      "Epoch 7/10000\n",
      " - 0s - loss: 0.4399 - binary_accuracy: 0.7931 - val_loss: 0.3694 - val_binary_accuracy: 0.8456\n",
      "Epoch 8/10000\n",
      " - 0s - loss: 0.4333 - binary_accuracy: 0.7986 - val_loss: 0.3588 - val_binary_accuracy: 0.8479\n",
      "Epoch 9/10000\n",
      " - 0s - loss: 0.4294 - binary_accuracy: 0.8005 - val_loss: 0.3531 - val_binary_accuracy: 0.8535\n",
      "Epoch 10/10000\n",
      " - 0s - loss: 0.4192 - binary_accuracy: 0.8051 - val_loss: 0.3464 - val_binary_accuracy: 0.8519\n",
      "Epoch 11/10000\n",
      " - 0s - loss: 0.4134 - binary_accuracy: 0.8059 - val_loss: 0.3416 - val_binary_accuracy: 0.8565\n",
      "Epoch 12/10000\n",
      " - 0s - loss: 0.4166 - binary_accuracy: 0.8067 - val_loss: 0.3411 - val_binary_accuracy: 0.8550\n",
      "Epoch 13/10000\n",
      " - 0s - loss: 0.4110 - binary_accuracy: 0.8138 - val_loss: 0.3360 - val_binary_accuracy: 0.8600\n",
      "Epoch 14/10000\n",
      " - 0s - loss: 0.4100 - binary_accuracy: 0.8088 - val_loss: 0.3326 - val_binary_accuracy: 0.8596\n",
      "Epoch 15/10000\n",
      " - 0s - loss: 0.3986 - binary_accuracy: 0.8176 - val_loss: 0.3301 - val_binary_accuracy: 0.8604\n",
      "Epoch 16/10000\n",
      " - 0s - loss: 0.3989 - binary_accuracy: 0.8203 - val_loss: 0.3271 - val_binary_accuracy: 0.8608\n",
      "Epoch 17/10000\n",
      " - 0s - loss: 0.3978 - binary_accuracy: 0.8196 - val_loss: 0.3259 - val_binary_accuracy: 0.8598\n",
      "Epoch 18/10000\n",
      " - 0s - loss: 0.4032 - binary_accuracy: 0.8125 - val_loss: 0.3231 - val_binary_accuracy: 0.8602\n",
      "Epoch 19/10000\n",
      " - 0s - loss: 0.3912 - binary_accuracy: 0.8229 - val_loss: 0.3246 - val_binary_accuracy: 0.8596\n",
      "Epoch 20/10000\n",
      " - 0s - loss: 0.3864 - binary_accuracy: 0.8254 - val_loss: 0.3241 - val_binary_accuracy: 0.8602\n",
      "Epoch 21/10000\n",
      " - 0s - loss: 0.3931 - binary_accuracy: 0.8184 - val_loss: 0.3228 - val_binary_accuracy: 0.8629\n",
      "Epoch 22/10000\n",
      " - 0s - loss: 0.3940 - binary_accuracy: 0.8207 - val_loss: 0.3216 - val_binary_accuracy: 0.8638\n",
      "Epoch 23/10000\n",
      " - 0s - loss: 0.3841 - binary_accuracy: 0.8240 - val_loss: 0.3202 - val_binary_accuracy: 0.8627\n",
      "Epoch 24/10000\n",
      " - 0s - loss: 0.3911 - binary_accuracy: 0.8214 - val_loss: 0.3193 - val_binary_accuracy: 0.8637\n",
      "Epoch 25/10000\n",
      " - 0s - loss: 0.3856 - binary_accuracy: 0.8245 - val_loss: 0.3170 - val_binary_accuracy: 0.8644\n",
      "Epoch 26/10000\n",
      " - 0s - loss: 0.3899 - binary_accuracy: 0.8221 - val_loss: 0.3172 - val_binary_accuracy: 0.8648\n",
      "Epoch 27/10000\n",
      " - 0s - loss: 0.3792 - binary_accuracy: 0.8264 - val_loss: 0.3163 - val_binary_accuracy: 0.8644\n",
      "Epoch 28/10000\n",
      " - 0s - loss: 0.3792 - binary_accuracy: 0.8266 - val_loss: 0.3157 - val_binary_accuracy: 0.8633\n",
      "Epoch 29/10000\n",
      " - 0s - loss: 0.3856 - binary_accuracy: 0.8248 - val_loss: 0.3159 - val_binary_accuracy: 0.8635\n",
      "Epoch 30/10000\n",
      " - 0s - loss: 0.3830 - binary_accuracy: 0.8251 - val_loss: 0.3141 - val_binary_accuracy: 0.8673\n",
      "Epoch 31/10000\n",
      " - 0s - loss: 0.3826 - binary_accuracy: 0.8257 - val_loss: 0.3127 - val_binary_accuracy: 0.8660\n",
      "Epoch 32/10000\n",
      " - 0s - loss: 0.3765 - binary_accuracy: 0.8284 - val_loss: 0.3138 - val_binary_accuracy: 0.8671\n",
      "Epoch 33/10000\n",
      " - 0s - loss: 0.3832 - binary_accuracy: 0.8259 - val_loss: 0.3113 - val_binary_accuracy: 0.8652\n",
      "Epoch 34/10000\n",
      " - 0s - loss: 0.3715 - binary_accuracy: 0.8324 - val_loss: 0.3132 - val_binary_accuracy: 0.8629\n",
      "Epoch 35/10000\n",
      " - 0s - loss: 0.3765 - binary_accuracy: 0.8283 - val_loss: 0.3127 - val_binary_accuracy: 0.8625\n",
      "Epoch 36/10000\n",
      " - 0s - loss: 0.3896 - binary_accuracy: 0.8243 - val_loss: 0.3122 - val_binary_accuracy: 0.8656\n",
      "Epoch 37/10000\n",
      " - 0s - loss: 0.3753 - binary_accuracy: 0.8310 - val_loss: 0.3110 - val_binary_accuracy: 0.8677\n",
      "Epoch 38/10000\n",
      " - 0s - loss: 0.3733 - binary_accuracy: 0.8272 - val_loss: 0.3111 - val_binary_accuracy: 0.8656\n",
      "Epoch 39/10000\n",
      " - 0s - loss: 0.3784 - binary_accuracy: 0.8297 - val_loss: 0.3131 - val_binary_accuracy: 0.8675\n",
      "Epoch 40/10000\n",
      " - 0s - loss: 0.3714 - binary_accuracy: 0.8308 - val_loss: 0.3134 - val_binary_accuracy: 0.8671\n",
      "Epoch 41/10000\n",
      " - 0s - loss: 0.3630 - binary_accuracy: 0.8339 - val_loss: 0.3119 - val_binary_accuracy: 0.8635\n",
      "Epoch 42/10000\n",
      " - 0s - loss: 0.3732 - binary_accuracy: 0.8326 - val_loss: 0.3126 - val_binary_accuracy: 0.8637\n",
      "Epoch 43/10000\n",
      " - 0s - loss: 0.3743 - binary_accuracy: 0.8347 - val_loss: 0.3111 - val_binary_accuracy: 0.8637\n",
      "Epoch 44/10000\n",
      " - 0s - loss: 0.3732 - binary_accuracy: 0.8306 - val_loss: 0.3106 - val_binary_accuracy: 0.8635\n",
      "Epoch 45/10000\n",
      " - 0s - loss: 0.3771 - binary_accuracy: 0.8303 - val_loss: 0.3114 - val_binary_accuracy: 0.8635\n",
      "Epoch 46/10000\n",
      " - 0s - loss: 0.3763 - binary_accuracy: 0.8310 - val_loss: 0.3107 - val_binary_accuracy: 0.8660\n",
      "Epoch 47/10000\n",
      " - 0s - loss: 0.3650 - binary_accuracy: 0.8352 - val_loss: 0.3113 - val_binary_accuracy: 0.8648\n",
      "Epoch 48/10000\n",
      " - 0s - loss: 0.3638 - binary_accuracy: 0.8382 - val_loss: 0.3103 - val_binary_accuracy: 0.8644\n",
      "Epoch 49/10000\n",
      " - 0s - loss: 0.3650 - binary_accuracy: 0.8377 - val_loss: 0.3095 - val_binary_accuracy: 0.8681\n",
      "Epoch 50/10000\n",
      " - 0s - loss: 0.3700 - binary_accuracy: 0.8334 - val_loss: 0.3086 - val_binary_accuracy: 0.8683\n",
      "Epoch 51/10000\n",
      " - 0s - loss: 0.3657 - binary_accuracy: 0.8355 - val_loss: 0.3085 - val_binary_accuracy: 0.8662\n",
      "Epoch 52/10000\n",
      " - 0s - loss: 0.3694 - binary_accuracy: 0.8332 - val_loss: 0.3070 - val_binary_accuracy: 0.8675\n",
      "Epoch 53/10000\n",
      " - 0s - loss: 0.3570 - binary_accuracy: 0.8399 - val_loss: 0.3080 - val_binary_accuracy: 0.8662\n",
      "Epoch 54/10000\n",
      " - 0s - loss: 0.3659 - binary_accuracy: 0.8361 - val_loss: 0.3085 - val_binary_accuracy: 0.8679\n",
      "Epoch 55/10000\n",
      " - 0s - loss: 0.3598 - binary_accuracy: 0.8389 - val_loss: 0.3081 - val_binary_accuracy: 0.8656\n",
      "Epoch 56/10000\n",
      " - 0s - loss: 0.3614 - binary_accuracy: 0.8351 - val_loss: 0.3055 - val_binary_accuracy: 0.8667\n",
      "Epoch 57/10000\n",
      " - 0s - loss: 0.3714 - binary_accuracy: 0.8322 - val_loss: 0.3065 - val_binary_accuracy: 0.8667\n",
      "Epoch 58/10000\n",
      " - 0s - loss: 0.3589 - binary_accuracy: 0.8383 - val_loss: 0.3063 - val_binary_accuracy: 0.8669\n",
      "Epoch 59/10000\n",
      " - 0s - loss: 0.3654 - binary_accuracy: 0.8345 - val_loss: 0.3057 - val_binary_accuracy: 0.8679\n",
      "Epoch 60/10000\n",
      " - 0s - loss: 0.3635 - binary_accuracy: 0.8399 - val_loss: 0.3055 - val_binary_accuracy: 0.8690\n",
      "Epoch 61/10000\n",
      " - 0s - loss: 0.3627 - binary_accuracy: 0.8369 - val_loss: 0.3046 - val_binary_accuracy: 0.8677\n",
      "Epoch 62/10000\n",
      " - 0s - loss: 0.3654 - binary_accuracy: 0.8370 - val_loss: 0.3083 - val_binary_accuracy: 0.8688\n",
      "Epoch 63/10000\n",
      " - 0s - loss: 0.3633 - binary_accuracy: 0.8334 - val_loss: 0.3068 - val_binary_accuracy: 0.8669\n",
      "Epoch 64/10000\n",
      " - 0s - loss: 0.3712 - binary_accuracy: 0.8341 - val_loss: 0.3043 - val_binary_accuracy: 0.8687\n",
      "Epoch 65/10000\n",
      " - 0s - loss: 0.3606 - binary_accuracy: 0.8375 - val_loss: 0.3047 - val_binary_accuracy: 0.8692\n",
      "Epoch 66/10000\n",
      " - 0s - loss: 0.3702 - binary_accuracy: 0.8347 - val_loss: 0.3058 - val_binary_accuracy: 0.8665\n",
      "Epoch 67/10000\n",
      " - 0s - loss: 0.3561 - binary_accuracy: 0.8397 - val_loss: 0.3051 - val_binary_accuracy: 0.8675\n",
      "Epoch 68/10000\n",
      " - 0s - loss: 0.3573 - binary_accuracy: 0.8397 - val_loss: 0.3043 - val_binary_accuracy: 0.8677\n",
      "Epoch 69/10000\n",
      " - 0s - loss: 0.3704 - binary_accuracy: 0.8347 - val_loss: 0.3070 - val_binary_accuracy: 0.8660\n",
      "Epoch 70/10000\n",
      " - 0s - loss: 0.3589 - binary_accuracy: 0.8420 - val_loss: 0.3064 - val_binary_accuracy: 0.8662\n",
      "Epoch 71/10000\n",
      " - 0s - loss: 0.3663 - binary_accuracy: 0.8351 - val_loss: 0.3052 - val_binary_accuracy: 0.8688\n",
      "Epoch 72/10000\n",
      " - 0s - loss: 0.3614 - binary_accuracy: 0.8358 - val_loss: 0.3043 - val_binary_accuracy: 0.8685\n",
      "Epoch 73/10000\n",
      " - 0s - loss: 0.3720 - binary_accuracy: 0.8343 - val_loss: 0.3071 - val_binary_accuracy: 0.8660\n",
      "Epoch 74/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.3617 - binary_accuracy: 0.8388 - val_loss: 0.3043 - val_binary_accuracy: 0.8681\n",
      "Epoch 75/10000\n",
      " - 0s - loss: 0.3583 - binary_accuracy: 0.8413 - val_loss: 0.3055 - val_binary_accuracy: 0.8677\n",
      "Epoch 76/10000\n",
      " - 0s - loss: 0.3660 - binary_accuracy: 0.8352 - val_loss: 0.3050 - val_binary_accuracy: 0.8656\n",
      "Epoch 77/10000\n",
      " - 0s - loss: 0.3637 - binary_accuracy: 0.8342 - val_loss: 0.3042 - val_binary_accuracy: 0.8646\n",
      "Epoch 78/10000\n",
      " - 0s - loss: 0.3532 - binary_accuracy: 0.8435 - val_loss: 0.3064 - val_binary_accuracy: 0.8652\n",
      "Epoch 79/10000\n",
      " - 0s - loss: 0.3610 - binary_accuracy: 0.8392 - val_loss: 0.3071 - val_binary_accuracy: 0.8662\n",
      "Epoch 80/10000\n",
      " - 0s - loss: 0.3547 - binary_accuracy: 0.8450 - val_loss: 0.3036 - val_binary_accuracy: 0.8687\n",
      "Epoch 81/10000\n",
      " - 0s - loss: 0.3595 - binary_accuracy: 0.8370 - val_loss: 0.3029 - val_binary_accuracy: 0.8681\n",
      "Epoch 82/10000\n",
      " - 0s - loss: 0.3590 - binary_accuracy: 0.8381 - val_loss: 0.3034 - val_binary_accuracy: 0.8665\n",
      "Epoch 83/10000\n",
      " - 0s - loss: 0.3564 - binary_accuracy: 0.8426 - val_loss: 0.3038 - val_binary_accuracy: 0.8687\n",
      "Epoch 84/10000\n",
      " - 0s - loss: 0.3498 - binary_accuracy: 0.8449 - val_loss: 0.3029 - val_binary_accuracy: 0.8677\n",
      "Epoch 85/10000\n",
      " - 0s - loss: 0.3554 - binary_accuracy: 0.8409 - val_loss: 0.3026 - val_binary_accuracy: 0.8669\n",
      "Epoch 86/10000\n",
      " - 0s - loss: 0.3566 - binary_accuracy: 0.8395 - val_loss: 0.3037 - val_binary_accuracy: 0.8685\n",
      "Epoch 87/10000\n",
      " - 0s - loss: 0.3534 - binary_accuracy: 0.8438 - val_loss: 0.3048 - val_binary_accuracy: 0.8646\n",
      "Epoch 88/10000\n",
      " - 0s - loss: 0.3561 - binary_accuracy: 0.8401 - val_loss: 0.3062 - val_binary_accuracy: 0.8673\n",
      "Epoch 89/10000\n",
      " - 0s - loss: 0.3538 - binary_accuracy: 0.8422 - val_loss: 0.3052 - val_binary_accuracy: 0.8673\n",
      "Epoch 90/10000\n",
      " - 0s - loss: 0.3521 - binary_accuracy: 0.8430 - val_loss: 0.3030 - val_binary_accuracy: 0.8687\n",
      "Epoch 91/10000\n",
      " - 0s - loss: 0.3558 - binary_accuracy: 0.8419 - val_loss: 0.3028 - val_binary_accuracy: 0.8665\n",
      "Epoch 92/10000\n",
      " - 0s - loss: 0.3523 - binary_accuracy: 0.8405 - val_loss: 0.3022 - val_binary_accuracy: 0.8665\n",
      "Epoch 93/10000\n",
      " - 0s - loss: 0.3544 - binary_accuracy: 0.8408 - val_loss: 0.3021 - val_binary_accuracy: 0.8662\n",
      "Epoch 94/10000\n",
      " - 0s - loss: 0.3549 - binary_accuracy: 0.8424 - val_loss: 0.3027 - val_binary_accuracy: 0.8679\n",
      "Epoch 95/10000\n",
      " - 0s - loss: 0.3468 - binary_accuracy: 0.8452 - val_loss: 0.3010 - val_binary_accuracy: 0.8688\n",
      "Epoch 96/10000\n",
      " - 0s - loss: 0.3592 - binary_accuracy: 0.8392 - val_loss: 0.3018 - val_binary_accuracy: 0.8687\n",
      "Epoch 97/10000\n",
      " - 0s - loss: 0.3486 - binary_accuracy: 0.8452 - val_loss: 0.3009 - val_binary_accuracy: 0.8694\n",
      "Epoch 98/10000\n",
      " - 0s - loss: 0.3463 - binary_accuracy: 0.8456 - val_loss: 0.3022 - val_binary_accuracy: 0.8677\n",
      "Epoch 99/10000\n",
      " - 0s - loss: 0.3504 - binary_accuracy: 0.8433 - val_loss: 0.3002 - val_binary_accuracy: 0.8696\n",
      "Epoch 100/10000\n",
      " - 0s - loss: 0.3571 - binary_accuracy: 0.8401 - val_loss: 0.3031 - val_binary_accuracy: 0.8696\n",
      "Epoch 101/10000\n",
      " - 0s - loss: 0.3519 - binary_accuracy: 0.8418 - val_loss: 0.3032 - val_binary_accuracy: 0.8713\n",
      "Epoch 102/10000\n",
      " - 0s - loss: 0.3568 - binary_accuracy: 0.8421 - val_loss: 0.3036 - val_binary_accuracy: 0.8654\n",
      "Epoch 103/10000\n",
      " - 0s - loss: 0.3527 - binary_accuracy: 0.8464 - val_loss: 0.3039 - val_binary_accuracy: 0.8667\n",
      "Epoch 104/10000\n",
      " - 0s - loss: 0.3388 - binary_accuracy: 0.8506 - val_loss: 0.3027 - val_binary_accuracy: 0.8687\n",
      "Epoch 105/10000\n",
      " - 0s - loss: 0.3451 - binary_accuracy: 0.8457 - val_loss: 0.3022 - val_binary_accuracy: 0.8673\n",
      "Epoch 106/10000\n",
      " - 0s - loss: 0.3592 - binary_accuracy: 0.8402 - val_loss: 0.3032 - val_binary_accuracy: 0.8673\n",
      "Epoch 107/10000\n",
      " - 0s - loss: 0.3458 - binary_accuracy: 0.8474 - val_loss: 0.3020 - val_binary_accuracy: 0.8667\n",
      "Epoch 108/10000\n",
      " - 0s - loss: 0.3545 - binary_accuracy: 0.8421 - val_loss: 0.3015 - val_binary_accuracy: 0.8662\n",
      "Epoch 109/10000\n",
      " - 0s - loss: 0.3537 - binary_accuracy: 0.8420 - val_loss: 0.3032 - val_binary_accuracy: 0.8656\n",
      "Epoch 110/10000\n",
      " - 0s - loss: 0.3554 - binary_accuracy: 0.8432 - val_loss: 0.3016 - val_binary_accuracy: 0.8665\n",
      "Epoch 111/10000\n",
      " - 0s - loss: 0.3476 - binary_accuracy: 0.8444 - val_loss: 0.3018 - val_binary_accuracy: 0.8692\n",
      "Epoch 112/10000\n",
      " - 0s - loss: 0.3435 - binary_accuracy: 0.8483 - val_loss: 0.3019 - val_binary_accuracy: 0.8658\n",
      "Epoch 113/10000\n",
      " - 0s - loss: 0.3537 - binary_accuracy: 0.8424 - val_loss: 0.3020 - val_binary_accuracy: 0.8681\n",
      "Epoch 114/10000\n",
      " - 0s - loss: 0.3476 - binary_accuracy: 0.8454 - val_loss: 0.3001 - val_binary_accuracy: 0.8683\n",
      "Epoch 115/10000\n",
      " - 0s - loss: 0.3482 - binary_accuracy: 0.8466 - val_loss: 0.3011 - val_binary_accuracy: 0.8667\n",
      "Epoch 116/10000\n",
      " - 0s - loss: 0.3572 - binary_accuracy: 0.8418 - val_loss: 0.3009 - val_binary_accuracy: 0.8665\n",
      "Epoch 117/10000\n",
      " - 0s - loss: 0.3494 - binary_accuracy: 0.8447 - val_loss: 0.3010 - val_binary_accuracy: 0.8681\n",
      "Epoch 118/10000\n",
      " - 0s - loss: 0.3475 - binary_accuracy: 0.8454 - val_loss: 0.3000 - val_binary_accuracy: 0.8698\n",
      "Epoch 119/10000\n",
      " - 0s - loss: 0.3441 - binary_accuracy: 0.8465 - val_loss: 0.3004 - val_binary_accuracy: 0.8681\n",
      "Epoch 120/10000\n",
      " - 0s - loss: 0.3459 - binary_accuracy: 0.8467 - val_loss: 0.3015 - val_binary_accuracy: 0.8669\n",
      "Epoch 121/10000\n",
      " - 0s - loss: 0.3440 - binary_accuracy: 0.8464 - val_loss: 0.3005 - val_binary_accuracy: 0.8696\n",
      "Epoch 122/10000\n",
      " - 0s - loss: 0.3434 - binary_accuracy: 0.8488 - val_loss: 0.3043 - val_binary_accuracy: 0.8673\n",
      "Epoch 123/10000\n",
      " - 0s - loss: 0.3502 - binary_accuracy: 0.8456 - val_loss: 0.3042 - val_binary_accuracy: 0.8685\n",
      "Epoch 124/10000\n",
      " - 0s - loss: 0.3502 - binary_accuracy: 0.8429 - val_loss: 0.3031 - val_binary_accuracy: 0.8679\n",
      "Epoch 125/10000\n",
      " - 0s - loss: 0.3526 - binary_accuracy: 0.8430 - val_loss: 0.3039 - val_binary_accuracy: 0.8662\n",
      "Epoch 126/10000\n",
      " - 0s - loss: 0.3550 - binary_accuracy: 0.8425 - val_loss: 0.3027 - val_binary_accuracy: 0.8669\n",
      "Epoch 127/10000\n",
      " - 0s - loss: 0.3475 - binary_accuracy: 0.8437 - val_loss: 0.3016 - val_binary_accuracy: 0.8687\n",
      "Epoch 128/10000\n",
      " - 0s - loss: 0.3500 - binary_accuracy: 0.8464 - val_loss: 0.3020 - val_binary_accuracy: 0.8690\n",
      "Epoch 129/10000\n",
      " - 0s - loss: 0.3476 - binary_accuracy: 0.8485 - val_loss: 0.3031 - val_binary_accuracy: 0.8673\n",
      "Epoch 130/10000\n",
      " - 0s - loss: 0.3469 - binary_accuracy: 0.8464 - val_loss: 0.3034 - val_binary_accuracy: 0.8671\n",
      "Epoch 131/10000\n",
      " - 0s - loss: 0.3486 - binary_accuracy: 0.8456 - val_loss: 0.3037 - val_binary_accuracy: 0.8677\n",
      "Epoch 132/10000\n",
      " - 0s - loss: 0.3445 - binary_accuracy: 0.8464 - val_loss: 0.3029 - val_binary_accuracy: 0.8663\n",
      "Epoch 133/10000\n",
      " - 0s - loss: 0.3498 - binary_accuracy: 0.8454 - val_loss: 0.3039 - val_binary_accuracy: 0.8652\n",
      "Epoch 134/10000\n",
      " - 0s - loss: 0.3478 - binary_accuracy: 0.8438 - val_loss: 0.3051 - val_binary_accuracy: 0.8642\n",
      "Epoch 135/10000\n",
      " - 0s - loss: 0.3486 - binary_accuracy: 0.8450 - val_loss: 0.3055 - val_binary_accuracy: 0.8671\n",
      "Epoch 136/10000\n",
      " - 0s - loss: 0.3378 - binary_accuracy: 0.8490 - val_loss: 0.3041 - val_binary_accuracy: 0.8679\n",
      "Epoch 137/10000\n",
      " - 0s - loss: 0.3492 - binary_accuracy: 0.8461 - val_loss: 0.3020 - val_binary_accuracy: 0.8658\n",
      "Epoch 138/10000\n",
      " - 0s - loss: 0.3453 - binary_accuracy: 0.8457 - val_loss: 0.3038 - val_binary_accuracy: 0.8658\n",
      "Epoch 139/10000\n",
      " - 0s - loss: 0.3529 - binary_accuracy: 0.8462 - val_loss: 0.3035 - val_binary_accuracy: 0.8658\n",
      "Epoch 140/10000\n",
      " - 0s - loss: 0.3527 - binary_accuracy: 0.8431 - val_loss: 0.3026 - val_binary_accuracy: 0.8663\n",
      "Epoch 141/10000\n",
      " - 0s - loss: 0.3437 - binary_accuracy: 0.8481 - val_loss: 0.3039 - val_binary_accuracy: 0.8683\n",
      "Epoch 142/10000\n",
      " - 0s - loss: 0.3467 - binary_accuracy: 0.8449 - val_loss: 0.3023 - val_binary_accuracy: 0.8679\n",
      "Epoch 143/10000\n",
      " - 0s - loss: 0.3442 - binary_accuracy: 0.8455 - val_loss: 0.3031 - val_binary_accuracy: 0.8671\n",
      "Epoch 144/10000\n",
      " - 0s - loss: 0.3405 - binary_accuracy: 0.8498 - val_loss: 0.3051 - val_binary_accuracy: 0.8669\n",
      "Epoch 145/10000\n",
      " - 0s - loss: 0.3449 - binary_accuracy: 0.8483 - val_loss: 0.3041 - val_binary_accuracy: 0.8677\n",
      "Epoch 146/10000\n",
      " - 0s - loss: 0.3474 - binary_accuracy: 0.8441 - val_loss: 0.3042 - val_binary_accuracy: 0.8663\n",
      "Epoch 147/10000\n",
      " - 0s - loss: 0.3388 - binary_accuracy: 0.8488 - val_loss: 0.3027 - val_binary_accuracy: 0.8675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/10000\n",
      " - 0s - loss: 0.3446 - binary_accuracy: 0.8463 - val_loss: 0.3025 - val_binary_accuracy: 0.8662\n",
      "Epoch 149/10000\n",
      " - 0s - loss: 0.3490 - binary_accuracy: 0.8453 - val_loss: 0.3011 - val_binary_accuracy: 0.8667\n",
      "Epoch 150/10000\n",
      " - 0s - loss: 0.3543 - binary_accuracy: 0.8439 - val_loss: 0.3024 - val_binary_accuracy: 0.8663\n",
      "Epoch 151/10000\n",
      " - 0s - loss: 0.3448 - binary_accuracy: 0.8496 - val_loss: 0.3029 - val_binary_accuracy: 0.8671\n",
      "Epoch 152/10000\n",
      " - 0s - loss: 0.3510 - binary_accuracy: 0.8439 - val_loss: 0.3027 - val_binary_accuracy: 0.8667\n",
      "Epoch 153/10000\n",
      " - 0s - loss: 0.3505 - binary_accuracy: 0.8435 - val_loss: 0.3023 - val_binary_accuracy: 0.8669\n",
      "Epoch 154/10000\n",
      " - 0s - loss: 0.3408 - binary_accuracy: 0.8508 - val_loss: 0.3032 - val_binary_accuracy: 0.8660\n",
      "Epoch 155/10000\n",
      " - 0s - loss: 0.3455 - binary_accuracy: 0.8459 - val_loss: 0.3041 - val_binary_accuracy: 0.8667\n",
      "Epoch 156/10000\n",
      " - 0s - loss: 0.3393 - binary_accuracy: 0.8524 - val_loss: 0.3021 - val_binary_accuracy: 0.8662\n",
      "Epoch 157/10000\n",
      " - 0s - loss: 0.3372 - binary_accuracy: 0.8532 - val_loss: 0.3023 - val_binary_accuracy: 0.8633\n",
      "Epoch 158/10000\n",
      " - 0s - loss: 0.3343 - binary_accuracy: 0.8491 - val_loss: 0.3017 - val_binary_accuracy: 0.8685\n",
      "Epoch 159/10000\n",
      " - 0s - loss: 0.3504 - binary_accuracy: 0.8465 - val_loss: 0.3020 - val_binary_accuracy: 0.8642\n",
      "Epoch 160/10000\n",
      " - 0s - loss: 0.3487 - binary_accuracy: 0.8477 - val_loss: 0.3031 - val_binary_accuracy: 0.8669\n",
      "Epoch 161/10000\n",
      " - 0s - loss: 0.3397 - binary_accuracy: 0.8492 - val_loss: 0.3028 - val_binary_accuracy: 0.8683\n",
      "Epoch 162/10000\n",
      " - 0s - loss: 0.3466 - binary_accuracy: 0.8452 - val_loss: 0.3022 - val_binary_accuracy: 0.8687\n",
      "Epoch 163/10000\n",
      " - 0s - loss: 0.3547 - binary_accuracy: 0.8385 - val_loss: 0.3024 - val_binary_accuracy: 0.8679\n",
      "Epoch 164/10000\n",
      " - 0s - loss: 0.3404 - binary_accuracy: 0.8482 - val_loss: 0.3018 - val_binary_accuracy: 0.8673\n",
      "Epoch 165/10000\n",
      " - 0s - loss: 0.3459 - binary_accuracy: 0.8476 - val_loss: 0.3020 - val_binary_accuracy: 0.8663\n",
      "Epoch 166/10000\n",
      " - 0s - loss: 0.3485 - binary_accuracy: 0.8438 - val_loss: 0.3037 - val_binary_accuracy: 0.8685\n",
      "Epoch 167/10000\n",
      " - 0s - loss: 0.3307 - binary_accuracy: 0.8533 - val_loss: 0.3025 - val_binary_accuracy: 0.8681\n",
      "Epoch 168/10000\n",
      " - 0s - loss: 0.3475 - binary_accuracy: 0.8482 - val_loss: 0.3024 - val_binary_accuracy: 0.8669\n",
      "Epoch 169/10000\n",
      " - 0s - loss: 0.3440 - binary_accuracy: 0.8470 - val_loss: 0.3019 - val_binary_accuracy: 0.8669\n",
      "Epoch 170/10000\n",
      " - 0s - loss: 0.3477 - binary_accuracy: 0.8486 - val_loss: 0.3021 - val_binary_accuracy: 0.8681\n",
      "Epoch 171/10000\n",
      " - 0s - loss: 0.3455 - binary_accuracy: 0.8468 - val_loss: 0.3029 - val_binary_accuracy: 0.8671\n",
      "Epoch 172/10000\n",
      " - 0s - loss: 0.3552 - binary_accuracy: 0.8413 - val_loss: 0.3031 - val_binary_accuracy: 0.8673\n",
      "Epoch 173/10000\n",
      " - 0s - loss: 0.3477 - binary_accuracy: 0.8463 - val_loss: 0.3013 - val_binary_accuracy: 0.8683\n",
      "Epoch 174/10000\n",
      " - 0s - loss: 0.3363 - binary_accuracy: 0.8504 - val_loss: 0.3018 - val_binary_accuracy: 0.8662\n",
      "Epoch 175/10000\n",
      " - 0s - loss: 0.3411 - binary_accuracy: 0.8509 - val_loss: 0.3014 - val_binary_accuracy: 0.8677\n",
      "Epoch 176/10000\n",
      " - 0s - loss: 0.3391 - binary_accuracy: 0.8491 - val_loss: 0.3019 - val_binary_accuracy: 0.8694\n",
      "Epoch 177/10000\n",
      " - 0s - loss: 0.3455 - binary_accuracy: 0.8447 - val_loss: 0.3018 - val_binary_accuracy: 0.8683\n",
      "Epoch 178/10000\n",
      " - 0s - loss: 0.3491 - binary_accuracy: 0.8447 - val_loss: 0.3029 - val_binary_accuracy: 0.8660\n",
      "Epoch 179/10000\n",
      " - 0s - loss: 0.3406 - binary_accuracy: 0.8511 - val_loss: 0.3016 - val_binary_accuracy: 0.8667\n",
      "Epoch 180/10000\n",
      " - 0s - loss: 0.3401 - binary_accuracy: 0.8537 - val_loss: 0.3022 - val_binary_accuracy: 0.8669\n",
      "Epoch 181/10000\n",
      " - 0s - loss: 0.3403 - binary_accuracy: 0.8493 - val_loss: 0.3026 - val_binary_accuracy: 0.8667\n",
      "Epoch 182/10000\n",
      " - 0s - loss: 0.3407 - binary_accuracy: 0.8484 - val_loss: 0.3026 - val_binary_accuracy: 0.8671\n",
      "Epoch 183/10000\n",
      " - 0s - loss: 0.3441 - binary_accuracy: 0.8450 - val_loss: 0.3042 - val_binary_accuracy: 0.8669\n",
      "Epoch 184/10000\n",
      " - 0s - loss: 0.3386 - binary_accuracy: 0.8501 - val_loss: 0.3032 - val_binary_accuracy: 0.8652\n",
      "Epoch 185/10000\n",
      " - 0s - loss: 0.3336 - binary_accuracy: 0.8505 - val_loss: 0.3018 - val_binary_accuracy: 0.8667\n",
      "Epoch 186/10000\n",
      " - 0s - loss: 0.3369 - binary_accuracy: 0.8488 - val_loss: 0.3034 - val_binary_accuracy: 0.8660\n",
      "Epoch 187/10000\n",
      " - 0s - loss: 0.3439 - binary_accuracy: 0.8472 - val_loss: 0.3029 - val_binary_accuracy: 0.8681\n",
      "Epoch 188/10000\n",
      " - 0s - loss: 0.3448 - binary_accuracy: 0.8495 - val_loss: 0.3018 - val_binary_accuracy: 0.8669\n",
      "Epoch 189/10000\n",
      " - 0s - loss: 0.3503 - binary_accuracy: 0.8444 - val_loss: 0.3024 - val_binary_accuracy: 0.8660\n",
      "Epoch 190/10000\n",
      " - 0s - loss: 0.3464 - binary_accuracy: 0.8458 - val_loss: 0.3026 - val_binary_accuracy: 0.8662\n",
      "Epoch 191/10000\n",
      " - 0s - loss: 0.3418 - binary_accuracy: 0.8468 - val_loss: 0.3022 - val_binary_accuracy: 0.8692\n",
      "Epoch 192/10000\n",
      " - 0s - loss: 0.3404 - binary_accuracy: 0.8527 - val_loss: 0.3022 - val_binary_accuracy: 0.8681\n",
      "Epoch 193/10000\n",
      " - 0s - loss: 0.3379 - binary_accuracy: 0.8512 - val_loss: 0.3007 - val_binary_accuracy: 0.8667\n",
      "Epoch 194/10000\n",
      " - 0s - loss: 0.3399 - binary_accuracy: 0.8494 - val_loss: 0.3017 - val_binary_accuracy: 0.8679\n",
      "Epoch 195/10000\n",
      " - 0s - loss: 0.3441 - binary_accuracy: 0.8479 - val_loss: 0.3022 - val_binary_accuracy: 0.8656\n",
      "Epoch 196/10000\n",
      " - 0s - loss: 0.3355 - binary_accuracy: 0.8523 - val_loss: 0.3024 - val_binary_accuracy: 0.8652\n",
      "Epoch 197/10000\n",
      " - 0s - loss: 0.3365 - binary_accuracy: 0.8507 - val_loss: 0.3028 - val_binary_accuracy: 0.8683\n",
      "Epoch 198/10000\n",
      " - 0s - loss: 0.3473 - binary_accuracy: 0.8483 - val_loss: 0.3041 - val_binary_accuracy: 0.8667\n",
      "Epoch 199/10000\n",
      " - 0s - loss: 0.3442 - binary_accuracy: 0.8473 - val_loss: 0.3040 - val_binary_accuracy: 0.8656\n",
      "Epoch 200/10000\n",
      " - 0s - loss: 0.3422 - binary_accuracy: 0.8487 - val_loss: 0.3023 - val_binary_accuracy: 0.8662\n",
      "Epoch 201/10000\n",
      " - 0s - loss: 0.3311 - binary_accuracy: 0.8551 - val_loss: 0.3034 - val_binary_accuracy: 0.8669\n",
      "Epoch 202/10000\n",
      " - 0s - loss: 0.3448 - binary_accuracy: 0.8429 - val_loss: 0.3025 - val_binary_accuracy: 0.8660\n",
      "Epoch 203/10000\n",
      " - 0s - loss: 0.3435 - binary_accuracy: 0.8514 - val_loss: 0.3027 - val_binary_accuracy: 0.8677\n",
      "Epoch 204/10000\n",
      " - 0s - loss: 0.3498 - binary_accuracy: 0.8464 - val_loss: 0.3022 - val_binary_accuracy: 0.8690\n",
      "Epoch 205/10000\n",
      " - 0s - loss: 0.3441 - binary_accuracy: 0.8482 - val_loss: 0.3005 - val_binary_accuracy: 0.8700\n",
      "Epoch 206/10000\n",
      " - 0s - loss: 0.3393 - binary_accuracy: 0.8496 - val_loss: 0.3014 - val_binary_accuracy: 0.8719\n",
      "Epoch 207/10000\n",
      " - 0s - loss: 0.3423 - binary_accuracy: 0.8490 - val_loss: 0.3024 - val_binary_accuracy: 0.8704\n",
      "Epoch 208/10000\n",
      " - 0s - loss: 0.3325 - binary_accuracy: 0.8531 - val_loss: 0.3008 - val_binary_accuracy: 0.8681\n",
      "Epoch 209/10000\n",
      " - 0s - loss: 0.3421 - binary_accuracy: 0.8508 - val_loss: 0.3010 - val_binary_accuracy: 0.8694\n",
      "Epoch 210/10000\n",
      " - 0s - loss: 0.3520 - binary_accuracy: 0.8430 - val_loss: 0.3018 - val_binary_accuracy: 0.8710\n",
      "Epoch 211/10000\n",
      " - 0s - loss: 0.3430 - binary_accuracy: 0.8460 - val_loss: 0.2997 - val_binary_accuracy: 0.8710\n",
      "Epoch 212/10000\n",
      " - 0s - loss: 0.3461 - binary_accuracy: 0.8459 - val_loss: 0.3014 - val_binary_accuracy: 0.8704\n",
      "Epoch 213/10000\n",
      " - 0s - loss: 0.3426 - binary_accuracy: 0.8487 - val_loss: 0.3007 - val_binary_accuracy: 0.8704\n",
      "Epoch 214/10000\n",
      " - 0s - loss: 0.3342 - binary_accuracy: 0.8520 - val_loss: 0.3013 - val_binary_accuracy: 0.8696\n",
      "Epoch 215/10000\n",
      " - 0s - loss: 0.3380 - binary_accuracy: 0.8483 - val_loss: 0.3010 - val_binary_accuracy: 0.8667\n",
      "Epoch 216/10000\n",
      " - 0s - loss: 0.3394 - binary_accuracy: 0.8514 - val_loss: 0.3015 - val_binary_accuracy: 0.8704\n",
      "Epoch 217/10000\n",
      " - 0s - loss: 0.3340 - binary_accuracy: 0.8530 - val_loss: 0.3000 - val_binary_accuracy: 0.8681\n",
      "Epoch 218/10000\n",
      " - 0s - loss: 0.3388 - binary_accuracy: 0.8480 - val_loss: 0.2996 - val_binary_accuracy: 0.8683\n",
      "Epoch 219/10000\n",
      " - 0s - loss: 0.3379 - binary_accuracy: 0.8495 - val_loss: 0.2995 - val_binary_accuracy: 0.8694\n",
      "Epoch 220/10000\n",
      " - 0s - loss: 0.3423 - binary_accuracy: 0.8474 - val_loss: 0.3002 - val_binary_accuracy: 0.8681\n",
      "Epoch 221/10000\n",
      " - 0s - loss: 0.3338 - binary_accuracy: 0.8519 - val_loss: 0.3022 - val_binary_accuracy: 0.8687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/10000\n",
      " - 0s - loss: 0.3414 - binary_accuracy: 0.8496 - val_loss: 0.3024 - val_binary_accuracy: 0.8690\n",
      "Epoch 223/10000\n",
      " - 0s - loss: 0.3332 - binary_accuracy: 0.8510 - val_loss: 0.3008 - val_binary_accuracy: 0.8681\n",
      "Epoch 224/10000\n",
      " - 0s - loss: 0.3452 - binary_accuracy: 0.8472 - val_loss: 0.2997 - val_binary_accuracy: 0.8702\n",
      "Epoch 225/10000\n",
      " - 0s - loss: 0.3368 - binary_accuracy: 0.8524 - val_loss: 0.3021 - val_binary_accuracy: 0.8687\n",
      "Epoch 226/10000\n",
      " - 0s - loss: 0.3357 - binary_accuracy: 0.8514 - val_loss: 0.3027 - val_binary_accuracy: 0.8675\n",
      "Epoch 227/10000\n",
      " - 0s - loss: 0.3386 - binary_accuracy: 0.8515 - val_loss: 0.3023 - val_binary_accuracy: 0.8690\n",
      "Epoch 228/10000\n",
      " - 0s - loss: 0.3436 - binary_accuracy: 0.8471 - val_loss: 0.3010 - val_binary_accuracy: 0.8702\n",
      "Epoch 229/10000\n",
      " - 0s - loss: 0.3377 - binary_accuracy: 0.8500 - val_loss: 0.3020 - val_binary_accuracy: 0.8692\n",
      "Epoch 230/10000\n",
      " - 0s - loss: 0.3396 - binary_accuracy: 0.8506 - val_loss: 0.3025 - val_binary_accuracy: 0.8681\n",
      "Epoch 231/10000\n",
      " - 0s - loss: 0.3349 - binary_accuracy: 0.8511 - val_loss: 0.3010 - val_binary_accuracy: 0.8696\n",
      "Epoch 232/10000\n",
      " - 0s - loss: 0.3295 - binary_accuracy: 0.8549 - val_loss: 0.3005 - val_binary_accuracy: 0.8687\n",
      "Epoch 233/10000\n",
      " - 0s - loss: 0.3383 - binary_accuracy: 0.8511 - val_loss: 0.2998 - val_binary_accuracy: 0.8685\n",
      "Epoch 234/10000\n",
      " - 0s - loss: 0.3383 - binary_accuracy: 0.8514 - val_loss: 0.3001 - val_binary_accuracy: 0.8685\n",
      "Epoch 235/10000\n",
      " - 0s - loss: 0.3415 - binary_accuracy: 0.8500 - val_loss: 0.3007 - val_binary_accuracy: 0.8688\n",
      "Epoch 236/10000\n",
      " - 0s - loss: 0.3389 - binary_accuracy: 0.8494 - val_loss: 0.3005 - val_binary_accuracy: 0.8710\n",
      "Epoch 237/10000\n",
      " - 0s - loss: 0.3329 - binary_accuracy: 0.8521 - val_loss: 0.3002 - val_binary_accuracy: 0.8721\n",
      "Epoch 238/10000\n",
      " - 0s - loss: 0.3395 - binary_accuracy: 0.8505 - val_loss: 0.3020 - val_binary_accuracy: 0.8700\n",
      "Epoch 239/10000\n",
      " - 0s - loss: 0.3464 - binary_accuracy: 0.8446 - val_loss: 0.3015 - val_binary_accuracy: 0.8712\n",
      "Epoch 240/10000\n",
      " - 0s - loss: 0.3355 - binary_accuracy: 0.8521 - val_loss: 0.3015 - val_binary_accuracy: 0.8708\n",
      "Epoch 241/10000\n",
      " - 0s - loss: 0.3337 - binary_accuracy: 0.8529 - val_loss: 0.3007 - val_binary_accuracy: 0.8704\n",
      "Epoch 242/10000\n",
      " - 0s - loss: 0.3331 - binary_accuracy: 0.8514 - val_loss: 0.3005 - val_binary_accuracy: 0.8708\n",
      "Epoch 243/10000\n",
      " - 0s - loss: 0.3368 - binary_accuracy: 0.8508 - val_loss: 0.2988 - val_binary_accuracy: 0.8690\n",
      "Epoch 244/10000\n",
      " - 0s - loss: 0.3360 - binary_accuracy: 0.8532 - val_loss: 0.2995 - val_binary_accuracy: 0.8717\n",
      "Epoch 245/10000\n",
      " - 0s - loss: 0.3353 - binary_accuracy: 0.8543 - val_loss: 0.2995 - val_binary_accuracy: 0.8719\n",
      "Epoch 246/10000\n",
      " - 0s - loss: 0.3414 - binary_accuracy: 0.8514 - val_loss: 0.3001 - val_binary_accuracy: 0.8696\n",
      "Epoch 247/10000\n",
      " - 0s - loss: 0.3324 - binary_accuracy: 0.8514 - val_loss: 0.3003 - val_binary_accuracy: 0.8712\n",
      "Epoch 248/10000\n",
      " - 0s - loss: 0.3361 - binary_accuracy: 0.8545 - val_loss: 0.3000 - val_binary_accuracy: 0.8696\n",
      "Epoch 249/10000\n",
      " - 0s - loss: 0.3330 - binary_accuracy: 0.8517 - val_loss: 0.3011 - val_binary_accuracy: 0.8692\n",
      "Epoch 250/10000\n",
      " - 0s - loss: 0.3360 - binary_accuracy: 0.8514 - val_loss: 0.3017 - val_binary_accuracy: 0.8694\n",
      "Epoch 251/10000\n",
      " - 0s - loss: 0.3402 - binary_accuracy: 0.8485 - val_loss: 0.3024 - val_binary_accuracy: 0.8688\n",
      "Epoch 252/10000\n",
      " - 0s - loss: 0.3453 - binary_accuracy: 0.8489 - val_loss: 0.3031 - val_binary_accuracy: 0.8685\n",
      "Epoch 253/10000\n",
      " - 0s - loss: 0.3342 - binary_accuracy: 0.8533 - val_loss: 0.3030 - val_binary_accuracy: 0.8677\n",
      "Epoch 254/10000\n",
      " - 0s - loss: 0.3395 - binary_accuracy: 0.8502 - val_loss: 0.3018 - val_binary_accuracy: 0.8663\n",
      "Epoch 255/10000\n",
      " - 0s - loss: 0.3404 - binary_accuracy: 0.8469 - val_loss: 0.3022 - val_binary_accuracy: 0.8671\n",
      "Epoch 256/10000\n",
      " - 0s - loss: 0.3395 - binary_accuracy: 0.8497 - val_loss: 0.3014 - val_binary_accuracy: 0.8694\n",
      "Epoch 257/10000\n",
      " - 0s - loss: 0.3326 - binary_accuracy: 0.8509 - val_loss: 0.3019 - val_binary_accuracy: 0.8685\n",
      "Epoch 258/10000\n",
      " - 0s - loss: 0.3363 - binary_accuracy: 0.8502 - val_loss: 0.3019 - val_binary_accuracy: 0.8673\n",
      "Epoch 259/10000\n",
      " - 0s - loss: 0.3412 - binary_accuracy: 0.8520 - val_loss: 0.3021 - val_binary_accuracy: 0.8665\n",
      "Epoch 260/10000\n",
      " - 0s - loss: 0.3380 - binary_accuracy: 0.8514 - val_loss: 0.3020 - val_binary_accuracy: 0.8692\n",
      "Epoch 261/10000\n",
      " - 0s - loss: 0.3415 - binary_accuracy: 0.8509 - val_loss: 0.3007 - val_binary_accuracy: 0.8702\n",
      "Epoch 262/10000\n",
      " - 0s - loss: 0.3407 - binary_accuracy: 0.8480 - val_loss: 0.3022 - val_binary_accuracy: 0.8667\n",
      "Epoch 263/10000\n",
      " - 0s - loss: 0.3371 - binary_accuracy: 0.8482 - val_loss: 0.3015 - val_binary_accuracy: 0.8696\n",
      "Epoch 264/10000\n",
      " - 0s - loss: 0.3340 - binary_accuracy: 0.8540 - val_loss: 0.3012 - val_binary_accuracy: 0.8687\n",
      "Epoch 265/10000\n",
      " - 0s - loss: 0.3378 - binary_accuracy: 0.8516 - val_loss: 0.3014 - val_binary_accuracy: 0.8698\n",
      "Epoch 266/10000\n",
      " - 0s - loss: 0.3383 - binary_accuracy: 0.8482 - val_loss: 0.3014 - val_binary_accuracy: 0.8683\n",
      "Epoch 267/10000\n",
      " - 0s - loss: 0.3395 - binary_accuracy: 0.8496 - val_loss: 0.3036 - val_binary_accuracy: 0.8673\n",
      "Epoch 268/10000\n",
      " - 0s - loss: 0.3265 - binary_accuracy: 0.8569 - val_loss: 0.3012 - val_binary_accuracy: 0.8692\n",
      "Epoch 269/10000\n",
      " - 0s - loss: 0.3367 - binary_accuracy: 0.8514 - val_loss: 0.3009 - val_binary_accuracy: 0.8704\n",
      "Epoch 270/10000\n",
      " - 0s - loss: 0.3393 - binary_accuracy: 0.8529 - val_loss: 0.3016 - val_binary_accuracy: 0.8708\n",
      "Epoch 271/10000\n",
      " - 0s - loss: 0.3426 - binary_accuracy: 0.8502 - val_loss: 0.3007 - val_binary_accuracy: 0.8688\n",
      "Epoch 272/10000\n",
      " - 0s - loss: 0.3348 - binary_accuracy: 0.8515 - val_loss: 0.3003 - val_binary_accuracy: 0.8673\n",
      "Epoch 273/10000\n",
      " - 0s - loss: 0.3385 - binary_accuracy: 0.8497 - val_loss: 0.2996 - val_binary_accuracy: 0.8696\n",
      "Epoch 274/10000\n",
      " - 0s - loss: 0.3446 - binary_accuracy: 0.8494 - val_loss: 0.2998 - val_binary_accuracy: 0.8669\n",
      "Epoch 275/10000\n",
      " - 0s - loss: 0.3384 - binary_accuracy: 0.8502 - val_loss: 0.2994 - val_binary_accuracy: 0.8675\n",
      "Epoch 276/10000\n",
      " - 0s - loss: 0.3345 - binary_accuracy: 0.8524 - val_loss: 0.3005 - val_binary_accuracy: 0.8673\n",
      "Epoch 277/10000\n",
      " - 0s - loss: 0.3321 - binary_accuracy: 0.8546 - val_loss: 0.3001 - val_binary_accuracy: 0.8694\n",
      "Epoch 278/10000\n",
      " - 0s - loss: 0.3311 - binary_accuracy: 0.8559 - val_loss: 0.2993 - val_binary_accuracy: 0.8685\n",
      "Epoch 279/10000\n",
      " - 0s - loss: 0.3398 - binary_accuracy: 0.8509 - val_loss: 0.2994 - val_binary_accuracy: 0.8683\n",
      "Epoch 280/10000\n",
      " - 0s - loss: 0.3388 - binary_accuracy: 0.8514 - val_loss: 0.3001 - val_binary_accuracy: 0.8677\n",
      "Epoch 281/10000\n",
      " - 0s - loss: 0.3410 - binary_accuracy: 0.8466 - val_loss: 0.3007 - val_binary_accuracy: 0.8683\n",
      "Epoch 282/10000\n",
      " - 0s - loss: 0.3316 - binary_accuracy: 0.8527 - val_loss: 0.3013 - val_binary_accuracy: 0.8690\n",
      "Epoch 283/10000\n",
      " - 0s - loss: 0.3341 - binary_accuracy: 0.8492 - val_loss: 0.2991 - val_binary_accuracy: 0.8690\n",
      "Epoch 284/10000\n",
      " - 0s - loss: 0.3322 - binary_accuracy: 0.8535 - val_loss: 0.3001 - val_binary_accuracy: 0.8683\n",
      "Epoch 285/10000\n",
      " - 0s - loss: 0.3349 - binary_accuracy: 0.8534 - val_loss: 0.3002 - val_binary_accuracy: 0.8685\n",
      "Epoch 286/10000\n",
      " - 0s - loss: 0.3335 - binary_accuracy: 0.8519 - val_loss: 0.3026 - val_binary_accuracy: 0.8685\n",
      "Epoch 287/10000\n",
      " - 0s - loss: 0.3297 - binary_accuracy: 0.8590 - val_loss: 0.3038 - val_binary_accuracy: 0.8679\n",
      "Epoch 288/10000\n",
      " - 0s - loss: 0.3382 - binary_accuracy: 0.8505 - val_loss: 0.3045 - val_binary_accuracy: 0.8667\n",
      "Epoch 289/10000\n",
      " - 0s - loss: 0.3361 - binary_accuracy: 0.8501 - val_loss: 0.3016 - val_binary_accuracy: 0.8719\n",
      "Epoch 290/10000\n",
      " - 0s - loss: 0.3423 - binary_accuracy: 0.8506 - val_loss: 0.3024 - val_binary_accuracy: 0.8679\n",
      "Epoch 291/10000\n",
      " - 0s - loss: 0.3349 - binary_accuracy: 0.8521 - val_loss: 0.3020 - val_binary_accuracy: 0.8671\n",
      "Epoch 292/10000\n",
      " - 0s - loss: 0.3304 - binary_accuracy: 0.8538 - val_loss: 0.3025 - val_binary_accuracy: 0.8696\n",
      "Epoch 293/10000\n",
      " - 0s - loss: 0.3365 - binary_accuracy: 0.8547 - val_loss: 0.3022 - val_binary_accuracy: 0.8694\n",
      "Epoch 294/10000\n",
      " - 0s - loss: 0.3341 - binary_accuracy: 0.8516 - val_loss: 0.3022 - val_binary_accuracy: 0.8683\n",
      "Epoch 295/10000\n",
      " - 0s - loss: 0.3319 - binary_accuracy: 0.8522 - val_loss: 0.3007 - val_binary_accuracy: 0.8712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/10000\n",
      " - 0s - loss: 0.3304 - binary_accuracy: 0.8530 - val_loss: 0.3000 - val_binary_accuracy: 0.8688\n",
      "Epoch 297/10000\n",
      " - 0s - loss: 0.3344 - binary_accuracy: 0.8536 - val_loss: 0.2997 - val_binary_accuracy: 0.8677\n",
      "Epoch 298/10000\n",
      " - 0s - loss: 0.3354 - binary_accuracy: 0.8513 - val_loss: 0.3003 - val_binary_accuracy: 0.8696\n",
      "Epoch 299/10000\n",
      " - 0s - loss: 0.3330 - binary_accuracy: 0.8516 - val_loss: 0.2983 - val_binary_accuracy: 0.8700\n",
      "Epoch 300/10000\n",
      " - 0s - loss: 0.3374 - binary_accuracy: 0.8520 - val_loss: 0.3001 - val_binary_accuracy: 0.8704\n",
      "Epoch 301/10000\n",
      " - 0s - loss: 0.3387 - binary_accuracy: 0.8484 - val_loss: 0.2994 - val_binary_accuracy: 0.8692\n",
      "Epoch 302/10000\n",
      " - 0s - loss: 0.3288 - binary_accuracy: 0.8549 - val_loss: 0.3020 - val_binary_accuracy: 0.8673\n",
      "Epoch 303/10000\n",
      " - 0s - loss: 0.3308 - binary_accuracy: 0.8541 - val_loss: 0.3005 - val_binary_accuracy: 0.8681\n",
      "Epoch 304/10000\n",
      " - 0s - loss: 0.3358 - binary_accuracy: 0.8489 - val_loss: 0.2995 - val_binary_accuracy: 0.8683\n",
      "Epoch 305/10000\n",
      " - 0s - loss: 0.3319 - binary_accuracy: 0.8527 - val_loss: 0.2994 - val_binary_accuracy: 0.8665\n",
      "Epoch 306/10000\n",
      " - 0s - loss: 0.3374 - binary_accuracy: 0.8511 - val_loss: 0.2987 - val_binary_accuracy: 0.8671\n",
      "Epoch 307/10000\n",
      " - 0s - loss: 0.3413 - binary_accuracy: 0.8501 - val_loss: 0.2991 - val_binary_accuracy: 0.8698\n",
      "Epoch 308/10000\n",
      " - 0s - loss: 0.3342 - binary_accuracy: 0.8498 - val_loss: 0.2989 - val_binary_accuracy: 0.8677\n",
      "Epoch 309/10000\n",
      " - 0s - loss: 0.3428 - binary_accuracy: 0.8502 - val_loss: 0.2993 - val_binary_accuracy: 0.8675\n",
      "Epoch 310/10000\n",
      " - 0s - loss: 0.3270 - binary_accuracy: 0.8579 - val_loss: 0.3003 - val_binary_accuracy: 0.8683\n",
      "Epoch 311/10000\n",
      " - 0s - loss: 0.3336 - binary_accuracy: 0.8549 - val_loss: 0.2990 - val_binary_accuracy: 0.8698\n",
      "Epoch 312/10000\n",
      " - 0s - loss: 0.3326 - binary_accuracy: 0.8552 - val_loss: 0.2998 - val_binary_accuracy: 0.8700\n",
      "Epoch 313/10000\n",
      " - 0s - loss: 0.3380 - binary_accuracy: 0.8521 - val_loss: 0.3004 - val_binary_accuracy: 0.8719\n",
      "Epoch 314/10000\n",
      " - 0s - loss: 0.3337 - binary_accuracy: 0.8529 - val_loss: 0.3004 - val_binary_accuracy: 0.8702\n",
      "Epoch 315/10000\n",
      " - 0s - loss: 0.3390 - binary_accuracy: 0.8512 - val_loss: 0.3018 - val_binary_accuracy: 0.8685\n",
      "Epoch 316/10000\n",
      " - 0s - loss: 0.3326 - binary_accuracy: 0.8512 - val_loss: 0.2991 - val_binary_accuracy: 0.8700\n",
      "Epoch 317/10000\n",
      " - 0s - loss: 0.3389 - binary_accuracy: 0.8487 - val_loss: 0.3012 - val_binary_accuracy: 0.8708\n",
      "Epoch 318/10000\n",
      " - 0s - loss: 0.3294 - binary_accuracy: 0.8577 - val_loss: 0.2985 - val_binary_accuracy: 0.8708\n",
      "Epoch 319/10000\n",
      " - 0s - loss: 0.3415 - binary_accuracy: 0.8500 - val_loss: 0.2999 - val_binary_accuracy: 0.8687\n",
      "Epoch 320/10000\n",
      " - 0s - loss: 0.3310 - binary_accuracy: 0.8538 - val_loss: 0.2992 - val_binary_accuracy: 0.8696\n",
      "Epoch 321/10000\n",
      " - 0s - loss: 0.3307 - binary_accuracy: 0.8553 - val_loss: 0.2995 - val_binary_accuracy: 0.8679\n",
      "Epoch 322/10000\n",
      " - 0s - loss: 0.3279 - binary_accuracy: 0.8557 - val_loss: 0.2997 - val_binary_accuracy: 0.8710\n",
      "Epoch 323/10000\n",
      " - 0s - loss: 0.3365 - binary_accuracy: 0.8503 - val_loss: 0.2987 - val_binary_accuracy: 0.8685\n",
      "Epoch 324/10000\n",
      " - 0s - loss: 0.3347 - binary_accuracy: 0.8501 - val_loss: 0.3000 - val_binary_accuracy: 0.8671\n",
      "Epoch 325/10000\n",
      " - 0s - loss: 0.3277 - binary_accuracy: 0.8537 - val_loss: 0.3011 - val_binary_accuracy: 0.8673\n",
      "Epoch 326/10000\n",
      " - 0s - loss: 0.3366 - binary_accuracy: 0.8514 - val_loss: 0.3015 - val_binary_accuracy: 0.8677\n",
      "Epoch 327/10000\n",
      " - 0s - loss: 0.3284 - binary_accuracy: 0.8568 - val_loss: 0.3008 - val_binary_accuracy: 0.8679\n",
      "Epoch 328/10000\n",
      " - 0s - loss: 0.3389 - binary_accuracy: 0.8540 - val_loss: 0.3000 - val_binary_accuracy: 0.8702\n",
      "Epoch 329/10000\n",
      " - 0s - loss: 0.3326 - binary_accuracy: 0.8541 - val_loss: 0.3002 - val_binary_accuracy: 0.8688\n",
      "Epoch 330/10000\n",
      " - 0s - loss: 0.3416 - binary_accuracy: 0.8505 - val_loss: 0.3000 - val_binary_accuracy: 0.8696\n",
      "Epoch 331/10000\n",
      " - 0s - loss: 0.3272 - binary_accuracy: 0.8570 - val_loss: 0.2996 - val_binary_accuracy: 0.8723\n",
      "Epoch 332/10000\n",
      " - 0s - loss: 0.3359 - binary_accuracy: 0.8517 - val_loss: 0.2995 - val_binary_accuracy: 0.8713\n",
      "Epoch 333/10000\n",
      " - 0s - loss: 0.3309 - binary_accuracy: 0.8567 - val_loss: 0.2979 - val_binary_accuracy: 0.8715\n",
      "Epoch 334/10000\n",
      " - 0s - loss: 0.3328 - binary_accuracy: 0.8534 - val_loss: 0.2989 - val_binary_accuracy: 0.8704\n",
      "Epoch 335/10000\n",
      " - 0s - loss: 0.3371 - binary_accuracy: 0.8490 - val_loss: 0.2995 - val_binary_accuracy: 0.8721\n",
      "Epoch 336/10000\n",
      " - 0s - loss: 0.3303 - binary_accuracy: 0.8572 - val_loss: 0.2980 - val_binary_accuracy: 0.8721\n",
      "Epoch 337/10000\n",
      " - 0s - loss: 0.3366 - binary_accuracy: 0.8518 - val_loss: 0.3004 - val_binary_accuracy: 0.8712\n",
      "Epoch 338/10000\n",
      " - 0s - loss: 0.3331 - binary_accuracy: 0.8514 - val_loss: 0.3010 - val_binary_accuracy: 0.8704\n",
      "Epoch 339/10000\n",
      " - 0s - loss: 0.3367 - binary_accuracy: 0.8513 - val_loss: 0.3002 - val_binary_accuracy: 0.8692\n",
      "Epoch 340/10000\n",
      " - 0s - loss: 0.3285 - binary_accuracy: 0.8547 - val_loss: 0.3008 - val_binary_accuracy: 0.8690\n",
      "Epoch 341/10000\n",
      " - 0s - loss: 0.3414 - binary_accuracy: 0.8492 - val_loss: 0.3017 - val_binary_accuracy: 0.8696\n",
      "Epoch 342/10000\n",
      " - 0s - loss: 0.3379 - binary_accuracy: 0.8515 - val_loss: 0.3019 - val_binary_accuracy: 0.8712\n",
      "Epoch 343/10000\n",
      " - 0s - loss: 0.3374 - binary_accuracy: 0.8526 - val_loss: 0.3017 - val_binary_accuracy: 0.8698\n",
      "Epoch 344/10000\n",
      " - 0s - loss: 0.3271 - binary_accuracy: 0.8562 - val_loss: 0.2994 - val_binary_accuracy: 0.8715\n",
      "Epoch 345/10000\n",
      " - 0s - loss: 0.3387 - binary_accuracy: 0.8513 - val_loss: 0.2996 - val_binary_accuracy: 0.8708\n",
      "Epoch 346/10000\n",
      " - 0s - loss: 0.3376 - binary_accuracy: 0.8510 - val_loss: 0.3002 - val_binary_accuracy: 0.8713\n",
      "Epoch 347/10000\n",
      " - 0s - loss: 0.3338 - binary_accuracy: 0.8516 - val_loss: 0.2999 - val_binary_accuracy: 0.8725\n",
      "Epoch 348/10000\n",
      " - 0s - loss: 0.3300 - binary_accuracy: 0.8559 - val_loss: 0.3000 - val_binary_accuracy: 0.8696\n",
      "Epoch 349/10000\n",
      " - 0s - loss: 0.3340 - binary_accuracy: 0.8544 - val_loss: 0.3006 - val_binary_accuracy: 0.8675\n",
      "Epoch 350/10000\n",
      " - 0s - loss: 0.3327 - binary_accuracy: 0.8511 - val_loss: 0.2996 - val_binary_accuracy: 0.8713\n",
      "Epoch 351/10000\n",
      " - 0s - loss: 0.3295 - binary_accuracy: 0.8530 - val_loss: 0.3014 - val_binary_accuracy: 0.8694\n",
      "Epoch 352/10000\n",
      " - 0s - loss: 0.3289 - binary_accuracy: 0.8568 - val_loss: 0.3028 - val_binary_accuracy: 0.8685\n",
      "Epoch 353/10000\n",
      " - 0s - loss: 0.3360 - binary_accuracy: 0.8529 - val_loss: 0.3020 - val_binary_accuracy: 0.8658\n",
      "Epoch 354/10000\n",
      " - 0s - loss: 0.3326 - binary_accuracy: 0.8514 - val_loss: 0.3024 - val_binary_accuracy: 0.8660\n",
      "Epoch 355/10000\n",
      " - 0s - loss: 0.3328 - binary_accuracy: 0.8524 - val_loss: 0.3019 - val_binary_accuracy: 0.8683\n",
      "Epoch 356/10000\n",
      " - 0s - loss: 0.3315 - binary_accuracy: 0.8520 - val_loss: 0.3001 - val_binary_accuracy: 0.8665\n",
      "Epoch 357/10000\n",
      " - 0s - loss: 0.3354 - binary_accuracy: 0.8515 - val_loss: 0.3018 - val_binary_accuracy: 0.8671\n",
      "Epoch 358/10000\n",
      " - 0s - loss: 0.3459 - binary_accuracy: 0.8482 - val_loss: 0.3014 - val_binary_accuracy: 0.8648\n",
      "Epoch 359/10000\n",
      " - 0s - loss: 0.3276 - binary_accuracy: 0.8567 - val_loss: 0.2998 - val_binary_accuracy: 0.8683\n",
      "Epoch 360/10000\n",
      " - 0s - loss: 0.3317 - binary_accuracy: 0.8548 - val_loss: 0.3022 - val_binary_accuracy: 0.8694\n",
      "Epoch 361/10000\n",
      " - 0s - loss: 0.3389 - binary_accuracy: 0.8491 - val_loss: 0.3008 - val_binary_accuracy: 0.8698\n",
      "Epoch 362/10000\n",
      " - 0s - loss: 0.3349 - binary_accuracy: 0.8536 - val_loss: 0.3008 - val_binary_accuracy: 0.8688\n",
      "Epoch 363/10000\n",
      " - 0s - loss: 0.3378 - binary_accuracy: 0.8493 - val_loss: 0.3004 - val_binary_accuracy: 0.8669\n",
      "Epoch 364/10000\n",
      " - 0s - loss: 0.3363 - binary_accuracy: 0.8486 - val_loss: 0.3006 - val_binary_accuracy: 0.8677\n",
      "Epoch 365/10000\n",
      " - 0s - loss: 0.3281 - binary_accuracy: 0.8582 - val_loss: 0.3003 - val_binary_accuracy: 0.8673\n",
      "Epoch 366/10000\n",
      " - 0s - loss: 0.3365 - binary_accuracy: 0.8503 - val_loss: 0.3000 - val_binary_accuracy: 0.8675\n",
      "Epoch 367/10000\n",
      " - 0s - loss: 0.3345 - binary_accuracy: 0.8544 - val_loss: 0.3000 - val_binary_accuracy: 0.8677\n",
      "Epoch 368/10000\n",
      " - 0s - loss: 0.3357 - binary_accuracy: 0.8524 - val_loss: 0.3008 - val_binary_accuracy: 0.8683\n",
      "Epoch 369/10000\n",
      " - 0s - loss: 0.3269 - binary_accuracy: 0.8577 - val_loss: 0.3016 - val_binary_accuracy: 0.8662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370/10000\n",
      " - 0s - loss: 0.3338 - binary_accuracy: 0.8536 - val_loss: 0.3008 - val_binary_accuracy: 0.8694\n",
      "Epoch 371/10000\n",
      " - 0s - loss: 0.3337 - binary_accuracy: 0.8528 - val_loss: 0.2999 - val_binary_accuracy: 0.8702\n",
      "Epoch 372/10000\n",
      " - 0s - loss: 0.3380 - binary_accuracy: 0.8517 - val_loss: 0.2991 - val_binary_accuracy: 0.8702\n",
      "Epoch 373/10000\n",
      " - 0s - loss: 0.3299 - binary_accuracy: 0.8556 - val_loss: 0.3000 - val_binary_accuracy: 0.8702\n",
      "Epoch 374/10000\n",
      " - 0s - loss: 0.3331 - binary_accuracy: 0.8532 - val_loss: 0.3001 - val_binary_accuracy: 0.8708\n",
      "Epoch 375/10000\n",
      " - 0s - loss: 0.3316 - binary_accuracy: 0.8542 - val_loss: 0.3010 - val_binary_accuracy: 0.8665\n",
      "Epoch 376/10000\n",
      " - 0s - loss: 0.3327 - binary_accuracy: 0.8530 - val_loss: 0.2996 - val_binary_accuracy: 0.8696\n",
      "Epoch 377/10000\n",
      " - 0s - loss: 0.3274 - binary_accuracy: 0.8554 - val_loss: 0.2977 - val_binary_accuracy: 0.8685\n",
      "Epoch 378/10000\n",
      " - 0s - loss: 0.3338 - binary_accuracy: 0.8501 - val_loss: 0.2986 - val_binary_accuracy: 0.8694\n",
      "Epoch 379/10000\n",
      " - 0s - loss: 0.3345 - binary_accuracy: 0.8532 - val_loss: 0.3009 - val_binary_accuracy: 0.8696\n",
      "Epoch 380/10000\n",
      " - 0s - loss: 0.3267 - binary_accuracy: 0.8568 - val_loss: 0.2992 - val_binary_accuracy: 0.8706\n",
      "Epoch 381/10000\n",
      " - 0s - loss: 0.3277 - binary_accuracy: 0.8538 - val_loss: 0.3005 - val_binary_accuracy: 0.8675\n",
      "Epoch 382/10000\n",
      " - 0s - loss: 0.3422 - binary_accuracy: 0.8504 - val_loss: 0.3000 - val_binary_accuracy: 0.8696\n",
      "Epoch 383/10000\n",
      " - 0s - loss: 0.3329 - binary_accuracy: 0.8540 - val_loss: 0.2996 - val_binary_accuracy: 0.8692\n",
      "Epoch 384/10000\n",
      " - 0s - loss: 0.3368 - binary_accuracy: 0.8522 - val_loss: 0.2997 - val_binary_accuracy: 0.8675\n",
      "Epoch 385/10000\n",
      " - 0s - loss: 0.3327 - binary_accuracy: 0.8526 - val_loss: 0.2994 - val_binary_accuracy: 0.8677\n",
      "Epoch 386/10000\n",
      " - 0s - loss: 0.3295 - binary_accuracy: 0.8568 - val_loss: 0.3007 - val_binary_accuracy: 0.8663\n",
      "Epoch 387/10000\n",
      " - 0s - loss: 0.3336 - binary_accuracy: 0.8524 - val_loss: 0.3000 - val_binary_accuracy: 0.8685\n",
      "Epoch 388/10000\n",
      " - 0s - loss: 0.3365 - binary_accuracy: 0.8486 - val_loss: 0.2997 - val_binary_accuracy: 0.8690\n",
      "Epoch 389/10000\n",
      " - 0s - loss: 0.3337 - binary_accuracy: 0.8520 - val_loss: 0.3000 - val_binary_accuracy: 0.8688\n",
      "Epoch 390/10000\n",
      " - 0s - loss: 0.3289 - binary_accuracy: 0.8568 - val_loss: 0.3009 - val_binary_accuracy: 0.8692\n",
      "Epoch 391/10000\n",
      " - 0s - loss: 0.3366 - binary_accuracy: 0.8514 - val_loss: 0.2997 - val_binary_accuracy: 0.8679\n",
      "Epoch 392/10000\n",
      " - 0s - loss: 0.3335 - binary_accuracy: 0.8548 - val_loss: 0.3010 - val_binary_accuracy: 0.8687\n",
      "Epoch 393/10000\n",
      " - 0s - loss: 0.3315 - binary_accuracy: 0.8555 - val_loss: 0.3005 - val_binary_accuracy: 0.8712\n",
      "Epoch 394/10000\n",
      " - 0s - loss: 0.3354 - binary_accuracy: 0.8548 - val_loss: 0.3016 - val_binary_accuracy: 0.8706\n",
      "Epoch 395/10000\n",
      " - 0s - loss: 0.3344 - binary_accuracy: 0.8525 - val_loss: 0.3015 - val_binary_accuracy: 0.8706\n",
      "Epoch 396/10000\n",
      " - 0s - loss: 0.3251 - binary_accuracy: 0.8572 - val_loss: 0.3019 - val_binary_accuracy: 0.8685\n",
      "Epoch 397/10000\n",
      " - 0s - loss: 0.3336 - binary_accuracy: 0.8542 - val_loss: 0.3008 - val_binary_accuracy: 0.8688\n",
      "Epoch 398/10000\n",
      " - 0s - loss: 0.3233 - binary_accuracy: 0.8598 - val_loss: 0.3016 - val_binary_accuracy: 0.8721\n",
      "Epoch 399/10000\n",
      " - 0s - loss: 0.3257 - binary_accuracy: 0.8556 - val_loss: 0.3011 - val_binary_accuracy: 0.8719\n",
      "Epoch 400/10000\n",
      " - 0s - loss: 0.3419 - binary_accuracy: 0.8519 - val_loss: 0.3005 - val_binary_accuracy: 0.8717\n",
      "Epoch 401/10000\n",
      " - 0s - loss: 0.3314 - binary_accuracy: 0.8548 - val_loss: 0.2998 - val_binary_accuracy: 0.8712\n",
      "Epoch 402/10000\n",
      " - 0s - loss: 0.3265 - binary_accuracy: 0.8570 - val_loss: 0.3007 - val_binary_accuracy: 0.8713\n",
      "Epoch 403/10000\n",
      " - 0s - loss: 0.3244 - binary_accuracy: 0.8582 - val_loss: 0.3011 - val_binary_accuracy: 0.8708\n",
      "Epoch 404/10000\n",
      " - 0s - loss: 0.3328 - binary_accuracy: 0.8503 - val_loss: 0.3006 - val_binary_accuracy: 0.8700\n",
      "Epoch 405/10000\n",
      " - 0s - loss: 0.3346 - binary_accuracy: 0.8549 - val_loss: 0.2992 - val_binary_accuracy: 0.8704\n",
      "Epoch 406/10000\n",
      " - 0s - loss: 0.3364 - binary_accuracy: 0.8539 - val_loss: 0.2999 - val_binary_accuracy: 0.8710\n",
      "Epoch 407/10000\n",
      " - 0s - loss: 0.3308 - binary_accuracy: 0.8554 - val_loss: 0.2996 - val_binary_accuracy: 0.8708\n",
      "Epoch 408/10000\n",
      " - 0s - loss: 0.3309 - binary_accuracy: 0.8545 - val_loss: 0.3007 - val_binary_accuracy: 0.8694\n",
      "Epoch 409/10000\n",
      " - 0s - loss: 0.3313 - binary_accuracy: 0.8550 - val_loss: 0.2989 - val_binary_accuracy: 0.8713\n",
      "Epoch 410/10000\n",
      " - 0s - loss: 0.3372 - binary_accuracy: 0.8545 - val_loss: 0.2992 - val_binary_accuracy: 0.8725\n",
      "Epoch 411/10000\n",
      " - 0s - loss: 0.3271 - binary_accuracy: 0.8587 - val_loss: 0.2995 - val_binary_accuracy: 0.8704\n",
      "Epoch 412/10000\n",
      " - 0s - loss: 0.3403 - binary_accuracy: 0.8473 - val_loss: 0.2985 - val_binary_accuracy: 0.8727\n",
      "Epoch 413/10000\n",
      " - 0s - loss: 0.3327 - binary_accuracy: 0.8541 - val_loss: 0.2987 - val_binary_accuracy: 0.8690\n",
      "Epoch 414/10000\n",
      " - 0s - loss: 0.3344 - binary_accuracy: 0.8553 - val_loss: 0.2984 - val_binary_accuracy: 0.8704\n",
      "Epoch 415/10000\n",
      " - 0s - loss: 0.3230 - binary_accuracy: 0.8604 - val_loss: 0.2991 - val_binary_accuracy: 0.8708\n",
      "Epoch 416/10000\n",
      " - 0s - loss: 0.3368 - binary_accuracy: 0.8524 - val_loss: 0.3009 - val_binary_accuracy: 0.8717\n",
      "Epoch 417/10000\n",
      " - 0s - loss: 0.3290 - binary_accuracy: 0.8546 - val_loss: 0.3005 - val_binary_accuracy: 0.8717\n",
      "Epoch 418/10000\n",
      " - 0s - loss: 0.3304 - binary_accuracy: 0.8539 - val_loss: 0.2986 - val_binary_accuracy: 0.8710\n",
      "Epoch 419/10000\n",
      " - 0s - loss: 0.3261 - binary_accuracy: 0.8566 - val_loss: 0.2997 - val_binary_accuracy: 0.8706\n",
      "Epoch 420/10000\n",
      " - 0s - loss: 0.3380 - binary_accuracy: 0.8502 - val_loss: 0.3017 - val_binary_accuracy: 0.8679\n",
      "Epoch 421/10000\n",
      " - 0s - loss: 0.3305 - binary_accuracy: 0.8523 - val_loss: 0.3006 - val_binary_accuracy: 0.8690\n",
      "Epoch 422/10000\n",
      " - 0s - loss: 0.3271 - binary_accuracy: 0.8581 - val_loss: 0.2991 - val_binary_accuracy: 0.8692\n",
      "Epoch 423/10000\n",
      " - 0s - loss: 0.3308 - binary_accuracy: 0.8511 - val_loss: 0.2995 - val_binary_accuracy: 0.8696\n",
      "Epoch 424/10000\n",
      " - 0s - loss: 0.3284 - binary_accuracy: 0.8553 - val_loss: 0.2995 - val_binary_accuracy: 0.8696\n",
      "Epoch 425/10000\n",
      " - 0s - loss: 0.3313 - binary_accuracy: 0.8557 - val_loss: 0.3007 - val_binary_accuracy: 0.8700\n",
      "Epoch 426/10000\n",
      " - 0s - loss: 0.3303 - binary_accuracy: 0.8547 - val_loss: 0.3001 - val_binary_accuracy: 0.8708\n",
      "Epoch 427/10000\n",
      " - 0s - loss: 0.3237 - binary_accuracy: 0.8571 - val_loss: 0.3010 - val_binary_accuracy: 0.8681\n",
      "Epoch 428/10000\n",
      " - 0s - loss: 0.3192 - binary_accuracy: 0.8590 - val_loss: 0.3005 - val_binary_accuracy: 0.8673\n",
      "Epoch 429/10000\n",
      " - 0s - loss: 0.3310 - binary_accuracy: 0.8545 - val_loss: 0.3003 - val_binary_accuracy: 0.8706\n",
      "Epoch 430/10000\n",
      " - 0s - loss: 0.3385 - binary_accuracy: 0.8530 - val_loss: 0.3004 - val_binary_accuracy: 0.8708\n",
      "Epoch 431/10000\n",
      " - 0s - loss: 0.3371 - binary_accuracy: 0.8525 - val_loss: 0.3000 - val_binary_accuracy: 0.8694\n",
      "Epoch 432/10000\n",
      " - 0s - loss: 0.3342 - binary_accuracy: 0.8528 - val_loss: 0.3007 - val_binary_accuracy: 0.8677\n",
      "Epoch 433/10000\n",
      " - 0s - loss: 0.3353 - binary_accuracy: 0.8528 - val_loss: 0.2999 - val_binary_accuracy: 0.8669\n",
      "Epoch 434/10000\n",
      " - 0s - loss: 0.3346 - binary_accuracy: 0.8586 - val_loss: 0.2998 - val_binary_accuracy: 0.8692\n",
      "Epoch 435/10000\n",
      " - 0s - loss: 0.3276 - binary_accuracy: 0.8538 - val_loss: 0.2994 - val_binary_accuracy: 0.8694\n",
      "Epoch 436/10000\n",
      " - 0s - loss: 0.3357 - binary_accuracy: 0.8483 - val_loss: 0.3000 - val_binary_accuracy: 0.8696\n",
      "Epoch 437/10000\n",
      " - 0s - loss: 0.3372 - binary_accuracy: 0.8530 - val_loss: 0.2982 - val_binary_accuracy: 0.8694\n",
      "Epoch 438/10000\n",
      " - 0s - loss: 0.3223 - binary_accuracy: 0.8573 - val_loss: 0.2985 - val_binary_accuracy: 0.8704\n",
      "Epoch 439/10000\n",
      " - 0s - loss: 0.3247 - binary_accuracy: 0.8572 - val_loss: 0.2980 - val_binary_accuracy: 0.8698\n",
      "Epoch 440/10000\n",
      " - 0s - loss: 0.3430 - binary_accuracy: 0.8499 - val_loss: 0.2993 - val_binary_accuracy: 0.8706\n",
      "Epoch 441/10000\n",
      " - 0s - loss: 0.3271 - binary_accuracy: 0.8551 - val_loss: 0.3005 - val_binary_accuracy: 0.8673\n",
      "Epoch 442/10000\n",
      " - 0s - loss: 0.3240 - binary_accuracy: 0.8611 - val_loss: 0.3008 - val_binary_accuracy: 0.8694\n",
      "Epoch 443/10000\n",
      " - 0s - loss: 0.3305 - binary_accuracy: 0.8563 - val_loss: 0.3001 - val_binary_accuracy: 0.8658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 444/10000\n",
      " - 0s - loss: 0.3338 - binary_accuracy: 0.8548 - val_loss: 0.3011 - val_binary_accuracy: 0.8698\n",
      "Epoch 445/10000\n",
      " - 0s - loss: 0.3379 - binary_accuracy: 0.8524 - val_loss: 0.3007 - val_binary_accuracy: 0.8706\n",
      "Epoch 446/10000\n",
      " - 0s - loss: 0.3300 - binary_accuracy: 0.8581 - val_loss: 0.2998 - val_binary_accuracy: 0.8690\n",
      "Epoch 447/10000\n",
      " - 0s - loss: 0.3329 - binary_accuracy: 0.8525 - val_loss: 0.3002 - val_binary_accuracy: 0.8683\n",
      "Epoch 448/10000\n",
      " - 0s - loss: 0.3273 - binary_accuracy: 0.8595 - val_loss: 0.3005 - val_binary_accuracy: 0.8652\n",
      "Epoch 449/10000\n",
      " - 0s - loss: 0.3377 - binary_accuracy: 0.8525 - val_loss: 0.3007 - val_binary_accuracy: 0.8681\n",
      "Epoch 450/10000\n",
      " - 0s - loss: 0.3345 - binary_accuracy: 0.8510 - val_loss: 0.3004 - val_binary_accuracy: 0.8675\n",
      "Epoch 451/10000\n",
      " - 0s - loss: 0.3394 - binary_accuracy: 0.8512 - val_loss: 0.2991 - val_binary_accuracy: 0.8681\n",
      "Epoch 452/10000\n",
      " - 0s - loss: 0.3255 - binary_accuracy: 0.8567 - val_loss: 0.3000 - val_binary_accuracy: 0.8681\n",
      "Epoch 453/10000\n",
      " - 0s - loss: 0.3344 - binary_accuracy: 0.8510 - val_loss: 0.2980 - val_binary_accuracy: 0.8679\n",
      "Epoch 454/10000\n",
      " - 0s - loss: 0.3292 - binary_accuracy: 0.8557 - val_loss: 0.2992 - val_binary_accuracy: 0.8702\n",
      "Epoch 455/10000\n",
      " - 0s - loss: 0.3278 - binary_accuracy: 0.8547 - val_loss: 0.3005 - val_binary_accuracy: 0.8685\n",
      "Epoch 456/10000\n",
      " - 0s - loss: 0.3322 - binary_accuracy: 0.8544 - val_loss: 0.3005 - val_binary_accuracy: 0.8700\n",
      "Epoch 457/10000\n",
      " - 0s - loss: 0.3331 - binary_accuracy: 0.8543 - val_loss: 0.3008 - val_binary_accuracy: 0.8690\n",
      "Epoch 458/10000\n",
      " - 0s - loss: 0.3389 - binary_accuracy: 0.8494 - val_loss: 0.2992 - val_binary_accuracy: 0.8688\n",
      "Epoch 459/10000\n",
      " - 0s - loss: 0.3274 - binary_accuracy: 0.8545 - val_loss: 0.2984 - val_binary_accuracy: 0.8708\n",
      "Epoch 460/10000\n",
      " - 0s - loss: 0.3314 - binary_accuracy: 0.8556 - val_loss: 0.2992 - val_binary_accuracy: 0.8704\n",
      "Epoch 461/10000\n",
      " - 0s - loss: 0.3219 - binary_accuracy: 0.8578 - val_loss: 0.2995 - val_binary_accuracy: 0.8688\n",
      "Epoch 462/10000\n",
      " - 0s - loss: 0.3260 - binary_accuracy: 0.8554 - val_loss: 0.3001 - val_binary_accuracy: 0.8713\n",
      "Epoch 463/10000\n",
      " - 0s - loss: 0.3349 - binary_accuracy: 0.8522 - val_loss: 0.2985 - val_binary_accuracy: 0.8706\n",
      "Epoch 464/10000\n",
      " - 0s - loss: 0.3319 - binary_accuracy: 0.8562 - val_loss: 0.2998 - val_binary_accuracy: 0.8710\n",
      "Epoch 465/10000\n",
      " - 0s - loss: 0.3266 - binary_accuracy: 0.8584 - val_loss: 0.3004 - val_binary_accuracy: 0.8710\n",
      "Epoch 466/10000\n",
      " - 0s - loss: 0.3235 - binary_accuracy: 0.8565 - val_loss: 0.2995 - val_binary_accuracy: 0.8698\n",
      "Epoch 467/10000\n",
      " - 0s - loss: 0.3242 - binary_accuracy: 0.8579 - val_loss: 0.3005 - val_binary_accuracy: 0.8708\n",
      "Epoch 468/10000\n",
      " - 0s - loss: 0.3231 - binary_accuracy: 0.8577 - val_loss: 0.2990 - val_binary_accuracy: 0.8704\n",
      "Epoch 469/10000\n",
      " - 0s - loss: 0.3317 - binary_accuracy: 0.8527 - val_loss: 0.2980 - val_binary_accuracy: 0.8708\n",
      "Epoch 470/10000\n",
      " - 0s - loss: 0.3331 - binary_accuracy: 0.8545 - val_loss: 0.2989 - val_binary_accuracy: 0.8719\n",
      "Epoch 471/10000\n",
      " - 0s - loss: 0.3337 - binary_accuracy: 0.8527 - val_loss: 0.2997 - val_binary_accuracy: 0.8710\n",
      "Epoch 472/10000\n",
      " - 0s - loss: 0.3286 - binary_accuracy: 0.8544 - val_loss: 0.2992 - val_binary_accuracy: 0.8712\n",
      "Epoch 473/10000\n",
      " - 0s - loss: 0.3277 - binary_accuracy: 0.8560 - val_loss: 0.2983 - val_binary_accuracy: 0.8721\n",
      "Epoch 474/10000\n",
      " - 0s - loss: 0.3276 - binary_accuracy: 0.8558 - val_loss: 0.2973 - val_binary_accuracy: 0.8719\n",
      "Epoch 475/10000\n",
      " - 0s - loss: 0.3337 - binary_accuracy: 0.8559 - val_loss: 0.2985 - val_binary_accuracy: 0.8715\n",
      "Epoch 476/10000\n",
      " - 0s - loss: 0.3279 - binary_accuracy: 0.8551 - val_loss: 0.2985 - val_binary_accuracy: 0.8704\n",
      "Epoch 477/10000\n",
      " - 0s - loss: 0.3378 - binary_accuracy: 0.8516 - val_loss: 0.2988 - val_binary_accuracy: 0.8706\n",
      "Epoch 478/10000\n",
      " - 0s - loss: 0.3384 - binary_accuracy: 0.8527 - val_loss: 0.2991 - val_binary_accuracy: 0.8696\n",
      "Epoch 479/10000\n",
      " - 0s - loss: 0.3289 - binary_accuracy: 0.8553 - val_loss: 0.2986 - val_binary_accuracy: 0.8706\n",
      "Epoch 480/10000\n",
      " - 0s - loss: 0.3236 - binary_accuracy: 0.8585 - val_loss: 0.2996 - val_binary_accuracy: 0.8698\n",
      "Epoch 481/10000\n",
      " - 0s - loss: 0.3259 - binary_accuracy: 0.8587 - val_loss: 0.2983 - val_binary_accuracy: 0.8712\n",
      "Epoch 482/10000\n",
      " - 0s - loss: 0.3324 - binary_accuracy: 0.8527 - val_loss: 0.2986 - val_binary_accuracy: 0.8694\n",
      "Epoch 483/10000\n",
      " - 0s - loss: 0.3276 - binary_accuracy: 0.8559 - val_loss: 0.2983 - val_binary_accuracy: 0.8704\n",
      "Epoch 484/10000\n",
      " - 0s - loss: 0.3317 - binary_accuracy: 0.8512 - val_loss: 0.2980 - val_binary_accuracy: 0.8719\n",
      "Epoch 485/10000\n",
      " - 0s - loss: 0.3248 - binary_accuracy: 0.8572 - val_loss: 0.2971 - val_binary_accuracy: 0.8721\n",
      "Epoch 486/10000\n",
      " - 0s - loss: 0.3305 - binary_accuracy: 0.8541 - val_loss: 0.2978 - val_binary_accuracy: 0.8700\n",
      "Epoch 487/10000\n",
      " - 0s - loss: 0.3236 - binary_accuracy: 0.8567 - val_loss: 0.2968 - val_binary_accuracy: 0.8700\n",
      "Epoch 488/10000\n",
      " - 0s - loss: 0.3219 - binary_accuracy: 0.8584 - val_loss: 0.2996 - val_binary_accuracy: 0.8688\n",
      "Epoch 489/10000\n",
      " - 0s - loss: 0.3236 - binary_accuracy: 0.8572 - val_loss: 0.2986 - val_binary_accuracy: 0.8692\n",
      "Epoch 490/10000\n",
      " - 0s - loss: 0.3261 - binary_accuracy: 0.8553 - val_loss: 0.2988 - val_binary_accuracy: 0.8702\n",
      "Epoch 491/10000\n",
      " - 0s - loss: 0.3316 - binary_accuracy: 0.8531 - val_loss: 0.2984 - val_binary_accuracy: 0.8677\n",
      "Epoch 492/10000\n",
      " - 0s - loss: 0.3336 - binary_accuracy: 0.8560 - val_loss: 0.2986 - val_binary_accuracy: 0.8683\n",
      "Epoch 493/10000\n",
      " - 0s - loss: 0.3312 - binary_accuracy: 0.8523 - val_loss: 0.3003 - val_binary_accuracy: 0.8704\n",
      "Epoch 494/10000\n",
      " - 0s - loss: 0.3273 - binary_accuracy: 0.8556 - val_loss: 0.2998 - val_binary_accuracy: 0.8723\n",
      "Epoch 495/10000\n",
      " - 0s - loss: 0.3250 - binary_accuracy: 0.8577 - val_loss: 0.2991 - val_binary_accuracy: 0.8692\n",
      "Epoch 496/10000\n",
      " - 0s - loss: 0.3282 - binary_accuracy: 0.8542 - val_loss: 0.2973 - val_binary_accuracy: 0.8698\n",
      "Epoch 497/10000\n",
      " - 0s - loss: 0.3324 - binary_accuracy: 0.8558 - val_loss: 0.2981 - val_binary_accuracy: 0.8713\n",
      "Epoch 498/10000\n",
      " - 0s - loss: 0.3410 - binary_accuracy: 0.8498 - val_loss: 0.2987 - val_binary_accuracy: 0.8694\n",
      "Epoch 499/10000\n",
      " - 0s - loss: 0.3267 - binary_accuracy: 0.8602 - val_loss: 0.3005 - val_binary_accuracy: 0.8698\n",
      "Epoch 500/10000\n",
      " - 0s - loss: 0.3205 - binary_accuracy: 0.8600 - val_loss: 0.2995 - val_binary_accuracy: 0.8692\n",
      "Epoch 501/10000\n",
      " - 0s - loss: 0.3327 - binary_accuracy: 0.8536 - val_loss: 0.2992 - val_binary_accuracy: 0.8704\n",
      "Epoch 502/10000\n",
      " - 0s - loss: 0.3264 - binary_accuracy: 0.8557 - val_loss: 0.2994 - val_binary_accuracy: 0.8698\n",
      "Epoch 503/10000\n",
      " - 0s - loss: 0.3187 - binary_accuracy: 0.8583 - val_loss: 0.2978 - val_binary_accuracy: 0.8719\n",
      "Epoch 504/10000\n",
      " - 0s - loss: 0.3264 - binary_accuracy: 0.8613 - val_loss: 0.2983 - val_binary_accuracy: 0.8708\n",
      "Epoch 505/10000\n",
      " - 0s - loss: 0.3291 - binary_accuracy: 0.8558 - val_loss: 0.2987 - val_binary_accuracy: 0.8696\n",
      "Epoch 506/10000\n",
      " - 0s - loss: 0.3315 - binary_accuracy: 0.8559 - val_loss: 0.2987 - val_binary_accuracy: 0.8700\n",
      "Epoch 507/10000\n",
      " - 0s - loss: 0.3310 - binary_accuracy: 0.8553 - val_loss: 0.2986 - val_binary_accuracy: 0.8694\n",
      "Epoch 508/10000\n",
      " - 0s - loss: 0.3355 - binary_accuracy: 0.8553 - val_loss: 0.3006 - val_binary_accuracy: 0.8688\n",
      "Epoch 509/10000\n",
      " - 0s - loss: 0.3212 - binary_accuracy: 0.8589 - val_loss: 0.2993 - val_binary_accuracy: 0.8690\n",
      "Epoch 510/10000\n",
      " - 0s - loss: 0.3283 - binary_accuracy: 0.8558 - val_loss: 0.2998 - val_binary_accuracy: 0.8690\n",
      "Epoch 511/10000\n",
      " - 0s - loss: 0.3244 - binary_accuracy: 0.8567 - val_loss: 0.2994 - val_binary_accuracy: 0.8692\n",
      "Epoch 512/10000\n",
      " - 0s - loss: 0.3381 - binary_accuracy: 0.8519 - val_loss: 0.3021 - val_binary_accuracy: 0.8685\n",
      "Epoch 513/10000\n",
      " - 0s - loss: 0.3328 - binary_accuracy: 0.8559 - val_loss: 0.3005 - val_binary_accuracy: 0.8681\n",
      "Epoch 514/10000\n",
      " - 0s - loss: 0.3305 - binary_accuracy: 0.8514 - val_loss: 0.3007 - val_binary_accuracy: 0.8721\n",
      "Epoch 515/10000\n",
      " - 0s - loss: 0.3228 - binary_accuracy: 0.8614 - val_loss: 0.3008 - val_binary_accuracy: 0.8713\n",
      "Epoch 516/10000\n",
      " - 0s - loss: 0.3309 - binary_accuracy: 0.8552 - val_loss: 0.2994 - val_binary_accuracy: 0.8702\n",
      "Epoch 517/10000\n",
      " - 0s - loss: 0.3334 - binary_accuracy: 0.8507 - val_loss: 0.2989 - val_binary_accuracy: 0.8694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 518/10000\n",
      " - 0s - loss: 0.3264 - binary_accuracy: 0.8565 - val_loss: 0.2988 - val_binary_accuracy: 0.8688\n",
      "Epoch 519/10000\n",
      " - 0s - loss: 0.3228 - binary_accuracy: 0.8573 - val_loss: 0.2991 - val_binary_accuracy: 0.8715\n",
      "Epoch 520/10000\n",
      " - 0s - loss: 0.3306 - binary_accuracy: 0.8549 - val_loss: 0.2984 - val_binary_accuracy: 0.8700\n",
      "Epoch 521/10000\n",
      " - 0s - loss: 0.3307 - binary_accuracy: 0.8543 - val_loss: 0.2977 - val_binary_accuracy: 0.8710\n",
      "Epoch 522/10000\n",
      " - 0s - loss: 0.3380 - binary_accuracy: 0.8503 - val_loss: 0.2998 - val_binary_accuracy: 0.8677\n",
      "Epoch 523/10000\n",
      " - 0s - loss: 0.3286 - binary_accuracy: 0.8561 - val_loss: 0.3011 - val_binary_accuracy: 0.8692\n",
      "Epoch 524/10000\n",
      " - 0s - loss: 0.3286 - binary_accuracy: 0.8563 - val_loss: 0.3015 - val_binary_accuracy: 0.8692\n",
      "Epoch 525/10000\n",
      " - 0s - loss: 0.3277 - binary_accuracy: 0.8543 - val_loss: 0.2993 - val_binary_accuracy: 0.8692\n",
      "Epoch 526/10000\n",
      " - 0s - loss: 0.3314 - binary_accuracy: 0.8533 - val_loss: 0.3019 - val_binary_accuracy: 0.8681\n",
      "Epoch 527/10000\n",
      " - 0s - loss: 0.3242 - binary_accuracy: 0.8564 - val_loss: 0.3000 - val_binary_accuracy: 0.8694\n",
      "Epoch 528/10000\n",
      " - 0s - loss: 0.3350 - binary_accuracy: 0.8530 - val_loss: 0.2988 - val_binary_accuracy: 0.8719\n",
      "Epoch 529/10000\n",
      " - 0s - loss: 0.3275 - binary_accuracy: 0.8559 - val_loss: 0.2994 - val_binary_accuracy: 0.8698\n",
      "Epoch 530/10000\n",
      " - 0s - loss: 0.3287 - binary_accuracy: 0.8554 - val_loss: 0.3016 - val_binary_accuracy: 0.8667\n",
      "Epoch 531/10000\n",
      " - 0s - loss: 0.3325 - binary_accuracy: 0.8563 - val_loss: 0.3007 - val_binary_accuracy: 0.8696\n",
      "Epoch 532/10000\n",
      " - 0s - loss: 0.3324 - binary_accuracy: 0.8536 - val_loss: 0.3013 - val_binary_accuracy: 0.8673\n",
      "Epoch 533/10000\n",
      " - 0s - loss: 0.3342 - binary_accuracy: 0.8514 - val_loss: 0.2998 - val_binary_accuracy: 0.8696\n",
      "Epoch 534/10000\n",
      " - 0s - loss: 0.3280 - binary_accuracy: 0.8529 - val_loss: 0.2998 - val_binary_accuracy: 0.8719\n",
      "Epoch 535/10000\n",
      " - 0s - loss: 0.3202 - binary_accuracy: 0.8596 - val_loss: 0.3000 - val_binary_accuracy: 0.8729\n",
      "Epoch 536/10000\n",
      " - 0s - loss: 0.3316 - binary_accuracy: 0.8539 - val_loss: 0.2997 - val_binary_accuracy: 0.8719\n",
      "Epoch 537/10000\n",
      " - 0s - loss: 0.3272 - binary_accuracy: 0.8528 - val_loss: 0.2993 - val_binary_accuracy: 0.8715\n",
      "Epoch 538/10000\n",
      " - 0s - loss: 0.3267 - binary_accuracy: 0.8576 - val_loss: 0.2985 - val_binary_accuracy: 0.8719\n",
      "Epoch 539/10000\n",
      " - 0s - loss: 0.3387 - binary_accuracy: 0.8546 - val_loss: 0.2991 - val_binary_accuracy: 0.8725\n",
      "Epoch 540/10000\n",
      " - 0s - loss: 0.3269 - binary_accuracy: 0.8543 - val_loss: 0.2979 - val_binary_accuracy: 0.8733\n",
      "Epoch 541/10000\n",
      " - 0s - loss: 0.3270 - binary_accuracy: 0.8526 - val_loss: 0.2986 - val_binary_accuracy: 0.8713\n",
      "Epoch 542/10000\n",
      " - 0s - loss: 0.3332 - binary_accuracy: 0.8548 - val_loss: 0.3013 - val_binary_accuracy: 0.8706\n",
      "Epoch 543/10000\n",
      " - 0s - loss: 0.3298 - binary_accuracy: 0.8533 - val_loss: 0.3004 - val_binary_accuracy: 0.8710\n",
      "Epoch 544/10000\n",
      " - 0s - loss: 0.3272 - binary_accuracy: 0.8571 - val_loss: 0.3004 - val_binary_accuracy: 0.8698\n",
      "Epoch 545/10000\n",
      " - 0s - loss: 0.3218 - binary_accuracy: 0.8584 - val_loss: 0.3005 - val_binary_accuracy: 0.8704\n",
      "Epoch 546/10000\n",
      " - 0s - loss: 0.3217 - binary_accuracy: 0.8588 - val_loss: 0.2996 - val_binary_accuracy: 0.8700\n",
      "Epoch 547/10000\n",
      " - 0s - loss: 0.3348 - binary_accuracy: 0.8533 - val_loss: 0.2988 - val_binary_accuracy: 0.8713\n",
      "Epoch 548/10000\n",
      " - 0s - loss: 0.3245 - binary_accuracy: 0.8590 - val_loss: 0.3011 - val_binary_accuracy: 0.8700\n",
      "Epoch 549/10000\n",
      " - 0s - loss: 0.3247 - binary_accuracy: 0.8580 - val_loss: 0.3008 - val_binary_accuracy: 0.8698\n",
      "Epoch 550/10000\n",
      " - 0s - loss: 0.3240 - binary_accuracy: 0.8604 - val_loss: 0.3011 - val_binary_accuracy: 0.8675\n",
      "Epoch 551/10000\n",
      " - 0s - loss: 0.3259 - binary_accuracy: 0.8565 - val_loss: 0.2998 - val_binary_accuracy: 0.8710\n",
      "Epoch 552/10000\n",
      " - 0s - loss: 0.3183 - binary_accuracy: 0.8596 - val_loss: 0.3002 - val_binary_accuracy: 0.8713\n",
      "Epoch 553/10000\n",
      " - 0s - loss: 0.3336 - binary_accuracy: 0.8535 - val_loss: 0.3002 - val_binary_accuracy: 0.8685\n",
      "Epoch 554/10000\n",
      " - 0s - loss: 0.3269 - binary_accuracy: 0.8563 - val_loss: 0.2984 - val_binary_accuracy: 0.8694\n",
      "Epoch 555/10000\n",
      " - 0s - loss: 0.3302 - binary_accuracy: 0.8566 - val_loss: 0.2988 - val_binary_accuracy: 0.8706\n",
      "Epoch 556/10000\n",
      " - 0s - loss: 0.3287 - binary_accuracy: 0.8543 - val_loss: 0.2995 - val_binary_accuracy: 0.8692\n",
      "Epoch 557/10000\n",
      " - 0s - loss: 0.3360 - binary_accuracy: 0.8502 - val_loss: 0.2988 - val_binary_accuracy: 0.8698\n",
      "Epoch 558/10000\n",
      " - 0s - loss: 0.3267 - binary_accuracy: 0.8583 - val_loss: 0.3015 - val_binary_accuracy: 0.8694\n",
      "Epoch 559/10000\n",
      " - 0s - loss: 0.3308 - binary_accuracy: 0.8559 - val_loss: 0.3001 - val_binary_accuracy: 0.8700\n",
      "Epoch 560/10000\n",
      " - 0s - loss: 0.3340 - binary_accuracy: 0.8531 - val_loss: 0.3001 - val_binary_accuracy: 0.8704\n",
      "Epoch 561/10000\n",
      " - 0s - loss: 0.3337 - binary_accuracy: 0.8521 - val_loss: 0.3002 - val_binary_accuracy: 0.8683\n",
      "Epoch 562/10000\n",
      " - 0s - loss: 0.3235 - binary_accuracy: 0.8572 - val_loss: 0.3003 - val_binary_accuracy: 0.8688\n",
      "Epoch 563/10000\n",
      " - 0s - loss: 0.3208 - binary_accuracy: 0.8593 - val_loss: 0.2998 - val_binary_accuracy: 0.8704\n",
      "Epoch 564/10000\n",
      " - 0s - loss: 0.3245 - binary_accuracy: 0.8579 - val_loss: 0.2998 - val_binary_accuracy: 0.8712\n",
      "Epoch 565/10000\n",
      " - 0s - loss: 0.3263 - binary_accuracy: 0.8577 - val_loss: 0.2984 - val_binary_accuracy: 0.8702\n",
      "Epoch 566/10000\n",
      " - 0s - loss: 0.3191 - binary_accuracy: 0.8577 - val_loss: 0.2995 - val_binary_accuracy: 0.8710\n",
      "Epoch 567/10000\n",
      " - 0s - loss: 0.3317 - binary_accuracy: 0.8559 - val_loss: 0.3005 - val_binary_accuracy: 0.8717\n",
      "Epoch 568/10000\n",
      " - 0s - loss: 0.3345 - binary_accuracy: 0.8542 - val_loss: 0.2998 - val_binary_accuracy: 0.8727\n",
      "Epoch 569/10000\n",
      " - 0s - loss: 0.3213 - binary_accuracy: 0.8579 - val_loss: 0.2988 - val_binary_accuracy: 0.8708\n",
      "Epoch 570/10000\n",
      " - 0s - loss: 0.3253 - binary_accuracy: 0.8585 - val_loss: 0.3005 - val_binary_accuracy: 0.8725\n",
      "Epoch 571/10000\n",
      " - 0s - loss: 0.3280 - binary_accuracy: 0.8555 - val_loss: 0.3002 - val_binary_accuracy: 0.8725\n",
      "Epoch 572/10000\n",
      " - 0s - loss: 0.3318 - binary_accuracy: 0.8560 - val_loss: 0.3008 - val_binary_accuracy: 0.8717\n",
      "Epoch 573/10000\n",
      " - 0s - loss: 0.3239 - binary_accuracy: 0.8568 - val_loss: 0.3013 - val_binary_accuracy: 0.8721\n",
      "Epoch 574/10000\n",
      " - 0s - loss: 0.3244 - binary_accuracy: 0.8571 - val_loss: 0.3001 - val_binary_accuracy: 0.8702\n",
      "Epoch 575/10000\n",
      " - 0s - loss: 0.3334 - binary_accuracy: 0.8531 - val_loss: 0.2996 - val_binary_accuracy: 0.8731\n",
      "Epoch 576/10000\n",
      " - 0s - loss: 0.3244 - binary_accuracy: 0.8602 - val_loss: 0.2990 - val_binary_accuracy: 0.8715\n",
      "Epoch 577/10000\n",
      " - 0s - loss: 0.3325 - binary_accuracy: 0.8531 - val_loss: 0.2994 - val_binary_accuracy: 0.8706\n",
      "Epoch 578/10000\n",
      " - 0s - loss: 0.3281 - binary_accuracy: 0.8552 - val_loss: 0.2987 - val_binary_accuracy: 0.8729\n",
      "Epoch 579/10000\n",
      " - 0s - loss: 0.3302 - binary_accuracy: 0.8540 - val_loss: 0.2997 - val_binary_accuracy: 0.8715\n",
      "Epoch 580/10000\n",
      " - 0s - loss: 0.3189 - binary_accuracy: 0.8597 - val_loss: 0.3003 - val_binary_accuracy: 0.8731\n",
      "Epoch 581/10000\n",
      " - 0s - loss: 0.3256 - binary_accuracy: 0.8545 - val_loss: 0.3002 - val_binary_accuracy: 0.8696\n",
      "Epoch 582/10000\n",
      " - 0s - loss: 0.3314 - binary_accuracy: 0.8566 - val_loss: 0.3001 - val_binary_accuracy: 0.8717\n",
      "Epoch 583/10000\n",
      " - 0s - loss: 0.3374 - binary_accuracy: 0.8500 - val_loss: 0.3008 - val_binary_accuracy: 0.8721\n",
      "Epoch 584/10000\n",
      " - 0s - loss: 0.3343 - binary_accuracy: 0.8526 - val_loss: 0.3002 - val_binary_accuracy: 0.8704\n",
      "Epoch 585/10000\n",
      " - 0s - loss: 0.3270 - binary_accuracy: 0.8542 - val_loss: 0.2984 - val_binary_accuracy: 0.8727\n",
      "Epoch 586/10000\n",
      " - 0s - loss: 0.3288 - binary_accuracy: 0.8554 - val_loss: 0.2988 - val_binary_accuracy: 0.8717\n",
      "Epoch 587/10000\n",
      " - 0s - loss: 0.3325 - binary_accuracy: 0.8515 - val_loss: 0.2983 - val_binary_accuracy: 0.8740\n",
      "Epoch 588/10000\n",
      " - 0s - loss: 0.3246 - binary_accuracy: 0.8559 - val_loss: 0.2986 - val_binary_accuracy: 0.8717\n",
      "Epoch 589/10000\n",
      " - 0s - loss: 0.3267 - binary_accuracy: 0.8556 - val_loss: 0.3000 - val_binary_accuracy: 0.8713\n",
      "Epoch 590/10000\n",
      " - 0s - loss: 0.3279 - binary_accuracy: 0.8565 - val_loss: 0.3000 - val_binary_accuracy: 0.8740\n",
      "Epoch 591/10000\n",
      " - 0s - loss: 0.3259 - binary_accuracy: 0.8578 - val_loss: 0.2997 - val_binary_accuracy: 0.8742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 592/10000\n",
      " - 0s - loss: 0.3257 - binary_accuracy: 0.8558 - val_loss: 0.2990 - val_binary_accuracy: 0.8721\n",
      "Epoch 593/10000\n",
      " - 0s - loss: 0.3230 - binary_accuracy: 0.8616 - val_loss: 0.2987 - val_binary_accuracy: 0.8713\n",
      "Epoch 594/10000\n",
      " - 0s - loss: 0.3249 - binary_accuracy: 0.8567 - val_loss: 0.2999 - val_binary_accuracy: 0.8694\n",
      "Epoch 595/10000\n",
      " - 0s - loss: 0.3246 - binary_accuracy: 0.8579 - val_loss: 0.3000 - val_binary_accuracy: 0.8712\n",
      "Epoch 596/10000\n",
      " - 0s - loss: 0.3334 - binary_accuracy: 0.8560 - val_loss: 0.3003 - val_binary_accuracy: 0.8719\n",
      "Epoch 597/10000\n",
      " - 0s - loss: 0.3394 - binary_accuracy: 0.8538 - val_loss: 0.2997 - val_binary_accuracy: 0.8710\n",
      "Epoch 598/10000\n",
      " - 0s - loss: 0.3243 - binary_accuracy: 0.8576 - val_loss: 0.2979 - val_binary_accuracy: 0.8706\n",
      "Epoch 599/10000\n",
      " - 0s - loss: 0.3277 - binary_accuracy: 0.8561 - val_loss: 0.2993 - val_binary_accuracy: 0.8717\n",
      "Epoch 600/10000\n",
      " - 0s - loss: 0.3328 - binary_accuracy: 0.8531 - val_loss: 0.2989 - val_binary_accuracy: 0.8694\n",
      "Epoch 601/10000\n",
      " - 0s - loss: 0.3276 - binary_accuracy: 0.8572 - val_loss: 0.3002 - val_binary_accuracy: 0.8696\n",
      "Epoch 602/10000\n",
      " - 0s - loss: 0.3242 - binary_accuracy: 0.8617 - val_loss: 0.2998 - val_binary_accuracy: 0.8694\n",
      "Epoch 603/10000\n",
      " - 0s - loss: 0.3352 - binary_accuracy: 0.8527 - val_loss: 0.3005 - val_binary_accuracy: 0.8725\n",
      "Epoch 604/10000\n",
      " - 0s - loss: 0.3275 - binary_accuracy: 0.8561 - val_loss: 0.3000 - val_binary_accuracy: 0.8704\n",
      "Epoch 605/10000\n",
      " - 0s - loss: 0.3286 - binary_accuracy: 0.8541 - val_loss: 0.3003 - val_binary_accuracy: 0.8713\n",
      "Epoch 606/10000\n",
      " - 0s - loss: 0.3269 - binary_accuracy: 0.8543 - val_loss: 0.2994 - val_binary_accuracy: 0.8717\n",
      "Epoch 607/10000\n",
      " - 0s - loss: 0.3250 - binary_accuracy: 0.8573 - val_loss: 0.3004 - val_binary_accuracy: 0.8692\n",
      "Epoch 608/10000\n",
      " - 0s - loss: 0.3246 - binary_accuracy: 0.8571 - val_loss: 0.2998 - val_binary_accuracy: 0.8704\n",
      "Epoch 609/10000\n",
      " - 0s - loss: 0.3251 - binary_accuracy: 0.8574 - val_loss: 0.3008 - val_binary_accuracy: 0.8696\n",
      "Epoch 610/10000\n",
      " - 0s - loss: 0.3275 - binary_accuracy: 0.8572 - val_loss: 0.2998 - val_binary_accuracy: 0.8727\n",
      "Epoch 611/10000\n",
      " - 0s - loss: 0.3281 - binary_accuracy: 0.8574 - val_loss: 0.2983 - val_binary_accuracy: 0.8744\n",
      "Epoch 612/10000\n",
      " - 0s - loss: 0.3292 - binary_accuracy: 0.8556 - val_loss: 0.2985 - val_binary_accuracy: 0.8719\n",
      "Epoch 613/10000\n",
      " - 0s - loss: 0.3286 - binary_accuracy: 0.8554 - val_loss: 0.3002 - val_binary_accuracy: 0.8735\n",
      "Epoch 614/10000\n",
      " - 0s - loss: 0.3332 - binary_accuracy: 0.8536 - val_loss: 0.3009 - val_binary_accuracy: 0.8727\n",
      "Epoch 615/10000\n",
      " - 0s - loss: 0.3260 - binary_accuracy: 0.8591 - val_loss: 0.2996 - val_binary_accuracy: 0.8729\n",
      "Epoch 616/10000\n",
      " - 0s - loss: 0.3180 - binary_accuracy: 0.8608 - val_loss: 0.2998 - val_binary_accuracy: 0.8744\n",
      "Epoch 617/10000\n",
      " - 0s - loss: 0.3222 - binary_accuracy: 0.8608 - val_loss: 0.3003 - val_binary_accuracy: 0.8742\n",
      "Epoch 618/10000\n",
      " - 0s - loss: 0.3200 - binary_accuracy: 0.8595 - val_loss: 0.3005 - val_binary_accuracy: 0.8731\n",
      "Epoch 619/10000\n",
      " - 0s - loss: 0.3169 - binary_accuracy: 0.8599 - val_loss: 0.3006 - val_binary_accuracy: 0.8723\n",
      "Epoch 620/10000\n",
      " - 0s - loss: 0.3199 - binary_accuracy: 0.8591 - val_loss: 0.2997 - val_binary_accuracy: 0.8721\n",
      "Epoch 621/10000\n",
      " - 0s - loss: 0.3267 - binary_accuracy: 0.8576 - val_loss: 0.2991 - val_binary_accuracy: 0.8704\n",
      "Epoch 622/10000\n",
      " - 0s - loss: 0.3319 - binary_accuracy: 0.8536 - val_loss: 0.3014 - val_binary_accuracy: 0.8708\n",
      "Epoch 623/10000\n",
      " - 0s - loss: 0.3227 - binary_accuracy: 0.8585 - val_loss: 0.3008 - val_binary_accuracy: 0.8704\n",
      "Epoch 624/10000\n",
      " - 0s - loss: 0.3353 - binary_accuracy: 0.8562 - val_loss: 0.3004 - val_binary_accuracy: 0.8685\n",
      "Epoch 625/10000\n",
      " - 0s - loss: 0.3398 - binary_accuracy: 0.8533 - val_loss: 0.3006 - val_binary_accuracy: 0.8685\n",
      "Epoch 626/10000\n",
      " - 0s - loss: 0.3313 - binary_accuracy: 0.8579 - val_loss: 0.3011 - val_binary_accuracy: 0.8698\n",
      "Epoch 627/10000\n",
      " - 0s - loss: 0.3258 - binary_accuracy: 0.8570 - val_loss: 0.3017 - val_binary_accuracy: 0.8706\n",
      "Epoch 628/10000\n",
      " - 0s - loss: 0.3287 - binary_accuracy: 0.8572 - val_loss: 0.3004 - val_binary_accuracy: 0.8727\n",
      "Epoch 629/10000\n",
      " - 0s - loss: 0.3320 - binary_accuracy: 0.8517 - val_loss: 0.2986 - val_binary_accuracy: 0.8721\n",
      "Epoch 630/10000\n",
      " - 0s - loss: 0.3227 - binary_accuracy: 0.8586 - val_loss: 0.3002 - val_binary_accuracy: 0.8723\n",
      "Epoch 631/10000\n",
      " - 0s - loss: 0.3274 - binary_accuracy: 0.8545 - val_loss: 0.2999 - val_binary_accuracy: 0.8700\n",
      "Epoch 632/10000\n",
      " - 0s - loss: 0.3186 - binary_accuracy: 0.8612 - val_loss: 0.2989 - val_binary_accuracy: 0.8723\n",
      "Epoch 633/10000\n",
      " - 0s - loss: 0.3391 - binary_accuracy: 0.8491 - val_loss: 0.2997 - val_binary_accuracy: 0.8737\n",
      "Epoch 634/10000\n",
      " - 0s - loss: 0.3237 - binary_accuracy: 0.8603 - val_loss: 0.2999 - val_binary_accuracy: 0.8704\n",
      "Epoch 635/10000\n",
      " - 0s - loss: 0.3273 - binary_accuracy: 0.8579 - val_loss: 0.2983 - val_binary_accuracy: 0.8733\n",
      "Epoch 636/10000\n",
      " - 0s - loss: 0.3251 - binary_accuracy: 0.8552 - val_loss: 0.3000 - val_binary_accuracy: 0.8738\n",
      "Epoch 637/10000\n",
      " - 0s - loss: 0.3298 - binary_accuracy: 0.8541 - val_loss: 0.2996 - val_binary_accuracy: 0.8738\n",
      "Epoch 638/10000\n",
      " - 0s - loss: 0.3338 - binary_accuracy: 0.8562 - val_loss: 0.2999 - val_binary_accuracy: 0.8717\n",
      "Epoch 639/10000\n",
      " - 0s - loss: 0.3345 - binary_accuracy: 0.8555 - val_loss: 0.3002 - val_binary_accuracy: 0.8710\n",
      "Epoch 640/10000\n",
      " - 0s - loss: 0.3229 - binary_accuracy: 0.8602 - val_loss: 0.2997 - val_binary_accuracy: 0.8706\n",
      "Epoch 641/10000\n",
      " - 0s - loss: 0.3337 - binary_accuracy: 0.8564 - val_loss: 0.3003 - val_binary_accuracy: 0.8696\n",
      "Epoch 642/10000\n",
      " - 0s - loss: 0.3202 - binary_accuracy: 0.8562 - val_loss: 0.2994 - val_binary_accuracy: 0.8706\n",
      "Epoch 643/10000\n",
      " - 0s - loss: 0.3268 - binary_accuracy: 0.8567 - val_loss: 0.2997 - val_binary_accuracy: 0.8710\n",
      "Epoch 644/10000\n",
      " - 0s - loss: 0.3275 - binary_accuracy: 0.8574 - val_loss: 0.3000 - val_binary_accuracy: 0.8690\n",
      "Epoch 645/10000\n",
      " - 0s - loss: 0.3282 - binary_accuracy: 0.8581 - val_loss: 0.2989 - val_binary_accuracy: 0.8710\n",
      "Epoch 646/10000\n",
      " - 0s - loss: 0.3244 - binary_accuracy: 0.8559 - val_loss: 0.3008 - val_binary_accuracy: 0.8704\n",
      "Epoch 647/10000\n",
      " - 0s - loss: 0.3340 - binary_accuracy: 0.8541 - val_loss: 0.3009 - val_binary_accuracy: 0.8692\n",
      "Epoch 648/10000\n",
      " - 0s - loss: 0.3210 - binary_accuracy: 0.8598 - val_loss: 0.2997 - val_binary_accuracy: 0.8706\n",
      "Epoch 649/10000\n",
      " - 0s - loss: 0.3218 - binary_accuracy: 0.8619 - val_loss: 0.3006 - val_binary_accuracy: 0.8729\n",
      "Epoch 650/10000\n",
      " - 0s - loss: 0.3293 - binary_accuracy: 0.8542 - val_loss: 0.2991 - val_binary_accuracy: 0.8740\n",
      "Epoch 651/10000\n",
      " - 0s - loss: 0.3315 - binary_accuracy: 0.8564 - val_loss: 0.3001 - val_binary_accuracy: 0.8723\n",
      "Epoch 652/10000\n",
      " - 0s - loss: 0.3314 - binary_accuracy: 0.8548 - val_loss: 0.2998 - val_binary_accuracy: 0.8708\n",
      "Epoch 653/10000\n",
      " - 0s - loss: 0.3230 - binary_accuracy: 0.8594 - val_loss: 0.2983 - val_binary_accuracy: 0.8731\n",
      "Epoch 654/10000\n",
      " - 0s - loss: 0.3213 - binary_accuracy: 0.8588 - val_loss: 0.2991 - val_binary_accuracy: 0.8708\n",
      "Epoch 655/10000\n",
      " - 0s - loss: 0.3328 - binary_accuracy: 0.8548 - val_loss: 0.2994 - val_binary_accuracy: 0.8717\n",
      "Epoch 656/10000\n",
      " - 0s - loss: 0.3213 - binary_accuracy: 0.8609 - val_loss: 0.2982 - val_binary_accuracy: 0.8721\n",
      "Epoch 657/10000\n",
      " - 0s - loss: 0.3218 - binary_accuracy: 0.8608 - val_loss: 0.2987 - val_binary_accuracy: 0.8729\n",
      "Epoch 658/10000\n",
      " - 0s - loss: 0.3243 - binary_accuracy: 0.8603 - val_loss: 0.2984 - val_binary_accuracy: 0.8702\n",
      "Epoch 659/10000\n",
      " - 0s - loss: 0.3254 - binary_accuracy: 0.8578 - val_loss: 0.2974 - val_binary_accuracy: 0.8708\n",
      "Epoch 660/10000\n",
      " - 0s - loss: 0.3286 - binary_accuracy: 0.8542 - val_loss: 0.2974 - val_binary_accuracy: 0.8708\n",
      "Epoch 661/10000\n",
      " - 0s - loss: 0.3279 - binary_accuracy: 0.8604 - val_loss: 0.2988 - val_binary_accuracy: 0.8713\n",
      "Epoch 662/10000\n",
      " - 0s - loss: 0.3240 - binary_accuracy: 0.8594 - val_loss: 0.3001 - val_binary_accuracy: 0.8708\n",
      "Epoch 663/10000\n",
      " - 0s - loss: 0.3231 - binary_accuracy: 0.8586 - val_loss: 0.2992 - val_binary_accuracy: 0.8681\n",
      "Epoch 664/10000\n",
      " - 0s - loss: 0.3217 - binary_accuracy: 0.8599 - val_loss: 0.3001 - val_binary_accuracy: 0.8688\n",
      "Epoch 665/10000\n",
      " - 0s - loss: 0.3185 - binary_accuracy: 0.8621 - val_loss: 0.2985 - val_binary_accuracy: 0.8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 666/10000\n",
      " - 0s - loss: 0.3273 - binary_accuracy: 0.8560 - val_loss: 0.2998 - val_binary_accuracy: 0.8704\n",
      "Epoch 667/10000\n",
      " - 0s - loss: 0.3332 - binary_accuracy: 0.8555 - val_loss: 0.2992 - val_binary_accuracy: 0.8700\n",
      "Epoch 668/10000\n",
      " - 0s - loss: 0.3147 - binary_accuracy: 0.8605 - val_loss: 0.2999 - val_binary_accuracy: 0.8690\n",
      "Epoch 669/10000\n",
      " - 0s - loss: 0.3285 - binary_accuracy: 0.8577 - val_loss: 0.2993 - val_binary_accuracy: 0.8706\n",
      "Epoch 670/10000\n",
      " - 0s - loss: 0.3315 - binary_accuracy: 0.8548 - val_loss: 0.3001 - val_binary_accuracy: 0.8715\n",
      "Epoch 671/10000\n",
      " - 0s - loss: 0.3304 - binary_accuracy: 0.8542 - val_loss: 0.3004 - val_binary_accuracy: 0.8717\n",
      "Epoch 672/10000\n",
      " - 0s - loss: 0.3213 - binary_accuracy: 0.8590 - val_loss: 0.2994 - val_binary_accuracy: 0.8710\n",
      "Epoch 673/10000\n",
      " - 0s - loss: 0.3208 - binary_accuracy: 0.8618 - val_loss: 0.2994 - val_binary_accuracy: 0.8725\n",
      "Epoch 674/10000\n",
      " - 0s - loss: 0.3259 - binary_accuracy: 0.8553 - val_loss: 0.2994 - val_binary_accuracy: 0.8733\n",
      "Epoch 675/10000\n",
      " - 0s - loss: 0.3298 - binary_accuracy: 0.8525 - val_loss: 0.2993 - val_binary_accuracy: 0.8700\n",
      "Epoch 676/10000\n",
      " - 0s - loss: 0.3237 - binary_accuracy: 0.8572 - val_loss: 0.2990 - val_binary_accuracy: 0.8702\n",
      "Epoch 677/10000\n",
      " - 0s - loss: 0.3195 - binary_accuracy: 0.8614 - val_loss: 0.2986 - val_binary_accuracy: 0.8723\n",
      "Epoch 678/10000\n",
      " - 0s - loss: 0.3172 - binary_accuracy: 0.8604 - val_loss: 0.2993 - val_binary_accuracy: 0.8713\n",
      "Epoch 679/10000\n",
      " - 0s - loss: 0.3251 - binary_accuracy: 0.8562 - val_loss: 0.2986 - val_binary_accuracy: 0.8717\n",
      "Epoch 680/10000\n",
      " - 0s - loss: 0.3300 - binary_accuracy: 0.8574 - val_loss: 0.2998 - val_binary_accuracy: 0.8715\n",
      "Epoch 681/10000\n",
      " - 0s - loss: 0.3305 - binary_accuracy: 0.8550 - val_loss: 0.3004 - val_binary_accuracy: 0.8717\n",
      "Epoch 682/10000\n",
      " - 0s - loss: 0.3226 - binary_accuracy: 0.8594 - val_loss: 0.3009 - val_binary_accuracy: 0.8717\n",
      "Epoch 683/10000\n",
      " - 0s - loss: 0.3239 - binary_accuracy: 0.8571 - val_loss: 0.2993 - val_binary_accuracy: 0.8725\n",
      "Epoch 684/10000\n",
      " - 0s - loss: 0.3264 - binary_accuracy: 0.8605 - val_loss: 0.2997 - val_binary_accuracy: 0.8727\n",
      "Epoch 685/10000\n",
      " - 0s - loss: 0.3304 - binary_accuracy: 0.8588 - val_loss: 0.2992 - val_binary_accuracy: 0.8725\n",
      "Epoch 686/10000\n",
      " - 0s - loss: 0.3255 - binary_accuracy: 0.8578 - val_loss: 0.2999 - val_binary_accuracy: 0.8698\n",
      "Epoch 687/10000\n",
      " - 0s - loss: 0.3279 - binary_accuracy: 0.8559 - val_loss: 0.2989 - val_binary_accuracy: 0.8704\n",
      "Epoch 688/10000\n",
      " - 0s - loss: 0.3299 - binary_accuracy: 0.8536 - val_loss: 0.3011 - val_binary_accuracy: 0.8715\n",
      "Epoch 689/10000\n",
      " - 0s - loss: 0.3223 - binary_accuracy: 0.8583 - val_loss: 0.3015 - val_binary_accuracy: 0.8717\n",
      "Epoch 690/10000\n",
      " - 0s - loss: 0.3366 - binary_accuracy: 0.8530 - val_loss: 0.3021 - val_binary_accuracy: 0.8727\n",
      "Epoch 691/10000\n",
      " - 0s - loss: 0.3335 - binary_accuracy: 0.8543 - val_loss: 0.3012 - val_binary_accuracy: 0.8719\n",
      "Epoch 692/10000\n",
      " - 0s - loss: 0.3289 - binary_accuracy: 0.8554 - val_loss: 0.3002 - val_binary_accuracy: 0.8729\n",
      "Epoch 693/10000\n",
      " - 0s - loss: 0.3284 - binary_accuracy: 0.8548 - val_loss: 0.3014 - val_binary_accuracy: 0.8727\n",
      "Epoch 694/10000\n",
      " - 0s - loss: 0.3269 - binary_accuracy: 0.8578 - val_loss: 0.3005 - val_binary_accuracy: 0.8740\n",
      "Epoch 695/10000\n",
      " - 0s - loss: 0.3277 - binary_accuracy: 0.8563 - val_loss: 0.3001 - val_binary_accuracy: 0.8712\n",
      "Epoch 696/10000\n",
      " - 0s - loss: 0.3294 - binary_accuracy: 0.8549 - val_loss: 0.2999 - val_binary_accuracy: 0.8712\n",
      "Epoch 697/10000\n",
      " - 0s - loss: 0.3255 - binary_accuracy: 0.8578 - val_loss: 0.3003 - val_binary_accuracy: 0.8717\n",
      "Epoch 698/10000\n",
      " - 0s - loss: 0.3242 - binary_accuracy: 0.8592 - val_loss: 0.2999 - val_binary_accuracy: 0.8713\n",
      "Epoch 699/10000\n",
      " - 0s - loss: 0.3303 - binary_accuracy: 0.8551 - val_loss: 0.3014 - val_binary_accuracy: 0.8733\n",
      "Epoch 700/10000\n",
      " - 0s - loss: 0.3275 - binary_accuracy: 0.8555 - val_loss: 0.3001 - val_binary_accuracy: 0.8717\n",
      "Epoch 701/10000\n",
      " - 0s - loss: 0.3274 - binary_accuracy: 0.8557 - val_loss: 0.2990 - val_binary_accuracy: 0.8731\n",
      "Epoch 702/10000\n",
      " - 0s - loss: 0.3249 - binary_accuracy: 0.8576 - val_loss: 0.3000 - val_binary_accuracy: 0.8712\n",
      "Epoch 703/10000\n",
      " - 0s - loss: 0.3294 - binary_accuracy: 0.8570 - val_loss: 0.2994 - val_binary_accuracy: 0.8706\n",
      "Epoch 704/10000\n",
      " - 0s - loss: 0.3261 - binary_accuracy: 0.8588 - val_loss: 0.3013 - val_binary_accuracy: 0.8712\n",
      "Epoch 705/10000\n",
      " - 0s - loss: 0.3202 - binary_accuracy: 0.8589 - val_loss: 0.3000 - val_binary_accuracy: 0.8710\n",
      "Epoch 706/10000\n",
      " - 0s - loss: 0.3333 - binary_accuracy: 0.8518 - val_loss: 0.3000 - val_binary_accuracy: 0.8696\n",
      "Epoch 707/10000\n",
      " - 0s - loss: 0.3244 - binary_accuracy: 0.8565 - val_loss: 0.2989 - val_binary_accuracy: 0.8710\n",
      "Epoch 708/10000\n",
      " - 0s - loss: 0.3314 - binary_accuracy: 0.8559 - val_loss: 0.3000 - val_binary_accuracy: 0.8708\n",
      "Epoch 709/10000\n",
      " - 0s - loss: 0.3276 - binary_accuracy: 0.8576 - val_loss: 0.2999 - val_binary_accuracy: 0.8706\n",
      "Epoch 710/10000\n",
      " - 0s - loss: 0.3263 - binary_accuracy: 0.8582 - val_loss: 0.2987 - val_binary_accuracy: 0.8735\n",
      "Epoch 711/10000\n",
      " - 0s - loss: 0.3273 - binary_accuracy: 0.8592 - val_loss: 0.2986 - val_binary_accuracy: 0.8721\n",
      "Epoch 712/10000\n",
      " - 0s - loss: 0.3295 - binary_accuracy: 0.8537 - val_loss: 0.2990 - val_binary_accuracy: 0.8729\n",
      "Epoch 713/10000\n",
      " - 0s - loss: 0.3261 - binary_accuracy: 0.8580 - val_loss: 0.2989 - val_binary_accuracy: 0.8704\n",
      "Epoch 714/10000\n",
      " - 0s - loss: 0.3187 - binary_accuracy: 0.8573 - val_loss: 0.2995 - val_binary_accuracy: 0.8710\n",
      "Epoch 715/10000\n",
      " - 0s - loss: 0.3210 - binary_accuracy: 0.8596 - val_loss: 0.2984 - val_binary_accuracy: 0.8750\n",
      "Epoch 716/10000\n",
      " - 0s - loss: 0.3243 - binary_accuracy: 0.8571 - val_loss: 0.2988 - val_binary_accuracy: 0.8723\n",
      "Epoch 717/10000\n",
      " - 0s - loss: 0.3370 - binary_accuracy: 0.8523 - val_loss: 0.2988 - val_binary_accuracy: 0.8702\n",
      "Epoch 718/10000\n",
      " - 0s - loss: 0.3263 - binary_accuracy: 0.8592 - val_loss: 0.2983 - val_binary_accuracy: 0.8717\n",
      "Epoch 719/10000\n",
      " - 0s - loss: 0.3252 - binary_accuracy: 0.8602 - val_loss: 0.2981 - val_binary_accuracy: 0.8717\n",
      "Epoch 720/10000\n",
      " - 0s - loss: 0.3181 - binary_accuracy: 0.8600 - val_loss: 0.2976 - val_binary_accuracy: 0.8712\n",
      "Epoch 721/10000\n",
      " - 0s - loss: 0.3211 - binary_accuracy: 0.8581 - val_loss: 0.2981 - val_binary_accuracy: 0.8715\n",
      "Epoch 722/10000\n",
      " - 0s - loss: 0.3235 - binary_accuracy: 0.8592 - val_loss: 0.2976 - val_binary_accuracy: 0.8721\n",
      "Epoch 723/10000\n",
      " - 0s - loss: 0.3297 - binary_accuracy: 0.8548 - val_loss: 0.2986 - val_binary_accuracy: 0.8706\n",
      "Epoch 724/10000\n",
      " - 0s - loss: 0.3238 - binary_accuracy: 0.8556 - val_loss: 0.2979 - val_binary_accuracy: 0.8717\n",
      "Epoch 725/10000\n",
      " - 0s - loss: 0.3238 - binary_accuracy: 0.8577 - val_loss: 0.2989 - val_binary_accuracy: 0.8727\n",
      "Epoch 726/10000\n",
      " - 0s - loss: 0.3291 - binary_accuracy: 0.8568 - val_loss: 0.2992 - val_binary_accuracy: 0.8731\n",
      "Epoch 727/10000\n",
      " - 0s - loss: 0.3248 - binary_accuracy: 0.8574 - val_loss: 0.2986 - val_binary_accuracy: 0.8727\n",
      "Epoch 728/10000\n",
      " - 0s - loss: 0.3274 - binary_accuracy: 0.8571 - val_loss: 0.2989 - val_binary_accuracy: 0.8710\n",
      "Epoch 729/10000\n",
      " - 0s - loss: 0.3250 - binary_accuracy: 0.8582 - val_loss: 0.2994 - val_binary_accuracy: 0.8683\n",
      "Epoch 730/10000\n",
      " - 0s - loss: 0.3347 - binary_accuracy: 0.8564 - val_loss: 0.2994 - val_binary_accuracy: 0.8702\n",
      "Epoch 731/10000\n",
      " - 0s - loss: 0.3245 - binary_accuracy: 0.8567 - val_loss: 0.2995 - val_binary_accuracy: 0.8710\n",
      "Epoch 732/10000\n",
      " - 0s - loss: 0.3238 - binary_accuracy: 0.8576 - val_loss: 0.2991 - val_binary_accuracy: 0.8702\n",
      "Epoch 733/10000\n",
      " - 0s - loss: 0.3207 - binary_accuracy: 0.8628 - val_loss: 0.3002 - val_binary_accuracy: 0.8700\n",
      "Epoch 734/10000\n",
      " - 0s - loss: 0.3292 - binary_accuracy: 0.8567 - val_loss: 0.2997 - val_binary_accuracy: 0.8692\n",
      "Epoch 735/10000\n",
      " - 0s - loss: 0.3266 - binary_accuracy: 0.8588 - val_loss: 0.2991 - val_binary_accuracy: 0.8708\n",
      "Epoch 736/10000\n",
      " - 0s - loss: 0.3341 - binary_accuracy: 0.8537 - val_loss: 0.3001 - val_binary_accuracy: 0.8704\n",
      "Epoch 737/10000\n",
      " - 0s - loss: 0.3196 - binary_accuracy: 0.8593 - val_loss: 0.2984 - val_binary_accuracy: 0.8710\n",
      "Epoch 738/10000\n",
      " - 0s - loss: 0.3195 - binary_accuracy: 0.8587 - val_loss: 0.2973 - val_binary_accuracy: 0.8725\n",
      "Epoch 739/10000\n",
      " - 0s - loss: 0.3271 - binary_accuracy: 0.8568 - val_loss: 0.2983 - val_binary_accuracy: 0.8702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 740/10000\n",
      " - 0s - loss: 0.3266 - binary_accuracy: 0.8585 - val_loss: 0.2986 - val_binary_accuracy: 0.8710\n",
      "Epoch 741/10000\n",
      " - 0s - loss: 0.3349 - binary_accuracy: 0.8542 - val_loss: 0.2974 - val_binary_accuracy: 0.8738\n",
      "Epoch 742/10000\n",
      " - 0s - loss: 0.3311 - binary_accuracy: 0.8546 - val_loss: 0.2983 - val_binary_accuracy: 0.8690\n",
      "Epoch 743/10000\n",
      " - 0s - loss: 0.3279 - binary_accuracy: 0.8581 - val_loss: 0.2987 - val_binary_accuracy: 0.8712\n",
      "Epoch 744/10000\n",
      " - 0s - loss: 0.3275 - binary_accuracy: 0.8574 - val_loss: 0.2998 - val_binary_accuracy: 0.8713\n",
      "Epoch 745/10000\n",
      " - 0s - loss: 0.3244 - binary_accuracy: 0.8557 - val_loss: 0.2992 - val_binary_accuracy: 0.8708\n",
      "Epoch 746/10000\n",
      " - 0s - loss: 0.3128 - binary_accuracy: 0.8637 - val_loss: 0.2974 - val_binary_accuracy: 0.8712\n",
      "Epoch 747/10000\n",
      " - 0s - loss: 0.3270 - binary_accuracy: 0.8568 - val_loss: 0.2970 - val_binary_accuracy: 0.8717\n",
      "Epoch 748/10000\n",
      " - 0s - loss: 0.3258 - binary_accuracy: 0.8588 - val_loss: 0.2991 - val_binary_accuracy: 0.8719\n",
      "Epoch 749/10000\n",
      " - 0s - loss: 0.3309 - binary_accuracy: 0.8514 - val_loss: 0.2992 - val_binary_accuracy: 0.8710\n",
      "Epoch 750/10000\n",
      " - 0s - loss: 0.3243 - binary_accuracy: 0.8577 - val_loss: 0.3000 - val_binary_accuracy: 0.8700\n",
      "Epoch 751/10000\n",
      " - 0s - loss: 0.3289 - binary_accuracy: 0.8571 - val_loss: 0.2993 - val_binary_accuracy: 0.8713\n",
      "Epoch 752/10000\n",
      " - 0s - loss: 0.3226 - binary_accuracy: 0.8621 - val_loss: 0.2990 - val_binary_accuracy: 0.8712\n",
      "Epoch 753/10000\n",
      " - 0s - loss: 0.3242 - binary_accuracy: 0.8558 - val_loss: 0.2997 - val_binary_accuracy: 0.8744\n",
      "Epoch 754/10000\n",
      " - 0s - loss: 0.3163 - binary_accuracy: 0.8625 - val_loss: 0.2981 - val_binary_accuracy: 0.8738\n",
      "Epoch 755/10000\n",
      " - 0s - loss: 0.3314 - binary_accuracy: 0.8535 - val_loss: 0.2988 - val_binary_accuracy: 0.8738\n",
      "Epoch 756/10000\n",
      " - 0s - loss: 0.3322 - binary_accuracy: 0.8531 - val_loss: 0.2978 - val_binary_accuracy: 0.8737\n",
      "Epoch 757/10000\n",
      " - 0s - loss: 0.3273 - binary_accuracy: 0.8566 - val_loss: 0.2978 - val_binary_accuracy: 0.8717\n",
      "Epoch 758/10000\n",
      " - 0s - loss: 0.3301 - binary_accuracy: 0.8584 - val_loss: 0.2984 - val_binary_accuracy: 0.8723\n",
      "Epoch 759/10000\n",
      " - 0s - loss: 0.3227 - binary_accuracy: 0.8578 - val_loss: 0.2974 - val_binary_accuracy: 0.8729\n",
      "Epoch 760/10000\n",
      " - 0s - loss: 0.3215 - binary_accuracy: 0.8584 - val_loss: 0.2968 - val_binary_accuracy: 0.8731\n",
      "Epoch 761/10000\n",
      " - 0s - loss: 0.3261 - binary_accuracy: 0.8590 - val_loss: 0.2967 - val_binary_accuracy: 0.8725\n",
      "Epoch 762/10000\n",
      " - 0s - loss: 0.3298 - binary_accuracy: 0.8555 - val_loss: 0.2972 - val_binary_accuracy: 0.8715\n",
      "Epoch 763/10000\n",
      " - 0s - loss: 0.3241 - binary_accuracy: 0.8587 - val_loss: 0.2966 - val_binary_accuracy: 0.8733\n",
      "Epoch 764/10000\n",
      " - 0s - loss: 0.3320 - binary_accuracy: 0.8538 - val_loss: 0.2964 - val_binary_accuracy: 0.8719\n",
      "Epoch 765/10000\n",
      " - 0s - loss: 0.3274 - binary_accuracy: 0.8558 - val_loss: 0.2960 - val_binary_accuracy: 0.8731\n",
      "Epoch 766/10000\n",
      " - 0s - loss: 0.3300 - binary_accuracy: 0.8541 - val_loss: 0.2972 - val_binary_accuracy: 0.8731\n",
      "Epoch 767/10000\n",
      " - 0s - loss: 0.3222 - binary_accuracy: 0.8573 - val_loss: 0.2985 - val_binary_accuracy: 0.8712\n",
      "Epoch 768/10000\n",
      " - 0s - loss: 0.3275 - binary_accuracy: 0.8554 - val_loss: 0.2983 - val_binary_accuracy: 0.8731\n",
      "Epoch 769/10000\n",
      " - 0s - loss: 0.3343 - binary_accuracy: 0.8507 - val_loss: 0.2977 - val_binary_accuracy: 0.8731\n",
      "Epoch 770/10000\n",
      " - 0s - loss: 0.3209 - binary_accuracy: 0.8618 - val_loss: 0.2971 - val_binary_accuracy: 0.8735\n",
      "Epoch 771/10000\n",
      " - 0s - loss: 0.3183 - binary_accuracy: 0.8629 - val_loss: 0.2976 - val_binary_accuracy: 0.8752\n",
      "Epoch 772/10000\n",
      " - 0s - loss: 0.3264 - binary_accuracy: 0.8559 - val_loss: 0.2982 - val_binary_accuracy: 0.8721\n",
      "Epoch 773/10000\n",
      " - 0s - loss: 0.3224 - binary_accuracy: 0.8594 - val_loss: 0.3001 - val_binary_accuracy: 0.8700\n",
      "Epoch 774/10000\n",
      " - 0s - loss: 0.3282 - binary_accuracy: 0.8593 - val_loss: 0.2997 - val_binary_accuracy: 0.8731\n",
      "Epoch 775/10000\n",
      " - 0s - loss: 0.3225 - binary_accuracy: 0.8579 - val_loss: 0.2978 - val_binary_accuracy: 0.8713\n",
      "Epoch 776/10000\n",
      " - 0s - loss: 0.3193 - binary_accuracy: 0.8598 - val_loss: 0.2978 - val_binary_accuracy: 0.8742\n",
      "Epoch 777/10000\n",
      " - 0s - loss: 0.3280 - binary_accuracy: 0.8538 - val_loss: 0.2966 - val_binary_accuracy: 0.8731\n",
      "Epoch 778/10000\n",
      " - 0s - loss: 0.3236 - binary_accuracy: 0.8577 - val_loss: 0.2959 - val_binary_accuracy: 0.8727\n",
      "Epoch 779/10000\n",
      " - 0s - loss: 0.3252 - binary_accuracy: 0.8574 - val_loss: 0.2965 - val_binary_accuracy: 0.8713\n",
      "Epoch 780/10000\n",
      " - 0s - loss: 0.3371 - binary_accuracy: 0.8507 - val_loss: 0.2983 - val_binary_accuracy: 0.8712\n",
      "Epoch 781/10000\n",
      " - 0s - loss: 0.3190 - binary_accuracy: 0.8620 - val_loss: 0.2981 - val_binary_accuracy: 0.8702\n",
      "Epoch 782/10000\n",
      " - 0s - loss: 0.3270 - binary_accuracy: 0.8591 - val_loss: 0.2973 - val_binary_accuracy: 0.8733\n",
      "Epoch 783/10000\n",
      " - 0s - loss: 0.3327 - binary_accuracy: 0.8547 - val_loss: 0.2978 - val_binary_accuracy: 0.8702\n",
      "Epoch 784/10000\n",
      " - 0s - loss: 0.3279 - binary_accuracy: 0.8562 - val_loss: 0.2973 - val_binary_accuracy: 0.8717\n",
      "Epoch 785/10000\n",
      " - 0s - loss: 0.3192 - binary_accuracy: 0.8591 - val_loss: 0.2980 - val_binary_accuracy: 0.8735\n",
      "Epoch 786/10000\n",
      " - 0s - loss: 0.3139 - binary_accuracy: 0.8643 - val_loss: 0.2969 - val_binary_accuracy: 0.8738\n",
      "Epoch 787/10000\n",
      " - 0s - loss: 0.3185 - binary_accuracy: 0.8574 - val_loss: 0.2969 - val_binary_accuracy: 0.8742\n",
      "Epoch 788/10000\n",
      " - 0s - loss: 0.3203 - binary_accuracy: 0.8597 - val_loss: 0.2977 - val_binary_accuracy: 0.8740\n",
      "Epoch 789/10000\n",
      " - 0s - loss: 0.3233 - binary_accuracy: 0.8589 - val_loss: 0.2984 - val_binary_accuracy: 0.8737\n",
      "Epoch 790/10000\n",
      " - 0s - loss: 0.3277 - binary_accuracy: 0.8565 - val_loss: 0.2996 - val_binary_accuracy: 0.8727\n",
      "Epoch 791/10000\n",
      " - 0s - loss: 0.3157 - binary_accuracy: 0.8622 - val_loss: 0.2991 - val_binary_accuracy: 0.8738\n",
      "Epoch 792/10000\n",
      " - 0s - loss: 0.3225 - binary_accuracy: 0.8600 - val_loss: 0.2990 - val_binary_accuracy: 0.8731\n",
      "Epoch 793/10000\n",
      " - 0s - loss: 0.3303 - binary_accuracy: 0.8538 - val_loss: 0.2984 - val_binary_accuracy: 0.8742\n",
      "Epoch 794/10000\n",
      " - 0s - loss: 0.3229 - binary_accuracy: 0.8561 - val_loss: 0.2972 - val_binary_accuracy: 0.8733\n",
      "Epoch 795/10000\n",
      " - 0s - loss: 0.3265 - binary_accuracy: 0.8554 - val_loss: 0.2977 - val_binary_accuracy: 0.8721\n",
      "Epoch 796/10000\n",
      " - 0s - loss: 0.3242 - binary_accuracy: 0.8602 - val_loss: 0.2978 - val_binary_accuracy: 0.8748\n",
      "Epoch 797/10000\n",
      " - 0s - loss: 0.3227 - binary_accuracy: 0.8581 - val_loss: 0.2980 - val_binary_accuracy: 0.8740\n",
      "Epoch 798/10000\n",
      " - 0s - loss: 0.3242 - binary_accuracy: 0.8596 - val_loss: 0.2980 - val_binary_accuracy: 0.8738\n",
      "Epoch 799/10000\n",
      " - 0s - loss: 0.3261 - binary_accuracy: 0.8581 - val_loss: 0.2989 - val_binary_accuracy: 0.8723\n",
      "Epoch 800/10000\n",
      " - 0s - loss: 0.3277 - binary_accuracy: 0.8554 - val_loss: 0.2975 - val_binary_accuracy: 0.8698\n",
      "Epoch 801/10000\n",
      " - 0s - loss: 0.3202 - binary_accuracy: 0.8605 - val_loss: 0.2967 - val_binary_accuracy: 0.8723\n",
      "Epoch 802/10000\n",
      " - 0s - loss: 0.3178 - binary_accuracy: 0.8641 - val_loss: 0.2980 - val_binary_accuracy: 0.8731\n",
      "Epoch 803/10000\n",
      " - 0s - loss: 0.3291 - binary_accuracy: 0.8539 - val_loss: 0.2985 - val_binary_accuracy: 0.8727\n",
      "Epoch 804/10000\n",
      " - 0s - loss: 0.3234 - binary_accuracy: 0.8579 - val_loss: 0.2973 - val_binary_accuracy: 0.8744\n",
      "Epoch 805/10000\n",
      " - 0s - loss: 0.3162 - binary_accuracy: 0.8624 - val_loss: 0.2981 - val_binary_accuracy: 0.8758\n",
      "Epoch 806/10000\n",
      " - 0s - loss: 0.3265 - binary_accuracy: 0.8562 - val_loss: 0.2976 - val_binary_accuracy: 0.8750\n",
      "Epoch 807/10000\n",
      " - 0s - loss: 0.3280 - binary_accuracy: 0.8566 - val_loss: 0.2965 - val_binary_accuracy: 0.8740\n",
      "Epoch 808/10000\n",
      " - 0s - loss: 0.3273 - binary_accuracy: 0.8537 - val_loss: 0.2971 - val_binary_accuracy: 0.8742\n",
      "Epoch 809/10000\n",
      " - 0s - loss: 0.3214 - binary_accuracy: 0.8598 - val_loss: 0.2979 - val_binary_accuracy: 0.8748\n",
      "Epoch 810/10000\n",
      " - 0s - loss: 0.3248 - binary_accuracy: 0.8562 - val_loss: 0.2980 - val_binary_accuracy: 0.8756\n",
      "Epoch 811/10000\n",
      " - 0s - loss: 0.3215 - binary_accuracy: 0.8611 - val_loss: 0.2976 - val_binary_accuracy: 0.8744\n",
      "Epoch 812/10000\n",
      " - 0s - loss: 0.3276 - binary_accuracy: 0.8572 - val_loss: 0.2974 - val_binary_accuracy: 0.8731\n",
      "Epoch 813/10000\n",
      " - 0s - loss: 0.3352 - binary_accuracy: 0.8539 - val_loss: 0.2989 - val_binary_accuracy: 0.8742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 814/10000\n",
      " - 0s - loss: 0.3244 - binary_accuracy: 0.8577 - val_loss: 0.2989 - val_binary_accuracy: 0.8744\n",
      "Epoch 815/10000\n",
      " - 0s - loss: 0.3211 - binary_accuracy: 0.8588 - val_loss: 0.2986 - val_binary_accuracy: 0.8729\n",
      "Epoch 816/10000\n",
      " - 0s - loss: 0.3190 - binary_accuracy: 0.8579 - val_loss: 0.2986 - val_binary_accuracy: 0.8740\n",
      "Epoch 817/10000\n",
      " - 0s - loss: 0.3213 - binary_accuracy: 0.8599 - val_loss: 0.2991 - val_binary_accuracy: 0.8706\n",
      "Epoch 818/10000\n",
      " - 0s - loss: 0.3234 - binary_accuracy: 0.8574 - val_loss: 0.2984 - val_binary_accuracy: 0.8754\n",
      "Epoch 819/10000\n",
      " - 0s - loss: 0.3320 - binary_accuracy: 0.8574 - val_loss: 0.2978 - val_binary_accuracy: 0.8758\n",
      "Epoch 820/10000\n",
      " - 0s - loss: 0.3249 - binary_accuracy: 0.8577 - val_loss: 0.2984 - val_binary_accuracy: 0.8752\n",
      "Epoch 821/10000\n",
      " - 0s - loss: 0.3246 - binary_accuracy: 0.8556 - val_loss: 0.2979 - val_binary_accuracy: 0.8731\n",
      "Epoch 822/10000\n",
      " - 0s - loss: 0.3236 - binary_accuracy: 0.8572 - val_loss: 0.2989 - val_binary_accuracy: 0.8725\n",
      "Epoch 823/10000\n",
      " - 0s - loss: 0.3299 - binary_accuracy: 0.8562 - val_loss: 0.2983 - val_binary_accuracy: 0.8735\n",
      "Epoch 824/10000\n",
      " - 0s - loss: 0.3281 - binary_accuracy: 0.8563 - val_loss: 0.2983 - val_binary_accuracy: 0.8738\n",
      "Epoch 825/10000\n",
      " - 0s - loss: 0.3228 - binary_accuracy: 0.8587 - val_loss: 0.2978 - val_binary_accuracy: 0.8727\n",
      "Epoch 826/10000\n",
      " - 0s - loss: 0.3230 - binary_accuracy: 0.8588 - val_loss: 0.2977 - val_binary_accuracy: 0.8737\n",
      "Epoch 827/10000\n",
      " - 0s - loss: 0.3204 - binary_accuracy: 0.8595 - val_loss: 0.2971 - val_binary_accuracy: 0.8746\n",
      "Epoch 828/10000\n",
      " - 0s - loss: 0.3153 - binary_accuracy: 0.8638 - val_loss: 0.2977 - val_binary_accuracy: 0.8740\n",
      "Epoch 829/10000\n",
      " - 0s - loss: 0.3264 - binary_accuracy: 0.8549 - val_loss: 0.2965 - val_binary_accuracy: 0.8748\n",
      "Epoch 830/10000\n",
      " - 0s - loss: 0.3215 - binary_accuracy: 0.8595 - val_loss: 0.2976 - val_binary_accuracy: 0.8727\n",
      "Epoch 831/10000\n",
      " - 0s - loss: 0.3286 - binary_accuracy: 0.8541 - val_loss: 0.2985 - val_binary_accuracy: 0.8742\n",
      "Epoch 832/10000\n",
      " - 0s - loss: 0.3180 - binary_accuracy: 0.8597 - val_loss: 0.2963 - val_binary_accuracy: 0.8744\n",
      "Epoch 833/10000\n",
      " - 0s - loss: 0.3275 - binary_accuracy: 0.8551 - val_loss: 0.2972 - val_binary_accuracy: 0.8727\n",
      "Epoch 834/10000\n",
      " - 0s - loss: 0.3225 - binary_accuracy: 0.8592 - val_loss: 0.2958 - val_binary_accuracy: 0.8740\n",
      "Epoch 835/10000\n",
      " - 0s - loss: 0.3229 - binary_accuracy: 0.8587 - val_loss: 0.2961 - val_binary_accuracy: 0.8738\n",
      "Epoch 836/10000\n",
      " - 0s - loss: 0.3193 - binary_accuracy: 0.8606 - val_loss: 0.2960 - val_binary_accuracy: 0.8742\n",
      "Epoch 837/10000\n",
      " - 0s - loss: 0.3203 - binary_accuracy: 0.8609 - val_loss: 0.2969 - val_binary_accuracy: 0.8737\n",
      "Epoch 838/10000\n",
      " - 0s - loss: 0.3310 - binary_accuracy: 0.8539 - val_loss: 0.2967 - val_binary_accuracy: 0.8738\n",
      "Epoch 839/10000\n",
      " - 0s - loss: 0.3252 - binary_accuracy: 0.8588 - val_loss: 0.2970 - val_binary_accuracy: 0.8737\n",
      "Epoch 840/10000\n",
      " - 0s - loss: 0.3301 - binary_accuracy: 0.8548 - val_loss: 0.2971 - val_binary_accuracy: 0.8754\n",
      "Epoch 841/10000\n",
      " - 0s - loss: 0.3319 - binary_accuracy: 0.8564 - val_loss: 0.2971 - val_binary_accuracy: 0.8744\n",
      "Epoch 842/10000\n",
      " - 0s - loss: 0.3211 - binary_accuracy: 0.8584 - val_loss: 0.2977 - val_binary_accuracy: 0.8754\n",
      "Epoch 843/10000\n",
      " - 0s - loss: 0.3257 - binary_accuracy: 0.8588 - val_loss: 0.2975 - val_binary_accuracy: 0.8735\n",
      "Epoch 844/10000\n",
      " - 0s - loss: 0.3229 - binary_accuracy: 0.8559 - val_loss: 0.2970 - val_binary_accuracy: 0.8737\n",
      "Epoch 845/10000\n",
      " - 0s - loss: 0.3168 - binary_accuracy: 0.8625 - val_loss: 0.2975 - val_binary_accuracy: 0.8721\n",
      "Epoch 846/10000\n",
      " - 0s - loss: 0.3221 - binary_accuracy: 0.8593 - val_loss: 0.2981 - val_binary_accuracy: 0.8725\n",
      "Epoch 847/10000\n",
      " - 0s - loss: 0.3275 - binary_accuracy: 0.8579 - val_loss: 0.2989 - val_binary_accuracy: 0.8744\n",
      "Epoch 848/10000\n",
      " - 0s - loss: 0.3295 - binary_accuracy: 0.8545 - val_loss: 0.2992 - val_binary_accuracy: 0.8748\n",
      "Epoch 849/10000\n",
      " - 0s - loss: 0.3265 - binary_accuracy: 0.8574 - val_loss: 0.3000 - val_binary_accuracy: 0.8752\n",
      "Epoch 850/10000\n",
      " - 0s - loss: 0.3298 - binary_accuracy: 0.8563 - val_loss: 0.3008 - val_binary_accuracy: 0.8738\n",
      "Epoch 851/10000\n",
      " - 0s - loss: 0.3259 - binary_accuracy: 0.8564 - val_loss: 0.3004 - val_binary_accuracy: 0.8750\n",
      "Epoch 852/10000\n",
      " - 0s - loss: 0.3196 - binary_accuracy: 0.8592 - val_loss: 0.2999 - val_binary_accuracy: 0.8756\n",
      "Epoch 853/10000\n",
      " - 0s - loss: 0.3190 - binary_accuracy: 0.8607 - val_loss: 0.2997 - val_binary_accuracy: 0.8742\n",
      "Epoch 854/10000\n",
      " - 0s - loss: 0.3229 - binary_accuracy: 0.8592 - val_loss: 0.2984 - val_binary_accuracy: 0.8735\n",
      "Epoch 855/10000\n",
      " - 0s - loss: 0.3267 - binary_accuracy: 0.8567 - val_loss: 0.2987 - val_binary_accuracy: 0.8754\n",
      "Epoch 856/10000\n",
      " - 0s - loss: 0.3235 - binary_accuracy: 0.8588 - val_loss: 0.2980 - val_binary_accuracy: 0.8740\n",
      "Epoch 857/10000\n",
      " - 0s - loss: 0.3296 - binary_accuracy: 0.8563 - val_loss: 0.2985 - val_binary_accuracy: 0.8750\n",
      "Epoch 858/10000\n",
      " - 0s - loss: 0.3235 - binary_accuracy: 0.8603 - val_loss: 0.2987 - val_binary_accuracy: 0.8756\n",
      "Epoch 859/10000\n",
      " - 0s - loss: 0.3236 - binary_accuracy: 0.8598 - val_loss: 0.3001 - val_binary_accuracy: 0.8721\n",
      "Epoch 860/10000\n",
      " - 0s - loss: 0.3268 - binary_accuracy: 0.8558 - val_loss: 0.2965 - val_binary_accuracy: 0.8737\n",
      "Epoch 861/10000\n",
      " - 0s - loss: 0.3272 - binary_accuracy: 0.8582 - val_loss: 0.2979 - val_binary_accuracy: 0.8731\n",
      "Epoch 862/10000\n",
      " - 0s - loss: 0.3176 - binary_accuracy: 0.8616 - val_loss: 0.2976 - val_binary_accuracy: 0.8756\n",
      "Epoch 863/10000\n",
      " - 0s - loss: 0.3242 - binary_accuracy: 0.8600 - val_loss: 0.2967 - val_binary_accuracy: 0.8725\n",
      "Epoch 864/10000\n",
      " - 0s - loss: 0.3286 - binary_accuracy: 0.8552 - val_loss: 0.2977 - val_binary_accuracy: 0.8762\n",
      "Epoch 865/10000\n",
      " - 0s - loss: 0.3244 - binary_accuracy: 0.8575 - val_loss: 0.2980 - val_binary_accuracy: 0.8750\n",
      "Epoch 866/10000\n",
      " - 0s - loss: 0.3110 - binary_accuracy: 0.8632 - val_loss: 0.2985 - val_binary_accuracy: 0.8748\n",
      "Epoch 867/10000\n",
      " - 0s - loss: 0.3314 - binary_accuracy: 0.8565 - val_loss: 0.2981 - val_binary_accuracy: 0.8744\n",
      "Epoch 868/10000\n",
      " - 0s - loss: 0.3188 - binary_accuracy: 0.8618 - val_loss: 0.2966 - val_binary_accuracy: 0.8740\n",
      "Epoch 869/10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-730b06a64dc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             callbacks=callbacks_list)\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;31m#class_weight={0:0.55, 1:5.22}) #, use_multiprocessing=True, workers=8) #, callbacks=[tbCallBack])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Model for dge to bdtnp.\n",
    "print(time.ctime(), 'Model build')\n",
    "\n",
    "a1 = Input(shape=(84,))\n",
    "e = Dense(84)(a1)\n",
    "e = BatchNormalization()(e)\n",
    "e = Dropout(0.3)(e)\n",
    "\n",
    "#c1 = Input(shape=(num_all,))\n",
    "#c3 = Dense(84)(c1)\n",
    "#c4 = BatchNormalization()(c3)\n",
    "#c5 = AlphaDropout(0.5)(c4)\n",
    "\n",
    "#e = concatenate([a4, c5])\n",
    "e = Dense(40)(e)\n",
    "e = BatchNormalization()(e)\n",
    "e = Activation('softplus')(e)\n",
    "e = Dropout(0.2)(e)\n",
    "\n",
    "output = Dense(num_situ, activation='sigmoid')(e)\n",
    "model = Model(inputs=[a1], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "#print(model.summary())\n",
    "print(time.strftime(\"%H:%M:%S\"), ' Fit')\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"models/weights-improvement-{epoch:02d}-{val_binary_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_binary_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "#tbCallBack = keras.callbacks.TensorBoard(log_dir='.', histogram_freq=0, write_graph=True, write_images=True)\n",
    "#history = \n",
    "model.fit(  x=[Z_], y=y_,\n",
    "            batch_size=10,\n",
    "            epochs=10000,\n",
    "            verbose=2,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks_list)\n",
    "            #class_weight={0:0.55, 1:5.22}) #, use_multiprocessing=True, workers=8) #, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55288311, 5.22740697])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "plt.title('Model accuracy')\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "print(np.average(history.history['val_acc']))\n",
    "print(np.max(history.history['val_acc']))\n",
    "#dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
    "#0.6216538506631668\n",
    "#0.7161538486297314\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try leave one out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%H:%M:%S\"), ' Fit')\n",
    "\n",
    "#Try differnet training ommitting one gene at a time.\n",
    "val_acc=np.empty((num_situ,2))\n",
    "for i in range(num_situ):\n",
    "    model.load_weights('model.h5')\n",
    "    X2_temp = np.delete(X2_train, i, axis=1)\n",
    "    #tbCallBack = keras.callbacks.TensorBoard(log_dir='.', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    history = model.fit(x=[X1_train, X2_temp, X3_train],\n",
    "                        y=y_train,\n",
    "                        batch_size=50,\n",
    "                        epochs=20,\n",
    "                        verbose=0,\n",
    "                        validation_split=0.3,\n",
    "                        class_weight={0:1, 1:10}) #, use_multiprocessing=True, workers=8) #, callbacks=[tbCallBack])\n",
    "    val_acc[i,0] = np.average(history.history['val_acc'])\n",
    "    val_acc[i,1] = np.max(history.history['val_acc'])\n",
    "    print(time.ctime(), f'i: {i}, val_acc average: {val_acc[i,0]}, max: {val_acc[i,1]}')\n",
    "\n",
    "np.save('val_acc.npy', val_acc)\n",
    "\n",
    "#Compare with results in connection weight genes (using all zeros but one).\n",
    "from scipy import stats\n",
    "\n",
    "loo = pd.read_csv('logs/2/loo.csv')\n",
    "cw = pd.read_csv('logs/2/cw.csv')\n",
    "tau, p_value = stats.kendalltau(loo['x'], cw['x'])\n",
    "print(f'tau - {tau}, p-value - {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 13 22:23:28 2018 Load model\n",
      "0  1  2  3  4  5  6  7  8  9  10  11  Tue Nov 13 22:25:07 2018 Done\n"
     ]
    }
   ],
   "source": [
    "#Use batch prediction=50\n",
    "import heapq\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def matthews_correlation_loss(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos =  y_pred\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = y_true\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = K.square(tp * tn - fp * fn)\n",
    "    denominator = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "\n",
    "    return 50 - 100 * numerator/(denominator + K.epsilon())\n",
    "\n",
    "\n",
    "print(time.ctime(), 'Load model')\n",
    "model = keras.models.load_model('weights-improvement-1284-0.41.hdf5', custom_objects={\"matthews_correlation\": matthews_correlation, \"matthews_correlation_loss\":matthews_correlation_loss}) #weights-improvement-2535-0.88.hdf5\n",
    "#model = load_model('c:/data/Dream/10-60_genes/model_sav_60.h5')\n",
    "#model = load_model('c:/data/Dream/9-40_genes/model_sav_40.h5')\n",
    "\n",
    "#Loop on all 3039 cells and provide 10 highest probable locations (search for the line/s in BDTNP providing the highest probability)\n",
    "result = pd.DataFrame()\n",
    "for index, row_d in d.iterrows():\n",
    "    print(index, ' ', end=\"\")\n",
    "    proba = []\n",
    "    for index2, row_b in bdtnp.iterrows():\n",
    "        X = np.vstack([row_b,row_d[cols]])\n",
    "        pred = model.predict([X[np.newaxis,:], row_d[np.newaxis,:]], batch_size=1)\n",
    "        proba.append(pred)\n",
    "\n",
    "    #list2 = [i[0][0] for i in heapq.nlargest(10, proba)]\n",
    "    #x = pd.Series(list2, index=['val1', 'val2', 'val3', 'val4', 'val5', 'val6', 'val7', 'val8', 'val9', 'val10'])\n",
    "    result = pd.concat([result,pd.DataFrame([sorted(range(len(proba)), key=lambda i: proba[i])[-10:]], columns=['i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7', 'i8', 'i9', 'i10'])])\n",
    "    #result = result.append(x, ignore_index=True)\n",
    "    if index > 10:\n",
    "        break\n",
    "\n",
    "#c = pd.DataFrame()\n",
    "#Loop on all cells in dge.csv and provide 10 highest probable locations (search for the line/s in BDTNP providing the higher probability).\n",
    "#for index, row_d in d.iterrows():\n",
    "#    print(index, ' ', end=\"\")\n",
    "    #Multiply row_d by 3039 (as a fixed row in inputs2 and 3).    \n",
    "#    row_expanded = pd.concat([row_d]*len(bdtnp), ignore_index=True, axis=1).T\n",
    "    #b_expanded = pd.concat([b, row_expanded], axis=1)\n",
    "    #pred = model.predict([b_expanded.iloc[glist_60_tom], b_expanded.iloc[:,num_situ:2*num_situ], b_expanded.iloc[:,2*num_situ:]], batch_size=50, verbose=0)\n",
    "    #pred = model.predict([bdtnp[glist_60_tom], row_expanded[glist_60_tom], row_expanded[list(set(d.columns) - set(glist_60_tom))]], batch_size=50, verbose=0)\n",
    "#    pred = model.predict([bdtnp[cols], row_expanded[cols], row_expanded], batch_size=50, verbose=0)\n",
    "#    c = pd.concat([c,pd.DataFrame([sorted(range(len(pred)), key=lambda i: pred[i])[-10:]], columns=['i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7', 'i8', 'i9', 'i10'])])\n",
    "#    if index > 10:\n",
    "#        break\n",
    "\n",
    "result.to_csv('ann_84.csv')\n",
    "print(time.ctime(), 'Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1297/1297 [==============================] - 0s 70us/step\n"
     ]
    }
   ],
   "source": [
    "# Using the dge to bdtnp model.\n",
    "\n",
    "model = keras.models.load_model('weights-improvement-12347-0.88.hdf5')\n",
    "c = pd.DataFrame()\n",
    "pred = model.predict([d1_bin], batch_size=50, verbose=1)\n",
    "results = pd.DataFrame(data=np.round(pred), index=[i for i in range(0,len(pred))], columns=glist_20)\n",
    "results.to_csv('ann_pred.csv', index=False)\n",
    "\n",
    "#Test results\n",
    "#real_count=0\n",
    "#for i, row in labels.iterrows():\n",
    "#    if (closest[i] == row[0]):\n",
    "#        real_count += 1\n",
    "#\n",
    "#print(real_count)\n",
    "#results.iloc[0:3]\n",
    "#meds = temp_results.median()\n",
    "#for col in temp_results:\n",
    "#    temp_results[col] = temp_results[col].apply(lambda x: 0 if x<= (meds[col] + i) else 1)\n",
    "#    locations[index] = pairwise_distances_argmin(temp_results, bdtnp_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danr</th>\n",
       "      <th>CG14427</th>\n",
       "      <th>dan</th>\n",
       "      <th>CG43394</th>\n",
       "      <th>ImpL2</th>\n",
       "      <th>Nek2</th>\n",
       "      <th>CG8147</th>\n",
       "      <th>Ama</th>\n",
       "      <th>Btk29A</th>\n",
       "      <th>trn</th>\n",
       "      <th>numb</th>\n",
       "      <th>prd</th>\n",
       "      <th>brk</th>\n",
       "      <th>tsh</th>\n",
       "      <th>pxb</th>\n",
       "      <th>dpn</th>\n",
       "      <th>ftz</th>\n",
       "      <th>Kr</th>\n",
       "      <th>h</th>\n",
       "      <th>eve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.952570</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999427</td>\n",
       "      <td>9.988462e-01</td>\n",
       "      <td>2.225487e-06</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>8.976961e-01</td>\n",
       "      <td>0.910867</td>\n",
       "      <td>9.992010e-01</td>\n",
       "      <td>4.948604e-06</td>\n",
       "      <td>9.973761e-01</td>\n",
       "      <td>1.085652e-04</td>\n",
       "      <td>4.932669e-04</td>\n",
       "      <td>1.096998e-04</td>\n",
       "      <td>3.081364e-06</td>\n",
       "      <td>0.692830</td>\n",
       "      <td>9.230255e-01</td>\n",
       "      <td>1.595170e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.073005</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.260960</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>9.924010e-01</td>\n",
       "      <td>4.919980e-04</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>4.686632e-02</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>3.026228e-01</td>\n",
       "      <td>5.767848e-01</td>\n",
       "      <td>9.956729e-01</td>\n",
       "      <td>3.016724e-02</td>\n",
       "      <td>6.589219e-04</td>\n",
       "      <td>9.117642e-03</td>\n",
       "      <td>9.999350e-01</td>\n",
       "      <td>0.033744</td>\n",
       "      <td>1.860791e-03</td>\n",
       "      <td>2.594135e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.128240</td>\n",
       "      <td>0.998572</td>\n",
       "      <td>2.026050e-05</td>\n",
       "      <td>7.032250e-03</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>5.656553e-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.185221e-05</td>\n",
       "      <td>9.262726e-01</td>\n",
       "      <td>1.872751e-04</td>\n",
       "      <td>9.973971e-01</td>\n",
       "      <td>9.825932e-01</td>\n",
       "      <td>5.684511e-02</td>\n",
       "      <td>9.997877e-01</td>\n",
       "      <td>0.999295</td>\n",
       "      <td>1.272992e-02</td>\n",
       "      <td>1.028869e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.999962e-01</td>\n",
       "      <td>0.788806</td>\n",
       "      <td>9.998572e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>9.279311e-01</td>\n",
       "      <td>6.623404e-07</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>9.910707e-01</td>\n",
       "      <td>0.174682</td>\n",
       "      <td>9.992439e-01</td>\n",
       "      <td>1.558326e-05</td>\n",
       "      <td>9.878981e-01</td>\n",
       "      <td>5.819520e-07</td>\n",
       "      <td>5.616950e-01</td>\n",
       "      <td>2.520103e-03</td>\n",
       "      <td>8.022946e-08</td>\n",
       "      <td>0.036852</td>\n",
       "      <td>9.923142e-01</td>\n",
       "      <td>4.249081e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999593</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>1.097144e-07</td>\n",
       "      <td>1.467953e-01</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>1.737630e-09</td>\n",
       "      <td>0.986156</td>\n",
       "      <td>4.429802e-06</td>\n",
       "      <td>9.999733e-01</td>\n",
       "      <td>1.644533e-03</td>\n",
       "      <td>9.999981e-01</td>\n",
       "      <td>9.961914e-01</td>\n",
       "      <td>9.938365e-01</td>\n",
       "      <td>9.665008e-01</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>9.889327e-01</td>\n",
       "      <td>7.089722e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.857821e-03</td>\n",
       "      <td>0.398339</td>\n",
       "      <td>6.580142e-03</td>\n",
       "      <td>0.995009</td>\n",
       "      <td>0.094408</td>\n",
       "      <td>9.904410e-01</td>\n",
       "      <td>9.999146e-01</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>7.737213e-01</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.550446e-04</td>\n",
       "      <td>8.348986e-03</td>\n",
       "      <td>1.097112e-04</td>\n",
       "      <td>6.478760e-03</td>\n",
       "      <td>4.778898e-04</td>\n",
       "      <td>8.386919e-01</td>\n",
       "      <td>9.999241e-01</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>3.880746e-03</td>\n",
       "      <td>9.181547e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.970232e-01</td>\n",
       "      <td>0.999684</td>\n",
       "      <td>9.999852e-01</td>\n",
       "      <td>0.993155</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>6.502574e-01</td>\n",
       "      <td>1.507457e-03</td>\n",
       "      <td>0.459570</td>\n",
       "      <td>2.764302e-01</td>\n",
       "      <td>0.912435</td>\n",
       "      <td>6.476845e-01</td>\n",
       "      <td>8.231743e-04</td>\n",
       "      <td>6.520438e-01</td>\n",
       "      <td>1.139168e-02</td>\n",
       "      <td>2.286469e-03</td>\n",
       "      <td>1.270115e-05</td>\n",
       "      <td>1.181825e-03</td>\n",
       "      <td>0.920938</td>\n",
       "      <td>2.793467e-04</td>\n",
       "      <td>2.107628e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.972035e-04</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>3.349570e-05</td>\n",
       "      <td>0.999668</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>8.212480e-06</td>\n",
       "      <td>1.815548e-04</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>9.984673e-01</td>\n",
       "      <td>0.999898</td>\n",
       "      <td>2.098095e-04</td>\n",
       "      <td>5.091575e-05</td>\n",
       "      <td>2.507595e-06</td>\n",
       "      <td>1.720824e-05</td>\n",
       "      <td>9.998615e-01</td>\n",
       "      <td>2.240034e-04</td>\n",
       "      <td>9.414664e-06</td>\n",
       "      <td>0.989408</td>\n",
       "      <td>1.642943e-04</td>\n",
       "      <td>5.226656e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.495757e-03</td>\n",
       "      <td>0.182013</td>\n",
       "      <td>4.974751e-02</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>9.591834e-03</td>\n",
       "      <td>9.999360e-01</td>\n",
       "      <td>0.999893</td>\n",
       "      <td>9.998872e-01</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>9.999392e-01</td>\n",
       "      <td>7.474542e-01</td>\n",
       "      <td>3.679442e-06</td>\n",
       "      <td>9.718327e-01</td>\n",
       "      <td>1.333877e-01</td>\n",
       "      <td>4.582791e-01</td>\n",
       "      <td>3.363704e-02</td>\n",
       "      <td>0.946534</td>\n",
       "      <td>9.827011e-01</td>\n",
       "      <td>9.960330e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.811024e-03</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>2.724231e-04</td>\n",
       "      <td>0.999914</td>\n",
       "      <td>0.999469</td>\n",
       "      <td>4.158520e-04</td>\n",
       "      <td>1.917854e-04</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>9.913943e-01</td>\n",
       "      <td>0.999727</td>\n",
       "      <td>3.918419e-04</td>\n",
       "      <td>1.168152e-04</td>\n",
       "      <td>1.891582e-04</td>\n",
       "      <td>4.295379e-06</td>\n",
       "      <td>9.955487e-01</td>\n",
       "      <td>1.478557e-05</td>\n",
       "      <td>1.859974e-04</td>\n",
       "      <td>0.633407</td>\n",
       "      <td>9.499377e-06</td>\n",
       "      <td>4.998521e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9.999993e-01</td>\n",
       "      <td>0.997766</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.668968</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>2.553989e-02</td>\n",
       "      <td>9.212085e-03</td>\n",
       "      <td>0.998669</td>\n",
       "      <td>9.818991e-05</td>\n",
       "      <td>0.957203</td>\n",
       "      <td>1.136290e-03</td>\n",
       "      <td>8.066000e-01</td>\n",
       "      <td>6.916312e-03</td>\n",
       "      <td>9.018955e-01</td>\n",
       "      <td>9.928236e-01</td>\n",
       "      <td>9.150981e-01</td>\n",
       "      <td>2.341511e-01</td>\n",
       "      <td>0.963215</td>\n",
       "      <td>7.768021e-01</td>\n",
       "      <td>5.764536e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.143619</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>3.550558e-04</td>\n",
       "      <td>2.620373e-03</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>4.401929e-03</td>\n",
       "      <td>0.685028</td>\n",
       "      <td>9.751267e-01</td>\n",
       "      <td>9.994123e-01</td>\n",
       "      <td>9.996619e-01</td>\n",
       "      <td>9.993830e-01</td>\n",
       "      <td>9.965026e-01</td>\n",
       "      <td>9.987382e-01</td>\n",
       "      <td>1.185822e-01</td>\n",
       "      <td>0.995253</td>\n",
       "      <td>9.969323e-01</td>\n",
       "      <td>9.813778e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.999760e-01</td>\n",
       "      <td>0.994280</td>\n",
       "      <td>9.999995e-01</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>0.996073</td>\n",
       "      <td>6.270727e-04</td>\n",
       "      <td>9.107602e-01</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>9.996758e-01</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>9.931347e-01</td>\n",
       "      <td>9.981224e-01</td>\n",
       "      <td>1.047253e-05</td>\n",
       "      <td>9.999948e-01</td>\n",
       "      <td>9.814327e-01</td>\n",
       "      <td>9.147418e-01</td>\n",
       "      <td>9.998261e-01</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>8.847806e-01</td>\n",
       "      <td>3.586725e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.865134e-03</td>\n",
       "      <td>0.638284</td>\n",
       "      <td>1.317361e-04</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.017298</td>\n",
       "      <td>9.998714e-01</td>\n",
       "      <td>9.981104e-01</td>\n",
       "      <td>0.038774</td>\n",
       "      <td>7.815810e-02</td>\n",
       "      <td>0.244115</td>\n",
       "      <td>2.733906e-03</td>\n",
       "      <td>1.631580e-02</td>\n",
       "      <td>9.454259e-01</td>\n",
       "      <td>1.690295e-03</td>\n",
       "      <td>2.211262e-02</td>\n",
       "      <td>9.957338e-01</td>\n",
       "      <td>3.480875e-03</td>\n",
       "      <td>0.027029</td>\n",
       "      <td>2.698334e-02</td>\n",
       "      <td>1.070119e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.615304e-03</td>\n",
       "      <td>0.999890</td>\n",
       "      <td>9.990397e-01</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>5.893366e-02</td>\n",
       "      <td>1.338275e-03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.659755e-01</td>\n",
       "      <td>0.243339</td>\n",
       "      <td>1.676398e-03</td>\n",
       "      <td>3.930203e-06</td>\n",
       "      <td>2.040161e-06</td>\n",
       "      <td>3.323668e-07</td>\n",
       "      <td>6.603202e-03</td>\n",
       "      <td>2.577902e-05</td>\n",
       "      <td>7.779146e-06</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>2.031399e-03</td>\n",
       "      <td>1.548131e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.393367e-07</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>2.693647e-08</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>9.999983e-01</td>\n",
       "      <td>9.999737e-01</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>4.211594e-01</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>4.940522e-06</td>\n",
       "      <td>6.464802e-09</td>\n",
       "      <td>2.028336e-01</td>\n",
       "      <td>7.572076e-06</td>\n",
       "      <td>5.822034e-07</td>\n",
       "      <td>3.205048e-04</td>\n",
       "      <td>3.045832e-06</td>\n",
       "      <td>0.999865</td>\n",
       "      <td>1.738679e-07</td>\n",
       "      <td>9.803601e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.999848</td>\n",
       "      <td>1.459149e-06</td>\n",
       "      <td>8.769456e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.367542e-08</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>7.736443e-08</td>\n",
       "      <td>9.425172e-04</td>\n",
       "      <td>8.211447e-09</td>\n",
       "      <td>9.999952e-01</td>\n",
       "      <td>9.757390e-04</td>\n",
       "      <td>8.728823e-06</td>\n",
       "      <td>9.196917e-04</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>5.957111e-03</td>\n",
       "      <td>9.998777e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.812086e-01</td>\n",
       "      <td>0.564072</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.116492</td>\n",
       "      <td>1.887775e-06</td>\n",
       "      <td>9.952095e-01</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>9.984468e-01</td>\n",
       "      <td>0.999681</td>\n",
       "      <td>9.998662e-01</td>\n",
       "      <td>9.999949e-01</td>\n",
       "      <td>4.112371e-05</td>\n",
       "      <td>9.999994e-01</td>\n",
       "      <td>9.923676e-01</td>\n",
       "      <td>9.695355e-01</td>\n",
       "      <td>9.997806e-01</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>5.820566e-01</td>\n",
       "      <td>1.634441e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.996334</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.504082</td>\n",
       "      <td>0.992520</td>\n",
       "      <td>2.282194e-01</td>\n",
       "      <td>9.886728e-01</td>\n",
       "      <td>0.994003</td>\n",
       "      <td>9.966336e-01</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>9.993253e-01</td>\n",
       "      <td>9.994853e-01</td>\n",
       "      <td>1.097422e-02</td>\n",
       "      <td>9.998964e-01</td>\n",
       "      <td>4.127295e-04</td>\n",
       "      <td>6.243932e-04</td>\n",
       "      <td>9.998981e-01</td>\n",
       "      <td>0.995208</td>\n",
       "      <td>6.144069e-01</td>\n",
       "      <td>2.557264e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.995290e-01</td>\n",
       "      <td>0.133104</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.996271</td>\n",
       "      <td>0.051228</td>\n",
       "      <td>9.970793e-01</td>\n",
       "      <td>2.451493e-03</td>\n",
       "      <td>0.989910</td>\n",
       "      <td>9.763867e-01</td>\n",
       "      <td>0.046796</td>\n",
       "      <td>9.990502e-01</td>\n",
       "      <td>6.390570e-03</td>\n",
       "      <td>6.596552e-03</td>\n",
       "      <td>2.045522e-06</td>\n",
       "      <td>9.012696e-01</td>\n",
       "      <td>4.779747e-01</td>\n",
       "      <td>4.163624e-07</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>9.997198e-01</td>\n",
       "      <td>8.621904e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.809125</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.982447</td>\n",
       "      <td>7.849649e-05</td>\n",
       "      <td>8.919598e-01</td>\n",
       "      <td>0.999904</td>\n",
       "      <td>9.998344e-01</td>\n",
       "      <td>0.991347</td>\n",
       "      <td>9.999064e-01</td>\n",
       "      <td>9.996842e-01</td>\n",
       "      <td>1.517768e-04</td>\n",
       "      <td>9.999899e-01</td>\n",
       "      <td>7.483673e-04</td>\n",
       "      <td>2.139357e-04</td>\n",
       "      <td>9.998947e-01</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>1.370329e-03</td>\n",
       "      <td>2.224371e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999863</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.751584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.413376e-05</td>\n",
       "      <td>5.347388e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.750275e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600709e-07</td>\n",
       "      <td>9.986628e-01</td>\n",
       "      <td>2.014484e-06</td>\n",
       "      <td>9.999235e-01</td>\n",
       "      <td>1.959178e-01</td>\n",
       "      <td>3.780622e-03</td>\n",
       "      <td>9.999964e-01</td>\n",
       "      <td>0.999053</td>\n",
       "      <td>3.804333e-02</td>\n",
       "      <td>2.570437e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.997407e-01</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>0.841562</td>\n",
       "      <td>0.978401</td>\n",
       "      <td>3.264277e-06</td>\n",
       "      <td>9.080054e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.722379e-08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.774804e-08</td>\n",
       "      <td>9.993616e-01</td>\n",
       "      <td>1.896762e-08</td>\n",
       "      <td>9.978809e-01</td>\n",
       "      <td>9.166709e-01</td>\n",
       "      <td>9.140667e-01</td>\n",
       "      <td>9.999651e-01</td>\n",
       "      <td>0.816317</td>\n",
       "      <td>9.456939e-01</td>\n",
       "      <td>6.941300e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.996756e-01</td>\n",
       "      <td>0.961402</td>\n",
       "      <td>9.999933e-01</td>\n",
       "      <td>0.957463</td>\n",
       "      <td>0.997574</td>\n",
       "      <td>8.018143e-01</td>\n",
       "      <td>8.378164e-01</td>\n",
       "      <td>0.903518</td>\n",
       "      <td>1.236204e-03</td>\n",
       "      <td>0.007377</td>\n",
       "      <td>5.552902e-03</td>\n",
       "      <td>1.163220e-01</td>\n",
       "      <td>8.741810e-01</td>\n",
       "      <td>8.932080e-01</td>\n",
       "      <td>1.407525e-02</td>\n",
       "      <td>3.657069e-02</td>\n",
       "      <td>2.596068e-01</td>\n",
       "      <td>0.191250</td>\n",
       "      <td>1.999731e-01</td>\n",
       "      <td>9.880601e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.357590e-04</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>1.309942e-05</td>\n",
       "      <td>0.999868</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>5.366280e-04</td>\n",
       "      <td>1.939070e-05</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>9.998235e-01</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>6.849265e-04</td>\n",
       "      <td>3.447213e-06</td>\n",
       "      <td>6.536806e-07</td>\n",
       "      <td>1.636066e-06</td>\n",
       "      <td>9.993469e-01</td>\n",
       "      <td>1.056744e-06</td>\n",
       "      <td>3.958464e-06</td>\n",
       "      <td>0.997777</td>\n",
       "      <td>9.571102e-07</td>\n",
       "      <td>9.843995e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.966834e-01</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>6.423675e-01</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.999382</td>\n",
       "      <td>3.252073e-01</td>\n",
       "      <td>3.819732e-06</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>9.953775e-01</td>\n",
       "      <td>0.963407</td>\n",
       "      <td>9.395645e-01</td>\n",
       "      <td>5.354946e-05</td>\n",
       "      <td>5.687503e-01</td>\n",
       "      <td>5.921737e-06</td>\n",
       "      <td>9.194887e-01</td>\n",
       "      <td>1.795016e-04</td>\n",
       "      <td>6.412070e-06</td>\n",
       "      <td>0.818450</td>\n",
       "      <td>1.410119e-02</td>\n",
       "      <td>2.079197e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.976103e-01</td>\n",
       "      <td>4.243368e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>9.880773e-01</td>\n",
       "      <td>0.999090</td>\n",
       "      <td>9.996192e-01</td>\n",
       "      <td>1.085425e-06</td>\n",
       "      <td>9.997811e-01</td>\n",
       "      <td>9.024175e-05</td>\n",
       "      <td>3.737268e-05</td>\n",
       "      <td>5.879258e-08</td>\n",
       "      <td>1.925434e-06</td>\n",
       "      <td>0.999434</td>\n",
       "      <td>2.634638e-04</td>\n",
       "      <td>7.155194e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.999926e-01</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.521378</td>\n",
       "      <td>0.997482</td>\n",
       "      <td>1.212213e-05</td>\n",
       "      <td>1.623061e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.982784e-07</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>1.571913e-06</td>\n",
       "      <td>9.989116e-01</td>\n",
       "      <td>1.794630e-06</td>\n",
       "      <td>9.994752e-01</td>\n",
       "      <td>9.997467e-01</td>\n",
       "      <td>9.932792e-01</td>\n",
       "      <td>8.784008e-01</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>9.942315e-01</td>\n",
       "      <td>1.421364e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.290200e-07</td>\n",
       "      <td>0.074248</td>\n",
       "      <td>1.633305e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.999992e-01</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>2.049086e-01</td>\n",
       "      <td>0.995819</td>\n",
       "      <td>9.575192e-07</td>\n",
       "      <td>6.270267e-07</td>\n",
       "      <td>9.988130e-01</td>\n",
       "      <td>2.106784e-08</td>\n",
       "      <td>3.456530e-06</td>\n",
       "      <td>9.997488e-01</td>\n",
       "      <td>2.526374e-01</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>6.032673e-06</td>\n",
       "      <td>1.446585e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9.984952e-01</td>\n",
       "      <td>0.996364</td>\n",
       "      <td>9.999988e-01</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.998054</td>\n",
       "      <td>5.185466e-05</td>\n",
       "      <td>9.917101e-01</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>9.992722e-01</td>\n",
       "      <td>0.994933</td>\n",
       "      <td>9.989964e-01</td>\n",
       "      <td>9.999472e-01</td>\n",
       "      <td>3.654560e-05</td>\n",
       "      <td>9.999973e-01</td>\n",
       "      <td>9.992108e-01</td>\n",
       "      <td>9.995689e-01</td>\n",
       "      <td>9.569515e-01</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>9.983876e-01</td>\n",
       "      <td>4.413893e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>9.998910e-01</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.805672</td>\n",
       "      <td>0.482736</td>\n",
       "      <td>3.877808e-06</td>\n",
       "      <td>9.159855e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.310827e-09</td>\n",
       "      <td>0.996079</td>\n",
       "      <td>2.904256e-08</td>\n",
       "      <td>9.909118e-01</td>\n",
       "      <td>2.973403e-10</td>\n",
       "      <td>9.461822e-01</td>\n",
       "      <td>9.003154e-01</td>\n",
       "      <td>5.748441e-01</td>\n",
       "      <td>8.968162e-01</td>\n",
       "      <td>0.106455</td>\n",
       "      <td>9.168677e-01</td>\n",
       "      <td>3.899791e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>9.996731e-01</td>\n",
       "      <td>0.859843</td>\n",
       "      <td>9.997267e-01</td>\n",
       "      <td>0.883014</td>\n",
       "      <td>0.886068</td>\n",
       "      <td>5.279544e-01</td>\n",
       "      <td>9.991554e-01</td>\n",
       "      <td>0.938228</td>\n",
       "      <td>6.531123e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.854854e-01</td>\n",
       "      <td>9.999777e-01</td>\n",
       "      <td>9.516612e-01</td>\n",
       "      <td>7.897650e-01</td>\n",
       "      <td>9.958682e-01</td>\n",
       "      <td>9.851004e-01</td>\n",
       "      <td>9.999923e-01</td>\n",
       "      <td>0.010582</td>\n",
       "      <td>7.781755e-01</td>\n",
       "      <td>2.496092e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>1.244802e-02</td>\n",
       "      <td>0.992493</td>\n",
       "      <td>1.813867e-02</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.585667</td>\n",
       "      <td>9.998382e-01</td>\n",
       "      <td>9.997755e-01</td>\n",
       "      <td>0.082235</td>\n",
       "      <td>5.800074e-02</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>2.697955e-03</td>\n",
       "      <td>5.853162e-02</td>\n",
       "      <td>9.996061e-01</td>\n",
       "      <td>1.145558e-03</td>\n",
       "      <td>7.614732e-03</td>\n",
       "      <td>9.875183e-01</td>\n",
       "      <td>9.999026e-01</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>5.571116e-02</td>\n",
       "      <td>2.419810e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>9.995771e-01</td>\n",
       "      <td>0.997964</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.102224</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>6.285930e-08</td>\n",
       "      <td>9.982022e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.397281e-10</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>1.219459e-10</td>\n",
       "      <td>9.194591e-01</td>\n",
       "      <td>6.604906e-13</td>\n",
       "      <td>9.991876e-01</td>\n",
       "      <td>4.916412e-02</td>\n",
       "      <td>1.357784e-03</td>\n",
       "      <td>9.991697e-01</td>\n",
       "      <td>0.996749</td>\n",
       "      <td>2.150211e-03</td>\n",
       "      <td>2.735774e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.069769</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.998935</td>\n",
       "      <td>0.992937</td>\n",
       "      <td>9.528593e-01</td>\n",
       "      <td>1.127196e-05</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>7.323690e-04</td>\n",
       "      <td>0.994680</td>\n",
       "      <td>9.341871e-01</td>\n",
       "      <td>9.999479e-01</td>\n",
       "      <td>9.999354e-01</td>\n",
       "      <td>9.691881e-06</td>\n",
       "      <td>9.999770e-01</td>\n",
       "      <td>9.999853e-01</td>\n",
       "      <td>3.738622e-05</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>9.999276e-01</td>\n",
       "      <td>7.104409e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>9.999919e-01</td>\n",
       "      <td>0.999857</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.132667</td>\n",
       "      <td>0.713718</td>\n",
       "      <td>9.492947e-06</td>\n",
       "      <td>9.943183e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.949519e-05</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>1.579181e-06</td>\n",
       "      <td>8.556538e-03</td>\n",
       "      <td>6.250071e-05</td>\n",
       "      <td>9.999804e-01</td>\n",
       "      <td>6.206724e-05</td>\n",
       "      <td>5.777020e-05</td>\n",
       "      <td>1.952201e-01</td>\n",
       "      <td>0.999779</td>\n",
       "      <td>1.425272e-05</td>\n",
       "      <td>9.174622e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.443121</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.997710</td>\n",
       "      <td>0.999042</td>\n",
       "      <td>9.966443e-01</td>\n",
       "      <td>5.262723e-06</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>6.644081e-04</td>\n",
       "      <td>0.998826</td>\n",
       "      <td>9.184116e-01</td>\n",
       "      <td>9.619719e-01</td>\n",
       "      <td>9.996790e-01</td>\n",
       "      <td>6.026992e-06</td>\n",
       "      <td>9.720117e-01</td>\n",
       "      <td>9.988384e-01</td>\n",
       "      <td>1.963415e-02</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>9.988311e-01</td>\n",
       "      <td>1.126539e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>9.999567e-01</td>\n",
       "      <td>0.991575</td>\n",
       "      <td>9.997956e-01</td>\n",
       "      <td>0.993017</td>\n",
       "      <td>0.999459</td>\n",
       "      <td>9.907833e-01</td>\n",
       "      <td>8.947784e-01</td>\n",
       "      <td>0.010496</td>\n",
       "      <td>7.641231e-02</td>\n",
       "      <td>0.990980</td>\n",
       "      <td>2.616576e-02</td>\n",
       "      <td>8.328405e-01</td>\n",
       "      <td>9.995334e-01</td>\n",
       "      <td>9.509410e-01</td>\n",
       "      <td>1.745957e-01</td>\n",
       "      <td>4.448363e-01</td>\n",
       "      <td>9.806773e-01</td>\n",
       "      <td>0.531551</td>\n",
       "      <td>6.200535e-01</td>\n",
       "      <td>1.029146e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>9.999980e-01</td>\n",
       "      <td>0.988574</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.986739</td>\n",
       "      <td>9.421500e-04</td>\n",
       "      <td>9.824730e-01</td>\n",
       "      <td>0.995236</td>\n",
       "      <td>9.953833e-01</td>\n",
       "      <td>0.265717</td>\n",
       "      <td>9.966738e-01</td>\n",
       "      <td>7.139744e-01</td>\n",
       "      <td>6.418411e-04</td>\n",
       "      <td>9.999890e-01</td>\n",
       "      <td>6.617320e-03</td>\n",
       "      <td>1.762567e-03</td>\n",
       "      <td>7.551752e-01</td>\n",
       "      <td>0.999513</td>\n",
       "      <td>3.736563e-03</td>\n",
       "      <td>9.415157e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>7.404959e-03</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>2.239620e-01</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>2.614255e-03</td>\n",
       "      <td>8.734784e-04</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>6.445584e-01</td>\n",
       "      <td>0.922678</td>\n",
       "      <td>9.933227e-06</td>\n",
       "      <td>4.853597e-05</td>\n",
       "      <td>1.666934e-05</td>\n",
       "      <td>2.462417e-06</td>\n",
       "      <td>1.905407e-01</td>\n",
       "      <td>8.292890e-06</td>\n",
       "      <td>8.724671e-05</td>\n",
       "      <td>0.099634</td>\n",
       "      <td>7.134842e-06</td>\n",
       "      <td>1.231409e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>9.999950e-01</td>\n",
       "      <td>0.923766</td>\n",
       "      <td>9.999970e-01</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.974799</td>\n",
       "      <td>2.214449e-04</td>\n",
       "      <td>8.968836e-01</td>\n",
       "      <td>0.141608</td>\n",
       "      <td>1.825624e-01</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>2.719495e-01</td>\n",
       "      <td>9.990975e-01</td>\n",
       "      <td>7.168865e-01</td>\n",
       "      <td>9.994874e-01</td>\n",
       "      <td>8.838552e-01</td>\n",
       "      <td>9.267399e-01</td>\n",
       "      <td>9.989949e-01</td>\n",
       "      <td>0.999571</td>\n",
       "      <td>9.292160e-01</td>\n",
       "      <td>1.456978e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.990453</td>\n",
       "      <td>7.210847e-04</td>\n",
       "      <td>9.974165e-01</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>9.723954e-01</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>9.485523e-01</td>\n",
       "      <td>7.265253e-03</td>\n",
       "      <td>3.298223e-05</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>3.002260e-04</td>\n",
       "      <td>2.857350e-04</td>\n",
       "      <td>1.813723e-03</td>\n",
       "      <td>0.999859</td>\n",
       "      <td>2.709757e-02</td>\n",
       "      <td>9.999926e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>9.999881e-01</td>\n",
       "      <td>0.886869</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>0.278497</td>\n",
       "      <td>1.413452e-03</td>\n",
       "      <td>9.985300e-01</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>9.998626e-01</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>9.997835e-01</td>\n",
       "      <td>9.999952e-01</td>\n",
       "      <td>1.304638e-04</td>\n",
       "      <td>9.999970e-01</td>\n",
       "      <td>9.982779e-01</td>\n",
       "      <td>9.987465e-01</td>\n",
       "      <td>9.997171e-01</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>9.877952e-01</td>\n",
       "      <td>2.779374e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>9.999994e-01</td>\n",
       "      <td>0.912460</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>0.984320</td>\n",
       "      <td>2.731469e-01</td>\n",
       "      <td>7.464710e-01</td>\n",
       "      <td>0.802343</td>\n",
       "      <td>7.061719e-01</td>\n",
       "      <td>0.996008</td>\n",
       "      <td>5.509360e-01</td>\n",
       "      <td>9.998620e-01</td>\n",
       "      <td>8.037775e-01</td>\n",
       "      <td>9.568705e-01</td>\n",
       "      <td>9.076113e-01</td>\n",
       "      <td>3.714641e-01</td>\n",
       "      <td>9.835144e-01</td>\n",
       "      <td>0.462684</td>\n",
       "      <td>3.897207e-02</td>\n",
       "      <td>1.726958e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.979875</td>\n",
       "      <td>0.539263</td>\n",
       "      <td>8.673732e-01</td>\n",
       "      <td>9.967802e-01</td>\n",
       "      <td>0.316683</td>\n",
       "      <td>1.841803e-01</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>6.494211e-02</td>\n",
       "      <td>9.881802e-01</td>\n",
       "      <td>9.248307e-01</td>\n",
       "      <td>9.835920e-01</td>\n",
       "      <td>3.185645e-03</td>\n",
       "      <td>2.644372e-04</td>\n",
       "      <td>9.999886e-01</td>\n",
       "      <td>0.225953</td>\n",
       "      <td>4.736198e-05</td>\n",
       "      <td>6.885214e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.934128</td>\n",
       "      <td>0.996983</td>\n",
       "      <td>3.443652e-05</td>\n",
       "      <td>7.327547e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.784814e-07</td>\n",
       "      <td>0.999732</td>\n",
       "      <td>1.390103e-08</td>\n",
       "      <td>5.073905e-01</td>\n",
       "      <td>1.633633e-08</td>\n",
       "      <td>2.217594e-01</td>\n",
       "      <td>9.074172e-01</td>\n",
       "      <td>1.225324e-01</td>\n",
       "      <td>8.958351e-01</td>\n",
       "      <td>0.814906</td>\n",
       "      <td>2.333156e-01</td>\n",
       "      <td>5.851128e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>9.995201e-01</td>\n",
       "      <td>0.901040</td>\n",
       "      <td>9.998468e-01</td>\n",
       "      <td>0.328823</td>\n",
       "      <td>0.998523</td>\n",
       "      <td>2.164039e-01</td>\n",
       "      <td>9.491063e-01</td>\n",
       "      <td>0.055259</td>\n",
       "      <td>3.131562e-01</td>\n",
       "      <td>0.140806</td>\n",
       "      <td>7.690450e-01</td>\n",
       "      <td>7.152768e-01</td>\n",
       "      <td>9.845268e-01</td>\n",
       "      <td>9.979879e-01</td>\n",
       "      <td>2.799568e-01</td>\n",
       "      <td>5.583459e-01</td>\n",
       "      <td>4.055885e-01</td>\n",
       "      <td>0.738170</td>\n",
       "      <td>9.140029e-01</td>\n",
       "      <td>9.546053e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>9.994294e-01</td>\n",
       "      <td>0.979857</td>\n",
       "      <td>9.999942e-01</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.998226</td>\n",
       "      <td>4.839577e-04</td>\n",
       "      <td>9.977396e-01</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>9.998423e-01</td>\n",
       "      <td>0.014088</td>\n",
       "      <td>9.998339e-01</td>\n",
       "      <td>9.209788e-01</td>\n",
       "      <td>1.632371e-05</td>\n",
       "      <td>9.999421e-01</td>\n",
       "      <td>2.611695e-01</td>\n",
       "      <td>6.656475e-01</td>\n",
       "      <td>2.072351e-01</td>\n",
       "      <td>0.999488</td>\n",
       "      <td>9.301651e-01</td>\n",
       "      <td>9.967242e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>9.985013e-01</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.252573</td>\n",
       "      <td>0.270605</td>\n",
       "      <td>4.177345e-07</td>\n",
       "      <td>9.956944e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.715670e-07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.013887e-08</td>\n",
       "      <td>9.999452e-01</td>\n",
       "      <td>1.351947e-09</td>\n",
       "      <td>9.503731e-01</td>\n",
       "      <td>9.989439e-01</td>\n",
       "      <td>9.969585e-01</td>\n",
       "      <td>9.996724e-01</td>\n",
       "      <td>0.510108</td>\n",
       "      <td>9.332535e-01</td>\n",
       "      <td>5.779290e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>5.479691e-01</td>\n",
       "      <td>0.831812</td>\n",
       "      <td>9.738950e-01</td>\n",
       "      <td>0.355249</td>\n",
       "      <td>0.847798</td>\n",
       "      <td>3.346031e-02</td>\n",
       "      <td>8.037581e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>8.600786e-01</td>\n",
       "      <td>0.176204</td>\n",
       "      <td>1.178343e-02</td>\n",
       "      <td>9.917070e-01</td>\n",
       "      <td>6.527213e-04</td>\n",
       "      <td>5.123333e-01</td>\n",
       "      <td>9.255978e-01</td>\n",
       "      <td>9.370759e-01</td>\n",
       "      <td>5.493162e-01</td>\n",
       "      <td>0.338772</td>\n",
       "      <td>5.859932e-01</td>\n",
       "      <td>3.287783e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>0.835811</td>\n",
       "      <td>9.999995e-01</td>\n",
       "      <td>0.490594</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>7.014070e-01</td>\n",
       "      <td>3.254855e-02</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>1.579665e-01</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>1.028931e-01</td>\n",
       "      <td>7.872433e-01</td>\n",
       "      <td>9.921652e-01</td>\n",
       "      <td>4.402867e-01</td>\n",
       "      <td>1.502271e-02</td>\n",
       "      <td>9.750821e-02</td>\n",
       "      <td>9.998955e-01</td>\n",
       "      <td>0.112548</td>\n",
       "      <td>1.571351e-02</td>\n",
       "      <td>1.980414e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.436333</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.012884</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>5.430749e-04</td>\n",
       "      <td>2.175986e-02</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>2.470908e-04</td>\n",
       "      <td>0.021632</td>\n",
       "      <td>5.665510e-01</td>\n",
       "      <td>9.863935e-01</td>\n",
       "      <td>9.986792e-01</td>\n",
       "      <td>9.998912e-01</td>\n",
       "      <td>9.966916e-01</td>\n",
       "      <td>9.937555e-01</td>\n",
       "      <td>5.954945e-04</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>9.915132e-01</td>\n",
       "      <td>9.998317e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>9.999995e-01</td>\n",
       "      <td>0.997660</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.007566</td>\n",
       "      <td>0.993312</td>\n",
       "      <td>1.493474e-04</td>\n",
       "      <td>9.982829e-01</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>9.990189e-01</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>9.950571e-01</td>\n",
       "      <td>9.999948e-01</td>\n",
       "      <td>4.915754e-04</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>9.985310e-01</td>\n",
       "      <td>9.988828e-01</td>\n",
       "      <td>9.994205e-01</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>9.962714e-01</td>\n",
       "      <td>5.054965e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>9.999981e-01</td>\n",
       "      <td>0.967733</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>0.355369</td>\n",
       "      <td>0.979441</td>\n",
       "      <td>1.329465e-01</td>\n",
       "      <td>9.685857e-01</td>\n",
       "      <td>0.800659</td>\n",
       "      <td>9.527197e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.807435e-01</td>\n",
       "      <td>9.999686e-01</td>\n",
       "      <td>9.020782e-01</td>\n",
       "      <td>9.986919e-01</td>\n",
       "      <td>9.932417e-01</td>\n",
       "      <td>9.975096e-01</td>\n",
       "      <td>9.995166e-01</td>\n",
       "      <td>0.644687</td>\n",
       "      <td>9.866655e-01</td>\n",
       "      <td>2.778176e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>5.513748e-08</td>\n",
       "      <td>0.665755</td>\n",
       "      <td>1.174142e-08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034603</td>\n",
       "      <td>9.999605e-01</td>\n",
       "      <td>9.994826e-01</td>\n",
       "      <td>0.135176</td>\n",
       "      <td>9.438224e-01</td>\n",
       "      <td>0.253242</td>\n",
       "      <td>3.213385e-07</td>\n",
       "      <td>4.344605e-08</td>\n",
       "      <td>2.371002e-01</td>\n",
       "      <td>1.256986e-09</td>\n",
       "      <td>1.832637e-06</td>\n",
       "      <td>1.579100e-03</td>\n",
       "      <td>1.061184e-04</td>\n",
       "      <td>0.854558</td>\n",
       "      <td>1.021122e-09</td>\n",
       "      <td>4.038845e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.194353</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.860118</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>9.841690e-01</td>\n",
       "      <td>6.474335e-04</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>5.035093e-04</td>\n",
       "      <td>0.857015</td>\n",
       "      <td>2.203493e-01</td>\n",
       "      <td>9.752623e-01</td>\n",
       "      <td>9.995641e-01</td>\n",
       "      <td>1.156092e-02</td>\n",
       "      <td>8.376536e-01</td>\n",
       "      <td>9.840236e-01</td>\n",
       "      <td>3.246557e-01</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>9.295815e-01</td>\n",
       "      <td>2.493776e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>4.514689e-02</td>\n",
       "      <td>0.999844</td>\n",
       "      <td>9.829219e-02</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.981693</td>\n",
       "      <td>6.149712e-01</td>\n",
       "      <td>8.217149e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.254349e-01</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>1.479901e-05</td>\n",
       "      <td>1.768205e-02</td>\n",
       "      <td>1.036665e-04</td>\n",
       "      <td>8.296981e-05</td>\n",
       "      <td>3.423075e-02</td>\n",
       "      <td>1.112360e-01</td>\n",
       "      <td>9.948124e-01</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>2.918106e-01</td>\n",
       "      <td>3.130289e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>9.999905e-01</td>\n",
       "      <td>0.999808</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.200327</td>\n",
       "      <td>0.050085</td>\n",
       "      <td>2.768207e-07</td>\n",
       "      <td>9.597699e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.420390e-11</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>7.862746e-11</td>\n",
       "      <td>6.743020e-02</td>\n",
       "      <td>1.883981e-10</td>\n",
       "      <td>9.994710e-01</td>\n",
       "      <td>2.139938e-01</td>\n",
       "      <td>6.098756e-03</td>\n",
       "      <td>1.582589e-03</td>\n",
       "      <td>0.928795</td>\n",
       "      <td>4.982027e-02</td>\n",
       "      <td>9.990647e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>9.852912e-01</td>\n",
       "      <td>0.957150</td>\n",
       "      <td>8.460226e-01</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>0.995615</td>\n",
       "      <td>8.212482e-01</td>\n",
       "      <td>9.042652e-05</td>\n",
       "      <td>0.018283</td>\n",
       "      <td>9.902616e-01</td>\n",
       "      <td>0.926492</td>\n",
       "      <td>9.839653e-01</td>\n",
       "      <td>5.238231e-04</td>\n",
       "      <td>8.565332e-01</td>\n",
       "      <td>1.652136e-04</td>\n",
       "      <td>8.808833e-01</td>\n",
       "      <td>1.574640e-02</td>\n",
       "      <td>9.586984e-05</td>\n",
       "      <td>0.225162</td>\n",
       "      <td>9.103803e-01</td>\n",
       "      <td>1.347866e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999919</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.137513</td>\n",
       "      <td>0.994096</td>\n",
       "      <td>2.740576e-07</td>\n",
       "      <td>9.891027e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.778894e-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.096510e-07</td>\n",
       "      <td>9.999985e-01</td>\n",
       "      <td>7.247409e-04</td>\n",
       "      <td>9.999679e-01</td>\n",
       "      <td>9.965875e-01</td>\n",
       "      <td>9.976386e-01</td>\n",
       "      <td>9.998993e-01</td>\n",
       "      <td>0.979211</td>\n",
       "      <td>9.688367e-01</td>\n",
       "      <td>2.240791e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1297 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              danr   CG14427           dan   CG43394     ImpL2          Nek2  \\\n",
       "0     1.000000e+00  0.952570  1.000000e+00  0.999968  0.999427  9.988462e-01   \n",
       "1     1.000000e+00  0.073005  1.000000e+00  0.260960  0.999972  9.924010e-01   \n",
       "2     1.000000e+00  0.999980  1.000000e+00  0.128240  0.998572  2.026050e-05   \n",
       "3     9.999962e-01  0.788806  9.998572e-01  1.000000  0.999795  9.279311e-01   \n",
       "4     1.000000e+00  0.999593  1.000000e+00  0.004106  0.999833  1.097144e-07   \n",
       "5     9.857821e-03  0.398339  6.580142e-03  0.995009  0.094408  9.904410e-01   \n",
       "6     9.970232e-01  0.999684  9.999852e-01  0.993155  0.999887  6.502574e-01   \n",
       "7     7.972035e-04  0.999992  3.349570e-05  0.999668  0.999975  8.212480e-06   \n",
       "8     2.495757e-03  0.182013  4.974751e-02  0.004361  0.016997  9.591834e-03   \n",
       "9     4.811024e-03  0.999988  2.724231e-04  0.999914  0.999469  4.158520e-04   \n",
       "10    9.999993e-01  0.997766  1.000000e+00  0.668968  0.999614  2.553989e-02   \n",
       "11    1.000000e+00  0.143619  1.000000e+00  0.001303  0.999999  3.550558e-04   \n",
       "12    9.999760e-01  0.994280  9.999995e-01  0.009707  0.996073  6.270727e-04   \n",
       "13    2.865134e-03  0.638284  1.317361e-04  0.999929  0.017298  9.998714e-01   \n",
       "14    4.615304e-03  0.999890  9.990397e-01  0.999935  0.999414  5.893366e-02   \n",
       "15    1.393367e-07  0.005413  2.693647e-08  0.999980  0.001292  9.999983e-01   \n",
       "16    1.000000e+00  0.999933  1.000000e+00  0.036880  0.999848  1.459149e-06   \n",
       "17    9.812086e-01  0.564072  9.999998e-01  0.000162  0.116492  1.887775e-06   \n",
       "18    1.000000e+00  0.996334  1.000000e+00  0.504082  0.992520  2.282194e-01   \n",
       "19    9.995290e-01  0.133104  1.000000e+00  0.996271  0.051228  9.970793e-01   \n",
       "20    1.000000e+00  0.809125  1.000000e+00  0.000241  0.982447  7.849649e-05   \n",
       "21    1.000000e+00  0.999863  1.000000e+00  0.751584  1.000000  5.413376e-05   \n",
       "22    9.997407e-01  0.999762  9.999999e-01  0.841562  0.978401  3.264277e-06   \n",
       "23    9.996756e-01  0.961402  9.999933e-01  0.957463  0.997574  8.018143e-01   \n",
       "24    7.357590e-04  0.999996  1.309942e-05  0.999868  0.999922  5.366280e-04   \n",
       "25    9.966834e-01  0.993407  6.423675e-01  0.999941  0.999382  3.252073e-01   \n",
       "26    1.000000e+00  0.999975  1.000000e+00  0.999989  1.000000  9.976103e-01   \n",
       "27    9.999926e-01  0.999962  1.000000e+00  0.521378  0.997482  1.212213e-05   \n",
       "28    6.290200e-07  0.074248  1.633305e-09  1.000000  0.000110  1.000000e+00   \n",
       "29    9.984952e-01  0.996364  9.999988e-01  0.000521  0.998054  5.185466e-05   \n",
       "...            ...       ...           ...       ...       ...           ...   \n",
       "1267  9.998910e-01  0.996986  1.000000e+00  0.805672  0.482736  3.877808e-06   \n",
       "1268  9.996731e-01  0.859843  9.997267e-01  0.883014  0.886068  5.279544e-01   \n",
       "1269  1.244802e-02  0.992493  1.813867e-02  0.999990  0.585667  9.998382e-01   \n",
       "1270  9.995771e-01  0.997964  1.000000e+00  0.102224  0.010748  6.285930e-08   \n",
       "1271  1.000000e+00  0.069769  1.000000e+00  0.998935  0.992937  9.528593e-01   \n",
       "1272  9.999919e-01  0.999857  1.000000e+00  0.132667  0.713718  9.492947e-06   \n",
       "1273  1.000000e+00  0.443121  1.000000e+00  0.997710  0.999042  9.966443e-01   \n",
       "1274  9.999567e-01  0.991575  9.997956e-01  0.993017  0.999459  9.907833e-01   \n",
       "1275  9.999980e-01  0.988574  1.000000e+00  0.001011  0.986739  9.421500e-04   \n",
       "1276  7.404959e-03  0.999955  2.239620e-01  0.999918  0.999655  2.614255e-03   \n",
       "1277  9.999950e-01  0.923766  9.999970e-01  0.010001  0.974799  2.214449e-04   \n",
       "1278  9.999999e-01  0.997800  1.000000e+00  0.003965  0.990453  7.210847e-04   \n",
       "1279  9.999881e-01  0.886869  1.000000e+00  0.002989  0.278497  1.413452e-03   \n",
       "1280  9.999994e-01  0.912460  1.000000e+00  0.368600  0.984320  2.731469e-01   \n",
       "1281  1.000000e+00  0.996180  1.000000e+00  0.979875  0.539263  8.673732e-01   \n",
       "1282  1.000000e+00  0.999991  1.000000e+00  0.934128  0.996983  3.443652e-05   \n",
       "1283  9.995201e-01  0.901040  9.998468e-01  0.328823  0.998523  2.164039e-01   \n",
       "1284  9.994294e-01  0.979857  9.999942e-01  0.000463  0.998226  4.839577e-04   \n",
       "1285  9.985013e-01  0.997169  1.000000e+00  0.252573  0.270605  4.177345e-07   \n",
       "1286  5.479691e-01  0.831812  9.738950e-01  0.355249  0.847798  3.346031e-02   \n",
       "1287  9.999999e-01  0.835811  9.999995e-01  0.490594  0.999739  7.014070e-01   \n",
       "1288  1.000000e+00  0.436333  1.000000e+00  0.012884  0.999974  5.430749e-04   \n",
       "1289  9.999995e-01  0.997660  1.000000e+00  0.007566  0.993312  1.493474e-04   \n",
       "1290  9.999981e-01  0.967733  9.999999e-01  0.355369  0.979441  1.329465e-01   \n",
       "1291  5.513748e-08  0.665755  1.174142e-08  1.000000  0.034603  9.999605e-01   \n",
       "1292  1.000000e+00  0.194353  1.000000e+00  0.860118  0.999990  9.841690e-01   \n",
       "1293  4.514689e-02  0.999844  9.829219e-02  0.999996  0.981693  6.149712e-01   \n",
       "1294  9.999905e-01  0.999808  1.000000e+00  0.200327  0.050085  2.768207e-07   \n",
       "1295  9.852912e-01  0.957150  8.460226e-01  0.999964  0.995615  8.212482e-01   \n",
       "1296  1.000000e+00  0.999919  1.000000e+00  0.137513  0.994096  2.740576e-07   \n",
       "\n",
       "            CG8147       Ama        Btk29A       trn          numb  \\\n",
       "0     2.225487e-06  0.000144  8.976961e-01  0.910867  9.992010e-01   \n",
       "1     4.919980e-04  0.003902  4.686632e-02  0.999998  3.026228e-01   \n",
       "2     7.032250e-03  0.999997  5.656553e-06  1.000000  2.185221e-05   \n",
       "3     6.623404e-07  0.001171  9.910707e-01  0.174682  9.992439e-01   \n",
       "4     1.467953e-01  0.999978  1.737630e-09  0.986156  4.429802e-06   \n",
       "5     9.999146e-01  0.999999  7.737213e-01  0.999999  1.550446e-04   \n",
       "6     1.507457e-03  0.459570  2.764302e-01  0.912435  6.476845e-01   \n",
       "7     1.815548e-04  0.999999  9.984673e-01  0.999898  2.098095e-04   \n",
       "8     9.999360e-01  0.999893  9.998872e-01  0.000308  9.999392e-01   \n",
       "9     1.917854e-04  0.999983  9.913943e-01  0.999727  3.918419e-04   \n",
       "10    9.212085e-03  0.998669  9.818991e-05  0.957203  1.136290e-03   \n",
       "11    2.620373e-03  0.000313  4.401929e-03  0.685028  9.751267e-01   \n",
       "12    9.107602e-01  0.999934  9.996758e-01  0.999992  9.931347e-01   \n",
       "13    9.981104e-01  0.038774  7.815810e-02  0.244115  2.733906e-03   \n",
       "14    1.338275e-03  1.000000  9.659755e-01  0.243339  1.676398e-03   \n",
       "15    9.999737e-01  0.007292  4.211594e-01  0.000633  4.940522e-06   \n",
       "16    8.769456e-02  1.000000  1.367542e-08  0.000287  7.736443e-08   \n",
       "17    9.952095e-01  0.999705  9.984468e-01  0.999681  9.998662e-01   \n",
       "18    9.886728e-01  0.994003  9.966336e-01  0.999992  9.993253e-01   \n",
       "19    2.451493e-03  0.989910  9.763867e-01  0.046796  9.990502e-01   \n",
       "20    8.919598e-01  0.999904  9.998344e-01  0.991347  9.999064e-01   \n",
       "21    5.347388e-02  1.000000  2.750275e-09  1.000000  1.600709e-07   \n",
       "22    9.080054e-01  1.000000  4.722379e-08  1.000000  1.774804e-08   \n",
       "23    8.378164e-01  0.903518  1.236204e-03  0.007377  5.552902e-03   \n",
       "24    1.939070e-05  0.999997  9.998235e-01  0.999912  6.849265e-04   \n",
       "25    3.819732e-06  0.011503  9.953775e-01  0.963407  9.395645e-01   \n",
       "26    4.243368e-07  0.000007  9.880773e-01  0.999090  9.996192e-01   \n",
       "27    1.623061e-01  1.000000  6.982784e-07  0.991716  1.571913e-06   \n",
       "28    9.999992e-01  0.008219  2.049086e-01  0.995819  9.575192e-07   \n",
       "29    9.917101e-01  0.999947  9.992722e-01  0.994933  9.989964e-01   \n",
       "...            ...       ...           ...       ...           ...   \n",
       "1267  9.159855e-01  1.000000  7.310827e-09  0.996079  2.904256e-08   \n",
       "1268  9.991554e-01  0.938228  6.531123e-01  1.000000  8.854854e-01   \n",
       "1269  9.997755e-01  0.082235  5.800074e-02  0.999999  2.697955e-03   \n",
       "1270  9.982022e-01  1.000000  2.397281e-10  0.999720  1.219459e-10   \n",
       "1271  1.127196e-05  0.000523  7.323690e-04  0.994680  9.341871e-01   \n",
       "1272  9.943183e-01  0.999998  1.949519e-05  0.015029  1.579181e-06   \n",
       "1273  5.262723e-06  0.000066  6.644081e-04  0.998826  9.184116e-01   \n",
       "1274  8.947784e-01  0.010496  7.641231e-02  0.990980  2.616576e-02   \n",
       "1275  9.824730e-01  0.995236  9.953833e-01  0.265717  9.966738e-01   \n",
       "1276  8.734784e-04  0.999997  6.445584e-01  0.922678  9.933227e-06   \n",
       "1277  8.968836e-01  0.141608  1.825624e-01  0.999910  2.719495e-01   \n",
       "1278  9.974165e-01  0.999943  9.723954e-01  0.000271  9.485523e-01   \n",
       "1279  9.985300e-01  0.999946  9.998626e-01  0.999993  9.997835e-01   \n",
       "1280  7.464710e-01  0.802343  7.061719e-01  0.996008  5.509360e-01   \n",
       "1281  9.967802e-01  0.316683  1.841803e-01  0.999999  6.494211e-02   \n",
       "1282  7.327547e-04  1.000000  1.784814e-07  0.999732  1.390103e-08   \n",
       "1283  9.491063e-01  0.055259  3.131562e-01  0.140806  7.690450e-01   \n",
       "1284  9.977396e-01  0.999772  9.998423e-01  0.014088  9.998339e-01   \n",
       "1285  9.956944e-01  1.000000  1.715670e-07  1.000000  1.013887e-08   \n",
       "1286  8.037581e-01  0.999998  8.600786e-01  0.176204  1.178343e-02   \n",
       "1287  3.254855e-02  0.006348  1.579665e-01  0.999986  1.028931e-01   \n",
       "1288  2.175986e-02  0.001911  2.470908e-04  0.021632  5.665510e-01   \n",
       "1289  9.982829e-01  0.999314  9.990189e-01  0.999993  9.950571e-01   \n",
       "1290  9.685857e-01  0.800659  9.527197e-01  1.000000  9.807435e-01   \n",
       "1291  9.994826e-01  0.135176  9.438224e-01  0.253242  3.213385e-07   \n",
       "1292  6.474335e-04  0.006079  5.035093e-04  0.857015  2.203493e-01   \n",
       "1293  8.217149e-01  1.000000  1.254349e-01  0.999526  1.479901e-05   \n",
       "1294  9.597699e-01  1.000000  2.420390e-11  0.001580  7.862746e-11   \n",
       "1295  9.042652e-05  0.018283  9.902616e-01  0.926492  9.839653e-01   \n",
       "1296  9.891027e-01  1.000000  3.778894e-06  1.000000  5.096510e-07   \n",
       "\n",
       "               prd           brk           tsh           pxb           dpn  \\\n",
       "0     4.948604e-06  9.973761e-01  1.085652e-04  4.932669e-04  1.096998e-04   \n",
       "1     5.767848e-01  9.956729e-01  3.016724e-02  6.589219e-04  9.117642e-03   \n",
       "2     9.262726e-01  1.872751e-04  9.973971e-01  9.825932e-01  5.684511e-02   \n",
       "3     1.558326e-05  9.878981e-01  5.819520e-07  5.616950e-01  2.520103e-03   \n",
       "4     9.999733e-01  1.644533e-03  9.999981e-01  9.961914e-01  9.938365e-01   \n",
       "5     8.348986e-03  1.097112e-04  6.478760e-03  4.778898e-04  8.386919e-01   \n",
       "6     8.231743e-04  6.520438e-01  1.139168e-02  2.286469e-03  1.270115e-05   \n",
       "7     5.091575e-05  2.507595e-06  1.720824e-05  9.998615e-01  2.240034e-04   \n",
       "8     7.474542e-01  3.679442e-06  9.718327e-01  1.333877e-01  4.582791e-01   \n",
       "9     1.168152e-04  1.891582e-04  4.295379e-06  9.955487e-01  1.478557e-05   \n",
       "10    8.066000e-01  6.916312e-03  9.018955e-01  9.928236e-01  9.150981e-01   \n",
       "11    9.994123e-01  9.996619e-01  9.993830e-01  9.965026e-01  9.987382e-01   \n",
       "12    9.981224e-01  1.047253e-05  9.999948e-01  9.814327e-01  9.147418e-01   \n",
       "13    1.631580e-02  9.454259e-01  1.690295e-03  2.211262e-02  9.957338e-01   \n",
       "14    3.930203e-06  2.040161e-06  3.323668e-07  6.603202e-03  2.577902e-05   \n",
       "15    6.464802e-09  2.028336e-01  7.572076e-06  5.822034e-07  3.205048e-04   \n",
       "16    9.425172e-04  8.211447e-09  9.999952e-01  9.757390e-04  8.728823e-06   \n",
       "17    9.999949e-01  4.112371e-05  9.999994e-01  9.923676e-01  9.695355e-01   \n",
       "18    9.994853e-01  1.097422e-02  9.998964e-01  4.127295e-04  6.243932e-04   \n",
       "19    6.390570e-03  6.596552e-03  2.045522e-06  9.012696e-01  4.779747e-01   \n",
       "20    9.996842e-01  1.517768e-04  9.999899e-01  7.483673e-04  2.139357e-04   \n",
       "21    9.986628e-01  2.014484e-06  9.999235e-01  1.959178e-01  3.780622e-03   \n",
       "22    9.993616e-01  1.896762e-08  9.978809e-01  9.166709e-01  9.140667e-01   \n",
       "23    1.163220e-01  8.741810e-01  8.932080e-01  1.407525e-02  3.657069e-02   \n",
       "24    3.447213e-06  6.536806e-07  1.636066e-06  9.993469e-01  1.056744e-06   \n",
       "25    5.354946e-05  5.687503e-01  5.921737e-06  9.194887e-01  1.795016e-04   \n",
       "26    1.085425e-06  9.997811e-01  9.024175e-05  3.737268e-05  5.879258e-08   \n",
       "27    9.989116e-01  1.794630e-06  9.994752e-01  9.997467e-01  9.932792e-01   \n",
       "28    6.270267e-07  9.988130e-01  2.106784e-08  3.456530e-06  9.997488e-01   \n",
       "29    9.999472e-01  3.654560e-05  9.999973e-01  9.992108e-01  9.995689e-01   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1267  9.909118e-01  2.973403e-10  9.461822e-01  9.003154e-01  5.748441e-01   \n",
       "1268  9.999777e-01  9.516612e-01  7.897650e-01  9.958682e-01  9.851004e-01   \n",
       "1269  5.853162e-02  9.996061e-01  1.145558e-03  7.614732e-03  9.875183e-01   \n",
       "1270  9.194591e-01  6.604906e-13  9.991876e-01  4.916412e-02  1.357784e-03   \n",
       "1271  9.999479e-01  9.999354e-01  9.691881e-06  9.999770e-01  9.999853e-01   \n",
       "1272  8.556538e-03  6.250071e-05  9.999804e-01  6.206724e-05  5.777020e-05   \n",
       "1273  9.619719e-01  9.996790e-01  6.026992e-06  9.720117e-01  9.988384e-01   \n",
       "1274  8.328405e-01  9.995334e-01  9.509410e-01  1.745957e-01  4.448363e-01   \n",
       "1275  7.139744e-01  6.418411e-04  9.999890e-01  6.617320e-03  1.762567e-03   \n",
       "1276  4.853597e-05  1.666934e-05  2.462417e-06  1.905407e-01  8.292890e-06   \n",
       "1277  9.990975e-01  7.168865e-01  9.994874e-01  8.838552e-01  9.267399e-01   \n",
       "1278  7.265253e-03  3.298223e-05  9.999999e-01  3.002260e-04  2.857350e-04   \n",
       "1279  9.999952e-01  1.304638e-04  9.999970e-01  9.982779e-01  9.987465e-01   \n",
       "1280  9.998620e-01  8.037775e-01  9.568705e-01  9.076113e-01  3.714641e-01   \n",
       "1281  9.881802e-01  9.248307e-01  9.835920e-01  3.185645e-03  2.644372e-04   \n",
       "1282  5.073905e-01  1.633633e-08  2.217594e-01  9.074172e-01  1.225324e-01   \n",
       "1283  7.152768e-01  9.845268e-01  9.979879e-01  2.799568e-01  5.583459e-01   \n",
       "1284  9.209788e-01  1.632371e-05  9.999421e-01  2.611695e-01  6.656475e-01   \n",
       "1285  9.999452e-01  1.351947e-09  9.503731e-01  9.989439e-01  9.969585e-01   \n",
       "1286  9.917070e-01  6.527213e-04  5.123333e-01  9.255978e-01  9.370759e-01   \n",
       "1287  7.872433e-01  9.921652e-01  4.402867e-01  1.502271e-02  9.750821e-02   \n",
       "1288  9.863935e-01  9.986792e-01  9.998912e-01  9.966916e-01  9.937555e-01   \n",
       "1289  9.999948e-01  4.915754e-04  9.999998e-01  9.985310e-01  9.988828e-01   \n",
       "1290  9.999686e-01  9.020782e-01  9.986919e-01  9.932417e-01  9.975096e-01   \n",
       "1291  4.344605e-08  2.371002e-01  1.256986e-09  1.832637e-06  1.579100e-03   \n",
       "1292  9.752623e-01  9.995641e-01  1.156092e-02  8.376536e-01  9.840236e-01   \n",
       "1293  1.768205e-02  1.036665e-04  8.296981e-05  3.423075e-02  1.112360e-01   \n",
       "1294  6.743020e-02  1.883981e-10  9.994710e-01  2.139938e-01  6.098756e-03   \n",
       "1295  5.238231e-04  8.565332e-01  1.652136e-04  8.808833e-01  1.574640e-02   \n",
       "1296  9.999985e-01  7.247409e-04  9.999679e-01  9.965875e-01  9.976386e-01   \n",
       "\n",
       "               ftz        Kr             h           eve  \n",
       "0     3.081364e-06  0.692830  9.230255e-01  1.595170e-04  \n",
       "1     9.999350e-01  0.033744  1.860791e-03  2.594135e-05  \n",
       "2     9.997877e-01  0.999295  1.272992e-02  1.028869e-05  \n",
       "3     8.022946e-08  0.036852  9.923142e-01  4.249081e-05  \n",
       "4     9.665008e-01  0.999558  9.889327e-01  7.089722e-01  \n",
       "5     9.999241e-01  0.009804  3.880746e-03  9.181547e-07  \n",
       "6     1.181825e-03  0.920938  2.793467e-04  2.107628e-02  \n",
       "7     9.414664e-06  0.989408  1.642943e-04  5.226656e-06  \n",
       "8     3.363704e-02  0.946534  9.827011e-01  9.960330e-01  \n",
       "9     1.859974e-04  0.633407  9.499377e-06  4.998521e-07  \n",
       "10    2.341511e-01  0.963215  7.768021e-01  5.764536e-01  \n",
       "11    1.185822e-01  0.995253  9.969323e-01  9.813778e-01  \n",
       "12    9.998261e-01  0.999960  8.847806e-01  3.586725e-03  \n",
       "13    3.480875e-03  0.027029  2.698334e-02  1.070119e-01  \n",
       "14    7.779146e-06  0.004214  2.031399e-03  1.548131e-04  \n",
       "15    3.045832e-06  0.999865  1.738679e-07  9.803601e-05  \n",
       "16    9.196917e-04  0.999866  5.957111e-03  9.998777e-01  \n",
       "17    9.997806e-01  0.999988  5.820566e-01  1.634441e-05  \n",
       "18    9.998981e-01  0.995208  6.144069e-01  2.557264e-04  \n",
       "19    4.163624e-07  0.000225  9.997198e-01  8.621904e-02  \n",
       "20    9.998947e-01  0.999401  1.370329e-03  2.224371e-01  \n",
       "21    9.999964e-01  0.999053  3.804333e-02  2.570437e-04  \n",
       "22    9.999651e-01  0.816317  9.456939e-01  6.941300e-06  \n",
       "23    2.596068e-01  0.191250  1.999731e-01  9.880601e-01  \n",
       "24    3.958464e-06  0.997777  9.571102e-07  9.843995e-07  \n",
       "25    6.412070e-06  0.818450  1.410119e-02  2.079197e-05  \n",
       "26    1.925434e-06  0.999434  2.634638e-04  7.155194e-06  \n",
       "27    8.784008e-01  0.998702  9.942315e-01  1.421364e-01  \n",
       "28    2.526374e-01  0.000324  6.032673e-06  1.446585e-07  \n",
       "29    9.569515e-01  0.999960  9.983876e-01  4.413893e-01  \n",
       "...            ...       ...           ...           ...  \n",
       "1267  8.968162e-01  0.106455  9.168677e-01  3.899791e-02  \n",
       "1268  9.999923e-01  0.010582  7.781755e-01  2.496092e-07  \n",
       "1269  9.999026e-01  0.000037  5.571116e-02  2.419810e-07  \n",
       "1270  9.991697e-01  0.996749  2.150211e-03  2.735774e-04  \n",
       "1271  3.738622e-05  0.000017  9.999276e-01  7.104409e-03  \n",
       "1272  1.952201e-01  0.999779  1.425272e-05  9.174622e-01  \n",
       "1273  1.963415e-02  0.000260  9.988311e-01  1.126539e-03  \n",
       "1274  9.806773e-01  0.531551  6.200535e-01  1.029146e-02  \n",
       "1275  7.551752e-01  0.999513  3.736563e-03  9.415157e-01  \n",
       "1276  8.724671e-05  0.099634  7.134842e-06  1.231409e-04  \n",
       "1277  9.989949e-01  0.999571  9.292160e-01  1.456978e-03  \n",
       "1278  1.813723e-03  0.999859  2.709757e-02  9.999926e-01  \n",
       "1279  9.997171e-01  0.999874  9.877952e-01  2.779374e-04  \n",
       "1280  9.835144e-01  0.462684  3.897207e-02  1.726958e-02  \n",
       "1281  9.999886e-01  0.225953  4.736198e-05  6.885214e-06  \n",
       "1282  8.958351e-01  0.814906  2.333156e-01  5.851128e-03  \n",
       "1283  4.055885e-01  0.738170  9.140029e-01  9.546053e-01  \n",
       "1284  2.072351e-01  0.999488  9.301651e-01  9.967242e-01  \n",
       "1285  9.996724e-01  0.510108  9.332535e-01  5.779290e-06  \n",
       "1286  5.493162e-01  0.338772  5.859932e-01  3.287783e-01  \n",
       "1287  9.998955e-01  0.112548  1.571351e-02  1.980414e-04  \n",
       "1288  5.954945e-04  0.999694  9.915132e-01  9.998317e-01  \n",
       "1289  9.994205e-01  0.999978  9.962714e-01  5.054965e-03  \n",
       "1290  9.995166e-01  0.644687  9.866655e-01  2.778176e-05  \n",
       "1291  1.061184e-04  0.854558  1.021122e-09  4.038845e-10  \n",
       "1292  3.246557e-01  0.020535  9.295815e-01  2.493776e-01  \n",
       "1293  9.948124e-01  0.000095  2.918106e-01  3.130289e-05  \n",
       "1294  1.582589e-03  0.928795  4.982027e-02  9.990647e-01  \n",
       "1295  9.586984e-05  0.225162  9.103803e-01  1.347866e-04  \n",
       "1296  9.998993e-01  0.979211  9.688367e-01  2.240791e-06  \n",
       "\n",
       "[1297 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NNMF to select best 20 genes using clustering over the feature matrix.\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#We partition b to two matrices. H will be the 10 special location characteristics of each in-situ gene (we assume\n",
    "# that b contains some hidden location information).\n",
    "model = NMF(n_components=10)\n",
    "#b.shape is (rows, columns) = (3039, 84)\n",
    "W = model.fit_transform(b)\n",
    "H = model.components_\n",
    "\n",
    "print(H.shape)\n",
    "print(np.shape(np.matmul(W,H)))\n",
    "print(np.matmul(W,H)[0])\n",
    "print(b.iloc[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "#We transpose H in order to get 84 samples (rows), and we use clustering to find the most dominant ones.\n",
    "kmeans = KMeans(n_clusters=20).fit(H.T)\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, H.T)\n",
    "print(closest)\n",
    "print(b.columns[closest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glist_20 = [3,16,80,77,19,52,53,57,78,68,62,0,75,21,66,26,81,51,63,7]\n",
    "glist_60 = [3,16,80,77,19,52,53,57,78,68,62,0,75,21,66,26,81,51,63,7,8,56,35,18,83,6,1,61,65,55,74,22,64,20,59,23,79,48,58,31,69,73,76,24,33,17,47,14,25,15,67,42,54,46,50,28,27,49,43,13]\n",
    "glist_20_knn_ver_1 = [39,64,56,30,3,70,79,14,27,67,16,59,73,19,44,49,83,24,40,35]\n",
    "glist_20_knn_ver_2 = [60,8,38,26,16,79,65,43,83,76,73,2,24,17,3,48,82,36,30,78]\n",
    "print(b.columns[glist_20_knn_ver_2])\n",
    "print(b.columns[list(set(glist_60) & set(glist_20_knn_ver_2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model check on other samples as requested by Avner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 31 22:29:24 2018 Selecting samples from d\n",
      "Wed Oct 31 22:29:24 2018 len(d_list): 100000\n"
     ]
    }
   ],
   "source": [
    "#Select 500000 samples from d.\n",
    "print(time.ctime(), 'Selecting samples from d')\n",
    "indicies = random.sample(range(len(d_false)), 100000)\n",
    "d_list = [d_false[i] for i in indicies]\n",
    "random.shuffle(d_list)\n",
    "len_list = len(d_list)\n",
    "print(time.ctime(), f'len(d_list): {len_list}')\n",
    "\n",
    "# Now need to run cell 'Create train input arrays'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 40s 405us/step\n",
      "Score: 0.31402943283319473, acc: 0.8329299947023392\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#model = load_model('c:\\\\data\\\\Dream\\\\2-84_genes_new_model\\\\model_sav.h5')\n",
    "score, acc = model.evaluate(x=[X1_train, X2_train, X3_train], y=y_train, batch_size=50)\n",
    "print(f'Score: {score}, acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxMCC handle cases with more than ten indices, use ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle rows with more than 10 indices: build data for prediction using ANN.\n",
    "num_situ = 60\n",
    "\n",
    "#Create the pairs. Left: d-array, right: b-array \n",
    "#Labels start from 0 in the original file. They indicate a specific row in b table.\n",
    "print(time.ctime(),'Create list of tuples')\n",
    "#labels.pkl contains a dictionary mapping of all 1270 cells to (possibly few) locations in [0,3038].\n",
    "pkl_file = open(f'data/labels_using_maxcc_{num_situ}.pkl', 'rb')\n",
    "ind_load = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "data_ind = pd.DataFrame(list(ind_load.items()))\n",
    "data_ind.drop([0], axis=1, inplace=True)\n",
    "data_ind[1] = [np.ndarray.flatten(data_ind[1][i]) for i in range(len(data_ind))]\n",
    "model = load_model('data/models/weights-improvement60-102-0.85.hdf5')\n",
    "\n",
    "loop = 0\n",
    "d_true = pd.DataFrame()\n",
    "#i is index in dge.\n",
    "for i in range(len(data_ind)):\n",
    "    if(loop%100 == 0):\n",
    "        print(loop, ' ', end=\"\")\n",
    "    loop = loop + 1\n",
    "    one_row = pd.DataFrame()\n",
    "    #j is index in bdtnp\n",
    "    for j in data_ind.iloc[i].iloc[0]:\n",
    "        pred = model.predict([bdtnp.iloc[j][glist_60_tom][np.newaxis,:],\n",
    "                              d.iloc[i][glist_60_tom][np.newaxis,:],\n",
    "                              d.iloc[i][np.newaxis,:]], batch_size=1)\n",
    "        one_row = pd.concat([one_row, pd.DataFrame([i,j,pred[0][0]]).T])\n",
    "    one_row.columns=['i', 'j', 'pred']\n",
    "    one_row = one_row.nlargest(10, 'pred')\n",
    "    one_row_df = pd.DataFrame(one_row.j).T.reset_index(drop=True)\n",
    "    d_true = pd.concat([d_true, one_row_df])\n",
    "\n",
    "d_true.to_csv('data/maxmcc_10_selected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
