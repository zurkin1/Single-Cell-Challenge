{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Model For Geometry Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydot\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Flatten, Dropout, Lambda, Activation, BatchNormalization, LocallyConnected1D, Reshape, AlphaDropout, Conv1D, MaxPooling1D\n",
    "from keras.activations import relu\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "num_situ = 20\n",
    "num_all = 8924\n",
    "glist_60 = [3,16,80,77,19,52,53,57,78,68,62,0,75,21,66,26,81,51,63,7,8,56,35,18,83,6,1,61,65,55,74,22,64,20,59,23,79,48,58,31,69,73,76,24,33,17,47,14,25,15,67,42,54,46,50,28,27,49,43,13]\n",
    "glist_60_tom = ['kni','Ance','brk','cad','eve','fkh','hb','hkb','ImpE2','oc','sna','srp','twi','zen','zfh1','Blimp-1','croc','D','Dfd','Doc3','dpn','fj','ftz','gt','h','ken','knrl','Kr','odd','peb','run','tkv','tll','tsh','zen2','Antp','apt','bowl','CG14427','CG17724','CG17786','CG8147','Cyp310a1','dan','disco','Doc2','E(spl)m5-HLH','Ilp4','ImpL2','Mes2','NetA','prd','rau','rho','toc','trn','aay','gk','ems','numb']\n",
    "glist_40_tom = ['kni','Ance','brk','cad','eve','fkh','hb','hkb','ImpE2','oc','sna','srp','twi','zen','zfh1','Blimp-1','croc','D','Dfd','Doc3','dpn','fj','ftz','gt','h','ken','knrl','Kr','odd','peb','run','tkv','tll','tsh','zen2','trn','E(spl)m5-HLH','CG17724','disco','dan']\n",
    "glist_20_tom = ['kni','Ance','brk','cad','eve','fkh','hb','hkb','ImpE2','oc','sna','srp','twi','zen','zfh1','gt','Kr','ftz','tkv','croc']\n",
    "glist = ['danr','CG14427','dan','CG43394','ImpL2','Nek2','CG8147','Ama','Btk29A','trn','numb','prd','brk','tsh','pxb','dpn','ftz','Kr','h','eve','Traf4','run','Blimp-1','lok','kni','tkv','MESR3','odd','noc','nub','Ilp4','aay','twi','bmm','hb','toc','rho','CG10479','gt','gk']\n",
    "glist_20 = ['danr','CG14427','dan','CG43394','ImpL2','Nek2','CG8147','Ama','Btk29A','trn','numb','prd','brk','tsh','pxb','dpn','ftz','Kr','h','eve']\n",
    "glist_84 = ['aay','Ama','Ance','Antp','apt','Blimp-1','bmm','bowl','brk','Btk29A','bun','cad','CenG1A','CG10479','CG11208','CG14427','CG17724','CG17786','CG43394','CG8147','cnc','croc','Cyp310a1','D','dan','danr','Dfd','disco','Doc2','Doc3','dpn','edl','ems','erm','Esp','E(spl)m5-HLH','eve','exex','fj','fkh','ftz','gk','gt','h','hb','hkb','htl','Ilp4','ImpE2','ImpL2','ken','kni','knrl','Kr','lok','Mdr49','Mes2','MESR3','mfas','Nek2','NetA','noc','nub','numb','oc','odd','peb','prd','pxb','rau','rho','run','sna','srp','tkv','tll','toc','Traf4','trn','tsh','twi','zen','zen2','zfh1']\n",
    "glist_40 = ['danr','CG14427','dan','CG43394','ImpL2','Nek2','CG8147','Ama','Btk29A','trn','numb','prd','brk','tsh','pxb','dpn','ftz','Kr','h','eve','Traf4','run','Blimp-1','lok','kni','tkv','MESR3','odd','noc','nub','Ilp4','aay','twi','bmm','hb','toc','rho','CG10479','gt','gk']\n",
    "glist_mcc_20 = ['run','h','noc','Traf4','pxb','aay','Btk29A','trn','odd','CG43394','bun','dpn','nub','CG10479','CG8147','Antp','ImpL2','kni','eve','CG14427']\n",
    "\n",
    "def diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]\n",
    "\n",
    "geometry = pd.read_csv('data/geometry.csv')\n",
    "\n",
    "#Changes 'na' to 'naa' and 'nan' to 'nana'\n",
    "#d = pd.read_csv('data/dge_raw.csv', index_col=0, header=None, encoding='ISO-8859-1').T\n",
    "d = pd.read_csv('data/magic_dge.csv')\n",
    "#d1_bin = pd.read_csv('data/dge_binarized_distMap_T.csv')\n",
    "#d2_bin = pd.read_csv('data/magic_dge_bin.csv')[glist_84]\n",
    "labels = pd.read_csv('data/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 17 23:31:23 2018 Create train input array for dge to bdtnp model\n",
      "0  100  200  300  400  500  600  700  800  900  1000  1100  1200  "
     ]
    }
   ],
   "source": [
    "print(time.ctime(), 'Create train input array for dge to geometry model')\n",
    "\n",
    "len_ = len(labels)\n",
    "X_ = np.empty((len_, num_all))\n",
    "y_ = np.empty((len_, 3))\n",
    "\n",
    "for index, row in labels.iterrows():\n",
    "    if (index % 100 == 0):\n",
    "        print(index, ' ', end=\"\")\n",
    "    #X_[index] = d_bin.iloc[index][glist_20]\n",
    "    Z_[index] = d1_bin.iloc[index] #[diff(glist_84, glist_20)]\n",
    "    #W_[index] = d2_bin.iloc[index]\n",
    "    y_[index] = bdtnp_bin.iloc[int(row[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model build\n",
    "print(time.strftime(\"%H:%M:%S\"), ' Model build')\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weigts = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "def my_accuracy(y_true, y_pred):\n",
    "    cnt=K.sum(class_weigts)    \n",
    "    err=K.sum(K.not_equal(K.argmax(y_pred,axis=-1)*class_weigts,K.argmax(y_true,axis=-1)*class_weigts))\n",
    "    acc=1.0-(err/cnt)\n",
    "    return acc\n",
    "\"\"\"\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "#    with tf.get_default_graph().gradient_override_map({\"Round\": \"Identity\"}):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def matthews_correlation_loss2(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos =  y_pred\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = y_true\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = K.square(tp * tn - fp * fn)\n",
    "    denominator = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "\n",
    "    return 50 - 100 * numerator/(denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def correlation_coefficient_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return 1 - K.square(r)\n",
    "\n",
    "\"\"\"\n",
    "def blockBuild(a_in, b_in):\n",
    "    #First input model\n",
    "    a_in = Dense(num_situ)(a_in)\n",
    "    a_in = AlphaDropout(0.2)(a_in)\n",
    "    a_in = BatchNormalization()(a_in)\n",
    "    a_in = Activation('softplus')(a_in)\n",
    "    \n",
    "    #Second input model\n",
    "    b_in = Dense(num_situ)(b_in)\n",
    "    b_in = AlphaDropout(0.2)(b_in)\n",
    "    b_in = BatchNormalization()(b_in)\n",
    "    b_in = Activation('softplus')(b_in)\n",
    "    #b = LeakyReLU()(b)\n",
    "    \n",
    "    y = concatenate([a_in, b_in], axis=0)\n",
    "    y = Dense(num_situ)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('softplus')(y)\n",
    "    return(y)\n",
    "\n",
    "a = Input(shape=(num_situ,))\n",
    "b = Input(shape=(num_situ,))\n",
    "\n",
    "first_block = blockBuild(a,b)\n",
    "second_block = blockBuild(a,first_block)\n",
    "third_block = blockBuild(second_block,b)\n",
    "\"\"\"\n",
    "\n",
    "a1 = Input(shape=(2,num_situ,))\n",
    "a2 = Conv1D (kernel_size = 2, filters = 8, activation='softplus')(a1)\n",
    "a3 = Flatten()(a2)\n",
    "a4 = Dense(2*num_situ)(a3)\n",
    "a5 = AlphaDropout(0.2)(a4)\n",
    "a6 = BatchNormalization()(a5)\n",
    "a7 = Activation('softplus')(a6)\n",
    "a8 = Dense(num_situ, activation='softplus')(a7)\n",
    "\n",
    "#Third input model\n",
    "c1 = Input(shape=(num_all,))\n",
    "c2 = Dense(num_situ)(c1)\n",
    "c3 = AlphaDropout(0.2)(c2)\n",
    "c4 = BatchNormalization()(c3)\n",
    "c5 = Activation('softplus')(c4)\n",
    "c6 = concatenate([a8,c1]) #third_block\n",
    "c7 = Dense(num_situ)(c6)\n",
    "c8 = BatchNormalization()(c7)\n",
    "c9 = Activation('softplus')(c8)\n",
    "c10 = Dense(10)(c9)\n",
    "c11 = BatchNormalization()(c10)\n",
    "c12 = Activation('softplus')(c11)\n",
    "output = Dense(1, activation='sigmoid')(c12)\n",
    "model = Model(inputs=[a1, c1], outputs=[output])\n",
    "#model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "model.compile(optimizer='adam', loss=matthews_correlation_loss2, metrics=[matthews_correlation]) # metrics=['binary_accuracy'])\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='my_res_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 18 00:04:46 2018 Model build\n",
      "00:04:47  Fit\n",
      "Train on 648 samples, validate on 649 samples\n",
      "Epoch 1/100\n",
      " - 4s - loss: 0.6872 - binary_accuracy: 0.5985 - val_loss: 0.5579 - val_binary_accuracy: 0.7116\n",
      "\n",
      "Epoch 00001: val_binary_accuracy improved from -inf to 0.71156, saving model to models/weights-improvement-01-0.71.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.5485 - binary_accuracy: 0.7162 - val_loss: 0.4959 - val_binary_accuracy: 0.7715\n",
      "\n",
      "Epoch 00002: val_binary_accuracy improved from 0.71156 to 0.77149, saving model to models/weights-improvement-02-0.77.hdf5\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.5039 - binary_accuracy: 0.7572 - val_loss: 0.4636 - val_binary_accuracy: 0.7920\n",
      "\n",
      "Epoch 00003: val_binary_accuracy improved from 0.77149 to 0.79199, saving model to models/weights-improvement-03-0.79.hdf5\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.4747 - binary_accuracy: 0.7806 - val_loss: 0.4416 - val_binary_accuracy: 0.8136\n",
      "\n",
      "Epoch 00004: val_binary_accuracy improved from 0.79199 to 0.81356, saving model to models/weights-improvement-04-0.81.hdf5\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.4505 - binary_accuracy: 0.7924 - val_loss: 0.4218 - val_binary_accuracy: 0.8236\n",
      "\n",
      "Epoch 00005: val_binary_accuracy improved from 0.81356 to 0.82357, saving model to models/weights-improvement-05-0.82.hdf5\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.4366 - binary_accuracy: 0.8040 - val_loss: 0.4086 - val_binary_accuracy: 0.8282\n",
      "\n",
      "Epoch 00006: val_binary_accuracy improved from 0.82357 to 0.82820, saving model to models/weights-improvement-06-0.83.hdf5\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.4191 - binary_accuracy: 0.8141 - val_loss: 0.3946 - val_binary_accuracy: 0.8390\n",
      "\n",
      "Epoch 00007: val_binary_accuracy improved from 0.82820 to 0.83898, saving model to models/weights-improvement-07-0.84.hdf5\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.4034 - binary_accuracy: 0.8265 - val_loss: 0.3831 - val_binary_accuracy: 0.8404\n",
      "\n",
      "Epoch 00008: val_binary_accuracy improved from 0.83898 to 0.84037, saving model to models/weights-improvement-08-0.84.hdf5\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.3838 - binary_accuracy: 0.8329 - val_loss: 0.3744 - val_binary_accuracy: 0.8429\n",
      "\n",
      "Epoch 00009: val_binary_accuracy improved from 0.84037 to 0.84291, saving model to models/weights-improvement-09-0.84.hdf5\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.3835 - binary_accuracy: 0.8311 - val_loss: 0.3670 - val_binary_accuracy: 0.8494\n",
      "\n",
      "Epoch 00010: val_binary_accuracy improved from 0.84291 to 0.84938, saving model to models/weights-improvement-10-0.85.hdf5\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.3659 - binary_accuracy: 0.8428 - val_loss: 0.3625 - val_binary_accuracy: 0.8458\n",
      "\n",
      "Epoch 00011: val_binary_accuracy did not improve from 0.84938\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.3630 - binary_accuracy: 0.8439 - val_loss: 0.3553 - val_binary_accuracy: 0.8489\n",
      "\n",
      "Epoch 00012: val_binary_accuracy did not improve from 0.84938\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.3586 - binary_accuracy: 0.8416 - val_loss: 0.3534 - val_binary_accuracy: 0.8493\n",
      "\n",
      "Epoch 00013: val_binary_accuracy did not improve from 0.84938\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.3589 - binary_accuracy: 0.8400 - val_loss: 0.3503 - val_binary_accuracy: 0.8500\n",
      "\n",
      "Epoch 00014: val_binary_accuracy improved from 0.84938 to 0.85000, saving model to models/weights-improvement-14-0.85.hdf5\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.3506 - binary_accuracy: 0.8439 - val_loss: 0.3468 - val_binary_accuracy: 0.8514\n",
      "\n",
      "Epoch 00015: val_binary_accuracy improved from 0.85000 to 0.85139, saving model to models/weights-improvement-15-0.85.hdf5\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.3408 - binary_accuracy: 0.8513 - val_loss: 0.3431 - val_binary_accuracy: 0.8536\n",
      "\n",
      "Epoch 00016: val_binary_accuracy improved from 0.85139 to 0.85362, saving model to models/weights-improvement-16-0.85.hdf5\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.3415 - binary_accuracy: 0.8497 - val_loss: 0.3423 - val_binary_accuracy: 0.8550\n",
      "\n",
      "Epoch 00017: val_binary_accuracy improved from 0.85362 to 0.85501, saving model to models/weights-improvement-17-0.86.hdf5\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.3432 - binary_accuracy: 0.8508 - val_loss: 0.3427 - val_binary_accuracy: 0.8542\n",
      "\n",
      "Epoch 00018: val_binary_accuracy did not improve from 0.85501\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.3362 - binary_accuracy: 0.8547 - val_loss: 0.3404 - val_binary_accuracy: 0.8539\n",
      "\n",
      "Epoch 00019: val_binary_accuracy did not improve from 0.85501\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.3334 - binary_accuracy: 0.8550 - val_loss: 0.3410 - val_binary_accuracy: 0.8530\n",
      "\n",
      "Epoch 00020: val_binary_accuracy did not improve from 0.85501\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.3264 - binary_accuracy: 0.8586 - val_loss: 0.3381 - val_binary_accuracy: 0.8545\n",
      "\n",
      "Epoch 00021: val_binary_accuracy did not improve from 0.85501\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.3265 - binary_accuracy: 0.8574 - val_loss: 0.3367 - val_binary_accuracy: 0.8554\n",
      "\n",
      "Epoch 00022: val_binary_accuracy improved from 0.85501 to 0.85539, saving model to models/weights-improvement-22-0.86.hdf5\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.3311 - binary_accuracy: 0.8539 - val_loss: 0.3385 - val_binary_accuracy: 0.8530\n",
      "\n",
      "Epoch 00023: val_binary_accuracy did not improve from 0.85539\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.3230 - binary_accuracy: 0.8610 - val_loss: 0.3379 - val_binary_accuracy: 0.8545\n",
      "\n",
      "Epoch 00024: val_binary_accuracy did not improve from 0.85539\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.3276 - binary_accuracy: 0.8576 - val_loss: 0.3392 - val_binary_accuracy: 0.8553\n",
      "\n",
      "Epoch 00025: val_binary_accuracy did not improve from 0.85539\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.3203 - binary_accuracy: 0.8611 - val_loss: 0.3363 - val_binary_accuracy: 0.8551\n",
      "\n",
      "Epoch 00026: val_binary_accuracy did not improve from 0.85539\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.3184 - binary_accuracy: 0.8613 - val_loss: 0.3377 - val_binary_accuracy: 0.8555\n",
      "\n",
      "Epoch 00027: val_binary_accuracy improved from 0.85539 to 0.85555, saving model to models/weights-improvement-27-0.86.hdf5\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.3178 - binary_accuracy: 0.8633 - val_loss: 0.3377 - val_binary_accuracy: 0.8550\n",
      "\n",
      "Epoch 00028: val_binary_accuracy did not improve from 0.85555\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.3197 - binary_accuracy: 0.8611 - val_loss: 0.3348 - val_binary_accuracy: 0.8565\n",
      "\n",
      "Epoch 00029: val_binary_accuracy improved from 0.85555 to 0.85655, saving model to models/weights-improvement-29-0.86.hdf5\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.3179 - binary_accuracy: 0.8625 - val_loss: 0.3360 - val_binary_accuracy: 0.8545\n",
      "\n",
      "Epoch 00030: val_binary_accuracy did not improve from 0.85655\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.3134 - binary_accuracy: 0.8667 - val_loss: 0.3373 - val_binary_accuracy: 0.8536\n",
      "\n",
      "Epoch 00031: val_binary_accuracy did not improve from 0.85655\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.3023 - binary_accuracy: 0.8666 - val_loss: 0.3368 - val_binary_accuracy: 0.8539\n",
      "\n",
      "Epoch 00032: val_binary_accuracy did not improve from 0.85655\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.3160 - binary_accuracy: 0.8604 - val_loss: 0.3373 - val_binary_accuracy: 0.8547\n",
      "\n",
      "Epoch 00033: val_binary_accuracy did not improve from 0.85655\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.3034 - binary_accuracy: 0.8677 - val_loss: 0.3363 - val_binary_accuracy: 0.8569\n",
      "\n",
      "Epoch 00034: val_binary_accuracy improved from 0.85655 to 0.85693, saving model to models/weights-improvement-34-0.86.hdf5\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.3098 - binary_accuracy: 0.8658 - val_loss: 0.3374 - val_binary_accuracy: 0.8555\n",
      "\n",
      "Epoch 00035: val_binary_accuracy did not improve from 0.85693\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.3200 - binary_accuracy: 0.8574 - val_loss: 0.3406 - val_binary_accuracy: 0.8538\n",
      "\n",
      "Epoch 00036: val_binary_accuracy did not improve from 0.85693\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.3060 - binary_accuracy: 0.8669 - val_loss: 0.3393 - val_binary_accuracy: 0.8535\n",
      "\n",
      "Epoch 00037: val_binary_accuracy did not improve from 0.85693\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.3185 - binary_accuracy: 0.8615 - val_loss: 0.3385 - val_binary_accuracy: 0.8555\n",
      "\n",
      "Epoch 00038: val_binary_accuracy did not improve from 0.85693\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.3072 - binary_accuracy: 0.8670 - val_loss: 0.3360 - val_binary_accuracy: 0.8552\n",
      "\n",
      "Epoch 00039: val_binary_accuracy did not improve from 0.85693\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.3034 - binary_accuracy: 0.8700 - val_loss: 0.3368 - val_binary_accuracy: 0.8571\n",
      "\n",
      "Epoch 00040: val_binary_accuracy improved from 0.85693 to 0.85709, saving model to models/weights-improvement-40-0.86.hdf5\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.2978 - binary_accuracy: 0.8747 - val_loss: 0.3362 - val_binary_accuracy: 0.8553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.3055 - binary_accuracy: 0.8675 - val_loss: 0.3396 - val_binary_accuracy: 0.8535\n",
      "\n",
      "Epoch 00042: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.2979 - binary_accuracy: 0.8734 - val_loss: 0.3375 - val_binary_accuracy: 0.8567\n",
      "\n",
      "Epoch 00043: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.2987 - binary_accuracy: 0.8710 - val_loss: 0.3385 - val_binary_accuracy: 0.8563\n",
      "\n",
      "Epoch 00044: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.2976 - binary_accuracy: 0.8715 - val_loss: 0.3356 - val_binary_accuracy: 0.8566\n",
      "\n",
      "Epoch 00045: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.3024 - binary_accuracy: 0.8695 - val_loss: 0.3390 - val_binary_accuracy: 0.8546\n",
      "\n",
      "Epoch 00046: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.2897 - binary_accuracy: 0.8750 - val_loss: 0.3379 - val_binary_accuracy: 0.8552\n",
      "\n",
      "Epoch 00047: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.3014 - binary_accuracy: 0.8683 - val_loss: 0.3370 - val_binary_accuracy: 0.8559\n",
      "\n",
      "Epoch 00048: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.3093 - binary_accuracy: 0.8654 - val_loss: 0.3380 - val_binary_accuracy: 0.8557\n",
      "\n",
      "Epoch 00049: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.2979 - binary_accuracy: 0.8698 - val_loss: 0.3398 - val_binary_accuracy: 0.8551\n",
      "\n",
      "Epoch 00050: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.2968 - binary_accuracy: 0.8743 - val_loss: 0.3384 - val_binary_accuracy: 0.8556\n",
      "\n",
      "Epoch 00051: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.2943 - binary_accuracy: 0.8740 - val_loss: 0.3367 - val_binary_accuracy: 0.8562\n",
      "\n",
      "Epoch 00052: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.2904 - binary_accuracy: 0.8755 - val_loss: 0.3376 - val_binary_accuracy: 0.8549\n",
      "\n",
      "Epoch 00053: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.2975 - binary_accuracy: 0.8701 - val_loss: 0.3393 - val_binary_accuracy: 0.8559\n",
      "\n",
      "Epoch 00054: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.2877 - binary_accuracy: 0.8752 - val_loss: 0.3414 - val_binary_accuracy: 0.8545\n",
      "\n",
      "Epoch 00055: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.2970 - binary_accuracy: 0.8702 - val_loss: 0.3408 - val_binary_accuracy: 0.8540\n",
      "\n",
      "Epoch 00056: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.2962 - binary_accuracy: 0.8702 - val_loss: 0.3423 - val_binary_accuracy: 0.8545\n",
      "\n",
      "Epoch 00057: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.2909 - binary_accuracy: 0.8738 - val_loss: 0.3423 - val_binary_accuracy: 0.8526\n",
      "\n",
      "Epoch 00058: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.2803 - binary_accuracy: 0.8805 - val_loss: 0.3432 - val_binary_accuracy: 0.8542\n",
      "\n",
      "Epoch 00059: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.2927 - binary_accuracy: 0.8724 - val_loss: 0.3407 - val_binary_accuracy: 0.8535\n",
      "\n",
      "Epoch 00060: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.2777 - binary_accuracy: 0.8811 - val_loss: 0.3413 - val_binary_accuracy: 0.8535\n",
      "\n",
      "Epoch 00061: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.2799 - binary_accuracy: 0.8817 - val_loss: 0.3417 - val_binary_accuracy: 0.8546\n",
      "\n",
      "Epoch 00062: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.2814 - binary_accuracy: 0.8791 - val_loss: 0.3424 - val_binary_accuracy: 0.8542\n",
      "\n",
      "Epoch 00063: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.2918 - binary_accuracy: 0.8742 - val_loss: 0.3449 - val_binary_accuracy: 0.8518\n",
      "\n",
      "Epoch 00064: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.2775 - binary_accuracy: 0.8790 - val_loss: 0.3415 - val_binary_accuracy: 0.8551\n",
      "\n",
      "Epoch 00065: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.2901 - binary_accuracy: 0.8731 - val_loss: 0.3421 - val_binary_accuracy: 0.8545\n",
      "\n",
      "Epoch 00066: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.2886 - binary_accuracy: 0.8742 - val_loss: 0.3444 - val_binary_accuracy: 0.8526\n",
      "\n",
      "Epoch 00067: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.2882 - binary_accuracy: 0.8727 - val_loss: 0.3429 - val_binary_accuracy: 0.8545\n",
      "\n",
      "Epoch 00068: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.2889 - binary_accuracy: 0.8773 - val_loss: 0.3446 - val_binary_accuracy: 0.8546\n",
      "\n",
      "Epoch 00069: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.2861 - binary_accuracy: 0.8750 - val_loss: 0.3425 - val_binary_accuracy: 0.8530\n",
      "\n",
      "Epoch 00070: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.2892 - binary_accuracy: 0.8726 - val_loss: 0.3450 - val_binary_accuracy: 0.8538\n",
      "\n",
      "Epoch 00071: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.2745 - binary_accuracy: 0.8826 - val_loss: 0.3446 - val_binary_accuracy: 0.8537\n",
      "\n",
      "Epoch 00072: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.2902 - binary_accuracy: 0.8748 - val_loss: 0.3457 - val_binary_accuracy: 0.8527\n",
      "\n",
      "Epoch 00073: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.2863 - binary_accuracy: 0.8769 - val_loss: 0.3416 - val_binary_accuracy: 0.8546\n",
      "\n",
      "Epoch 00074: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.2740 - binary_accuracy: 0.8816 - val_loss: 0.3452 - val_binary_accuracy: 0.8528\n",
      "\n",
      "Epoch 00075: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.2836 - binary_accuracy: 0.8757 - val_loss: 0.3454 - val_binary_accuracy: 0.8529\n",
      "\n",
      "Epoch 00076: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.2829 - binary_accuracy: 0.8782 - val_loss: 0.3420 - val_binary_accuracy: 0.8545\n",
      "\n",
      "Epoch 00077: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.2807 - binary_accuracy: 0.8792 - val_loss: 0.3422 - val_binary_accuracy: 0.8529\n",
      "\n",
      "Epoch 00078: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.2810 - binary_accuracy: 0.8769 - val_loss: 0.3445 - val_binary_accuracy: 0.8537\n",
      "\n",
      "Epoch 00079: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.2817 - binary_accuracy: 0.8768 - val_loss: 0.3435 - val_binary_accuracy: 0.8551\n",
      "\n",
      "Epoch 00080: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.2818 - binary_accuracy: 0.8809 - val_loss: 0.3452 - val_binary_accuracy: 0.8542\n",
      "\n",
      "Epoch 00081: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.2810 - binary_accuracy: 0.8797 - val_loss: 0.3470 - val_binary_accuracy: 0.8546\n",
      "\n",
      "Epoch 00082: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.2766 - binary_accuracy: 0.8819 - val_loss: 0.3448 - val_binary_accuracy: 0.8544\n",
      "\n",
      "Epoch 00083: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.2702 - binary_accuracy: 0.8851 - val_loss: 0.3478 - val_binary_accuracy: 0.8518\n",
      "\n",
      "Epoch 00084: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.2722 - binary_accuracy: 0.8835 - val_loss: 0.3471 - val_binary_accuracy: 0.8526\n",
      "\n",
      "Epoch 00085: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.2789 - binary_accuracy: 0.8796 - val_loss: 0.3464 - val_binary_accuracy: 0.8534\n",
      "\n",
      "Epoch 00086: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.2719 - binary_accuracy: 0.8870 - val_loss: 0.3446 - val_binary_accuracy: 0.8553\n",
      "\n",
      "Epoch 00087: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.2849 - binary_accuracy: 0.8770 - val_loss: 0.3479 - val_binary_accuracy: 0.8549\n",
      "\n",
      "Epoch 00088: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.2799 - binary_accuracy: 0.8789 - val_loss: 0.3479 - val_binary_accuracy: 0.8535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00089: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.2701 - binary_accuracy: 0.8833 - val_loss: 0.3495 - val_binary_accuracy: 0.8539\n",
      "\n",
      "Epoch 00090: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.2668 - binary_accuracy: 0.8871 - val_loss: 0.3484 - val_binary_accuracy: 0.8514\n",
      "\n",
      "Epoch 00091: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.2692 - binary_accuracy: 0.8835 - val_loss: 0.3504 - val_binary_accuracy: 0.8518\n",
      "\n",
      "Epoch 00092: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.2716 - binary_accuracy: 0.8811 - val_loss: 0.3531 - val_binary_accuracy: 0.8526\n",
      "\n",
      "Epoch 00093: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.2681 - binary_accuracy: 0.8850 - val_loss: 0.3492 - val_binary_accuracy: 0.8528\n",
      "\n",
      "Epoch 00094: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.2633 - binary_accuracy: 0.8864 - val_loss: 0.3492 - val_binary_accuracy: 0.8534\n",
      "\n",
      "Epoch 00095: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.2716 - binary_accuracy: 0.8856 - val_loss: 0.3491 - val_binary_accuracy: 0.8554\n",
      "\n",
      "Epoch 00096: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.2735 - binary_accuracy: 0.8802 - val_loss: 0.3502 - val_binary_accuracy: 0.8529\n",
      "\n",
      "Epoch 00097: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.2681 - binary_accuracy: 0.8860 - val_loss: 0.3514 - val_binary_accuracy: 0.8505\n",
      "\n",
      "Epoch 00098: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.2662 - binary_accuracy: 0.8880 - val_loss: 0.3494 - val_binary_accuracy: 0.8526\n",
      "\n",
      "Epoch 00099: val_binary_accuracy did not improve from 0.85709\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.2687 - binary_accuracy: 0.8842 - val_loss: 0.3501 - val_binary_accuracy: 0.8532\n",
      "\n",
      "Epoch 00100: val_binary_accuracy did not improve from 0.85709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2894b2df908>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model for dge to bdtnp.\n",
    "print(time.ctime(), 'Model build')\n",
    "\n",
    "a1 = Input(shape=(84,))\n",
    "a2 = Dense(84)(a1)\n",
    "a3 = BatchNormalization()(a2)\n",
    "#a4 = Dropout(0.2)(a3)\n",
    "\n",
    "#c1 = Input(shape=(num_all,))\n",
    "#c3 = Dense(84)(c1)\n",
    "#c4 = BatchNormalization()(c3)\n",
    "#c5 = AlphaDropout(0.5)(c4)\n",
    "\n",
    "#e = concatenate([a4, c5])\n",
    "e = Dense(40)(a3)\n",
    "e = BatchNormalization()(e)\n",
    "e = Activation('softplus')(e)\n",
    "e = Dropout(0.1)(e)\n",
    "\n",
    "output = Dense(num_situ, activation='sigmoid')(e)\n",
    "model = Model(inputs=[a1], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "#print(model.summary())\n",
    "print(time.strftime(\"%H:%M:%S\"), ' Fit')\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"models/weights-improvement-{epoch:02d}-{val_binary_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_binary_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "#tbCallBack = keras.callbacks.TensorBoard(log_dir='.', histogram_freq=0, write_graph=True, write_images=True)\n",
    "#history = \n",
    "model.fit(  x=[Z_], y=y_,\n",
    "            batch_size=10,\n",
    "            epochs=100,\n",
    "            verbose=2,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks_list)\n",
    "            #class_weight={0:0.55, 1:5.22}) #, use_multiprocessing=True, workers=8) #, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55288311, 5.22740697])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "plt.title('Model accuracy')\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "print(np.average(history.history['val_acc']))\n",
    "print(np.max(history.history['val_acc']))\n",
    "#dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
    "#0.6216538506631668\n",
    "#0.7161538486297314\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try leave one out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%H:%M:%S\"), ' Fit')\n",
    "\n",
    "#Try differnet training ommitting one gene at a time.\n",
    "val_acc=np.empty((num_situ,2))\n",
    "for i in range(num_situ):\n",
    "    model.load_weights('model.h5')\n",
    "    X2_temp = np.delete(X2_train, i, axis=1)\n",
    "    #tbCallBack = keras.callbacks.TensorBoard(log_dir='.', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    history = model.fit(x=[X1_train, X2_temp, X3_train],\n",
    "                        y=y_train,\n",
    "                        batch_size=50,\n",
    "                        epochs=20,\n",
    "                        verbose=0,\n",
    "                        validation_split=0.3,\n",
    "                        class_weight={0:1, 1:10}) #, use_multiprocessing=True, workers=8) #, callbacks=[tbCallBack])\n",
    "    val_acc[i,0] = np.average(history.history['val_acc'])\n",
    "    val_acc[i,1] = np.max(history.history['val_acc'])\n",
    "    print(time.ctime(), f'i: {i}, val_acc average: {val_acc[i,0]}, max: {val_acc[i,1]}')\n",
    "\n",
    "np.save('val_acc.npy', val_acc)\n",
    "\n",
    "#Compare with results in connection weight genes (using all zeros but one).\n",
    "from scipy import stats\n",
    "\n",
    "loo = pd.read_csv('logs/2/loo.csv')\n",
    "cw = pd.read_csv('logs/2/cw.csv')\n",
    "tau, p_value = stats.kendalltau(loo['x'], cw['x'])\n",
    "print(f'tau - {tau}, p-value - {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 13 22:23:28 2018 Load model\n",
      "0  1  2  3  4  5  6  7  8  9  10  11  Tue Nov 13 22:25:07 2018 Done\n"
     ]
    }
   ],
   "source": [
    "#Use batch prediction=50\n",
    "import heapq\n",
    "\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "\n",
    "def matthews_correlation_loss(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos =  y_pred\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = y_true\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = K.square(tp * tn - fp * fn)\n",
    "    denominator = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "\n",
    "    return 50 - 100 * numerator/(denominator + K.epsilon())\n",
    "\n",
    "\n",
    "print(time.ctime(), 'Load model')\n",
    "model = keras.models.load_model('weights-improvement-1284-0.41.hdf5', custom_objects={\"matthews_correlation\": matthews_correlation, \"matthews_correlation_loss\":matthews_correlation_loss}) #weights-improvement-2535-0.88.hdf5\n",
    "#model = load_model('c:/data/Dream/10-60_genes/model_sav_60.h5')\n",
    "#model = load_model('c:/data/Dream/9-40_genes/model_sav_40.h5')\n",
    "\n",
    "#Loop on all 3039 cells and provide 10 highest probable locations (search for the line/s in BDTNP providing the highest probability)\n",
    "result = pd.DataFrame()\n",
    "for index, row_d in d.iterrows():\n",
    "    print(index, ' ', end=\"\")\n",
    "    proba = []\n",
    "    for index2, row_b in bdtnp.iterrows():\n",
    "        X = np.vstack([row_b,row_d[cols]])\n",
    "        pred = model.predict([X[np.newaxis,:], row_d[np.newaxis,:]], batch_size=1)\n",
    "        proba.append(pred)\n",
    "\n",
    "    #list2 = [i[0][0] for i in heapq.nlargest(10, proba)]\n",
    "    #x = pd.Series(list2, index=['val1', 'val2', 'val3', 'val4', 'val5', 'val6', 'val7', 'val8', 'val9', 'val10'])\n",
    "    result = pd.concat([result,pd.DataFrame([sorted(range(len(proba)), key=lambda i: proba[i])[-10:]], columns=['i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7', 'i8', 'i9', 'i10'])])\n",
    "    #result = result.append(x, ignore_index=True)\n",
    "    if index > 10:\n",
    "        break\n",
    "\n",
    "#c = pd.DataFrame()\n",
    "#Loop on all cells in dge.csv and provide 10 highest probable locations (search for the line/s in BDTNP providing the higher probability).\n",
    "#for index, row_d in d.iterrows():\n",
    "#    print(index, ' ', end=\"\")\n",
    "    #Multiply row_d by 3039 (as a fixed row in inputs2 and 3).    \n",
    "#    row_expanded = pd.concat([row_d]*len(bdtnp), ignore_index=True, axis=1).T\n",
    "    #b_expanded = pd.concat([b, row_expanded], axis=1)\n",
    "    #pred = model.predict([b_expanded.iloc[glist_60_tom], b_expanded.iloc[:,num_situ:2*num_situ], b_expanded.iloc[:,2*num_situ:]], batch_size=50, verbose=0)\n",
    "    #pred = model.predict([bdtnp[glist_60_tom], row_expanded[glist_60_tom], row_expanded[list(set(d.columns) - set(glist_60_tom))]], batch_size=50, verbose=0)\n",
    "#    pred = model.predict([bdtnp[cols], row_expanded[cols], row_expanded], batch_size=50, verbose=0)\n",
    "#    c = pd.concat([c,pd.DataFrame([sorted(range(len(pred)), key=lambda i: pred[i])[-10:]], columns=['i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7', 'i8', 'i9', 'i10'])])\n",
    "#    if index > 10:\n",
    "#        break\n",
    "\n",
    "result.to_csv('ann_84.csv')\n",
    "print(time.ctime(), 'Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1297/1297 [==============================] - 0s 70us/step\n"
     ]
    }
   ],
   "source": [
    "# Using the dge to bdtnp model.\n",
    "\n",
    "model = keras.models.load_model('weights-improvement-12347-0.88.hdf5')\n",
    "c = pd.DataFrame()\n",
    "pred = model.predict([d1_bin], batch_size=50, verbose=1)\n",
    "results = pd.DataFrame(data=np.round(pred), index=[i for i in range(0,len(pred))], columns=glist_20)\n",
    "results.to_csv('ann_pred.csv', index=False)\n",
    "\n",
    "#Test results\n",
    "#real_count=0\n",
    "#for i, row in labels.iterrows():\n",
    "#    if (closest[i] == row[0]):\n",
    "#        real_count += 1\n",
    "#\n",
    "#print(real_count)\n",
    "#results.iloc[0:3]\n",
    "#meds = temp_results.median()\n",
    "#for col in temp_results:\n",
    "#    temp_results[col] = temp_results[col].apply(lambda x: 0 if x<= (meds[col] + i) else 1)\n",
    "#    locations[index] = pairwise_distances_argmin(temp_results, bdtnp_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danr</th>\n",
       "      <th>CG14427</th>\n",
       "      <th>dan</th>\n",
       "      <th>CG43394</th>\n",
       "      <th>ImpL2</th>\n",
       "      <th>Nek2</th>\n",
       "      <th>CG8147</th>\n",
       "      <th>Ama</th>\n",
       "      <th>Btk29A</th>\n",
       "      <th>trn</th>\n",
       "      <th>numb</th>\n",
       "      <th>prd</th>\n",
       "      <th>brk</th>\n",
       "      <th>tsh</th>\n",
       "      <th>pxb</th>\n",
       "      <th>dpn</th>\n",
       "      <th>ftz</th>\n",
       "      <th>Kr</th>\n",
       "      <th>h</th>\n",
       "      <th>eve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.952570</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>0.999427</td>\n",
       "      <td>9.988462e-01</td>\n",
       "      <td>2.225487e-06</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>8.976961e-01</td>\n",
       "      <td>0.910867</td>\n",
       "      <td>9.992010e-01</td>\n",
       "      <td>4.948604e-06</td>\n",
       "      <td>9.973761e-01</td>\n",
       "      <td>1.085652e-04</td>\n",
       "      <td>4.932669e-04</td>\n",
       "      <td>1.096998e-04</td>\n",
       "      <td>3.081364e-06</td>\n",
       "      <td>0.692830</td>\n",
       "      <td>9.230255e-01</td>\n",
       "      <td>1.595170e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.073005</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.260960</td>\n",
       "      <td>0.999972</td>\n",
       "      <td>9.924010e-01</td>\n",
       "      <td>4.919980e-04</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>4.686632e-02</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>3.026228e-01</td>\n",
       "      <td>5.767848e-01</td>\n",
       "      <td>9.956729e-01</td>\n",
       "      <td>3.016724e-02</td>\n",
       "      <td>6.589219e-04</td>\n",
       "      <td>9.117642e-03</td>\n",
       "      <td>9.999350e-01</td>\n",
       "      <td>0.033744</td>\n",
       "      <td>1.860791e-03</td>\n",
       "      <td>2.594135e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.128240</td>\n",
       "      <td>0.998572</td>\n",
       "      <td>2.026050e-05</td>\n",
       "      <td>7.032250e-03</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>5.656553e-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.185221e-05</td>\n",
       "      <td>9.262726e-01</td>\n",
       "      <td>1.872751e-04</td>\n",
       "      <td>9.973971e-01</td>\n",
       "      <td>9.825932e-01</td>\n",
       "      <td>5.684511e-02</td>\n",
       "      <td>9.997877e-01</td>\n",
       "      <td>0.999295</td>\n",
       "      <td>1.272992e-02</td>\n",
       "      <td>1.028869e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.999962e-01</td>\n",
       "      <td>0.788806</td>\n",
       "      <td>9.998572e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>9.279311e-01</td>\n",
       "      <td>6.623404e-07</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>9.910707e-01</td>\n",
       "      <td>0.174682</td>\n",
       "      <td>9.992439e-01</td>\n",
       "      <td>1.558326e-05</td>\n",
       "      <td>9.878981e-01</td>\n",
       "      <td>5.819520e-07</td>\n",
       "      <td>5.616950e-01</td>\n",
       "      <td>2.520103e-03</td>\n",
       "      <td>8.022946e-08</td>\n",
       "      <td>0.036852</td>\n",
       "      <td>9.923142e-01</td>\n",
       "      <td>4.249081e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999593</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>1.097144e-07</td>\n",
       "      <td>1.467953e-01</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>1.737630e-09</td>\n",
       "      <td>0.986156</td>\n",
       "      <td>4.429802e-06</td>\n",
       "      <td>9.999733e-01</td>\n",
       "      <td>1.644533e-03</td>\n",
       "      <td>9.999981e-01</td>\n",
       "      <td>9.961914e-01</td>\n",
       "      <td>9.938365e-01</td>\n",
       "      <td>9.665008e-01</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>9.889327e-01</td>\n",
       "      <td>7.089722e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.857821e-03</td>\n",
       "      <td>0.398339</td>\n",
       "      <td>6.580142e-03</td>\n",
       "      <td>0.995009</td>\n",
       "      <td>0.094408</td>\n",
       "      <td>9.904410e-01</td>\n",
       "      <td>9.999146e-01</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>7.737213e-01</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.550446e-04</td>\n",
       "      <td>8.348986e-03</td>\n",
       "      <td>1.097112e-04</td>\n",
       "      <td>6.478760e-03</td>\n",
       "      <td>4.778898e-04</td>\n",
       "      <td>8.386919e-01</td>\n",
       "      <td>9.999241e-01</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>3.880746e-03</td>\n",
       "      <td>9.181547e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.970232e-01</td>\n",
       "      <td>0.999684</td>\n",
       "      <td>9.999852e-01</td>\n",
       "      <td>0.993155</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>6.502574e-01</td>\n",
       "      <td>1.507457e-03</td>\n",
       "      <td>0.459570</td>\n",
       "      <td>2.764302e-01</td>\n",
       "      <td>0.912435</td>\n",
       "      <td>6.476845e-01</td>\n",
       "      <td>8.231743e-04</td>\n",
       "      <td>6.520438e-01</td>\n",
       "      <td>1.139168e-02</td>\n",
       "      <td>2.286469e-03</td>\n",
       "      <td>1.270115e-05</td>\n",
       "      <td>1.181825e-03</td>\n",
       "      <td>0.920938</td>\n",
       "      <td>2.793467e-04</td>\n",
       "      <td>2.107628e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.972035e-04</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>3.349570e-05</td>\n",
       "      <td>0.999668</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>8.212480e-06</td>\n",
       "      <td>1.815548e-04</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>9.984673e-01</td>\n",
       "      <td>0.999898</td>\n",
       "      <td>2.098095e-04</td>\n",
       "      <td>5.091575e-05</td>\n",
       "      <td>2.507595e-06</td>\n",
       "      <td>1.720824e-05</td>\n",
       "      <td>9.998615e-01</td>\n",
       "      <td>2.240034e-04</td>\n",
       "      <td>9.414664e-06</td>\n",
       "      <td>0.989408</td>\n",
       "      <td>1.642943e-04</td>\n",
       "      <td>5.226656e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.495757e-03</td>\n",
       "      <td>0.182013</td>\n",
       "      <td>4.974751e-02</td>\n",
       "      <td>0.004361</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>9.591834e-03</td>\n",
       "      <td>9.999360e-01</td>\n",
       "      <td>0.999893</td>\n",
       "      <td>9.998872e-01</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>9.999392e-01</td>\n",
       "      <td>7.474542e-01</td>\n",
       "      <td>3.679442e-06</td>\n",
       "      <td>9.718327e-01</td>\n",
       "      <td>1.333877e-01</td>\n",
       "      <td>4.582791e-01</td>\n",
       "      <td>3.363704e-02</td>\n",
       "      <td>0.946534</td>\n",
       "      <td>9.827011e-01</td>\n",
       "      <td>9.960330e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.811024e-03</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>2.724231e-04</td>\n",
       "      <td>0.999914</td>\n",
       "      <td>0.999469</td>\n",
       "      <td>4.158520e-04</td>\n",
       "      <td>1.917854e-04</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>9.913943e-01</td>\n",
       "      <td>0.999727</td>\n",
       "      <td>3.918419e-04</td>\n",
       "      <td>1.168152e-04</td>\n",
       "      <td>1.891582e-04</td>\n",
       "      <td>4.295379e-06</td>\n",
       "      <td>9.955487e-01</td>\n",
       "      <td>1.478557e-05</td>\n",
       "      <td>1.859974e-04</td>\n",
       "      <td>0.633407</td>\n",
       "      <td>9.499377e-06</td>\n",
       "      <td>4.998521e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9.999993e-01</td>\n",
       "      <td>0.997766</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.668968</td>\n",
       "      <td>0.999614</td>\n",
       "      <td>2.553989e-02</td>\n",
       "      <td>9.212085e-03</td>\n",
       "      <td>0.998669</td>\n",
       "      <td>9.818991e-05</td>\n",
       "      <td>0.957203</td>\n",
       "      <td>1.136290e-03</td>\n",
       "      <td>8.066000e-01</td>\n",
       "      <td>6.916312e-03</td>\n",
       "      <td>9.018955e-01</td>\n",
       "      <td>9.928236e-01</td>\n",
       "      <td>9.150981e-01</td>\n",
       "      <td>2.341511e-01</td>\n",
       "      <td>0.963215</td>\n",
       "      <td>7.768021e-01</td>\n",
       "      <td>5.764536e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.143619</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>3.550558e-04</td>\n",
       "      <td>2.620373e-03</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>4.401929e-03</td>\n",
       "      <td>0.685028</td>\n",
       "      <td>9.751267e-01</td>\n",
       "      <td>9.994123e-01</td>\n",
       "      <td>9.996619e-01</td>\n",
       "      <td>9.993830e-01</td>\n",
       "      <td>9.965026e-01</td>\n",
       "      <td>9.987382e-01</td>\n",
       "      <td>1.185822e-01</td>\n",
       "      <td>0.995253</td>\n",
       "      <td>9.969323e-01</td>\n",
       "      <td>9.813778e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.999760e-01</td>\n",
       "      <td>0.994280</td>\n",
       "      <td>9.999995e-01</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>0.996073</td>\n",
       "      <td>6.270727e-04</td>\n",
       "      <td>9.107602e-01</td>\n",
       "      <td>0.999934</td>\n",
       "      <td>9.996758e-01</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>9.931347e-01</td>\n",
       "      <td>9.981224e-01</td>\n",
       "      <td>1.047253e-05</td>\n",
       "      <td>9.999948e-01</td>\n",
       "      <td>9.814327e-01</td>\n",
       "      <td>9.147418e-01</td>\n",
       "      <td>9.998261e-01</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>8.847806e-01</td>\n",
       "      <td>3.586725e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.865134e-03</td>\n",
       "      <td>0.638284</td>\n",
       "      <td>1.317361e-04</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>0.017298</td>\n",
       "      <td>9.998714e-01</td>\n",
       "      <td>9.981104e-01</td>\n",
       "      <td>0.038774</td>\n",
       "      <td>7.815810e-02</td>\n",
       "      <td>0.244115</td>\n",
       "      <td>2.733906e-03</td>\n",
       "      <td>1.631580e-02</td>\n",
       "      <td>9.454259e-01</td>\n",
       "      <td>1.690295e-03</td>\n",
       "      <td>2.211262e-02</td>\n",
       "      <td>9.957338e-01</td>\n",
       "      <td>3.480875e-03</td>\n",
       "      <td>0.027029</td>\n",
       "      <td>2.698334e-02</td>\n",
       "      <td>1.070119e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.615304e-03</td>\n",
       "      <td>0.999890</td>\n",
       "      <td>9.990397e-01</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>5.893366e-02</td>\n",
       "      <td>1.338275e-03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.659755e-01</td>\n",
       "      <td>0.243339</td>\n",
       "      <td>1.676398e-03</td>\n",
       "      <td>3.930203e-06</td>\n",
       "      <td>2.040161e-06</td>\n",
       "      <td>3.323668e-07</td>\n",
       "      <td>6.603202e-03</td>\n",
       "      <td>2.577902e-05</td>\n",
       "      <td>7.779146e-06</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>2.031399e-03</td>\n",
       "      <td>1.548131e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.393367e-07</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>2.693647e-08</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>9.999983e-01</td>\n",
       "      <td>9.999737e-01</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>4.211594e-01</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>4.940522e-06</td>\n",
       "      <td>6.464802e-09</td>\n",
       "      <td>2.028336e-01</td>\n",
       "      <td>7.572076e-06</td>\n",
       "      <td>5.822034e-07</td>\n",
       "      <td>3.205048e-04</td>\n",
       "      <td>3.045832e-06</td>\n",
       "      <td>0.999865</td>\n",
       "      <td>1.738679e-07</td>\n",
       "      <td>9.803601e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.036880</td>\n",
       "      <td>0.999848</td>\n",
       "      <td>1.459149e-06</td>\n",
       "      <td>8.769456e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.367542e-08</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>7.736443e-08</td>\n",
       "      <td>9.425172e-04</td>\n",
       "      <td>8.211447e-09</td>\n",
       "      <td>9.999952e-01</td>\n",
       "      <td>9.757390e-04</td>\n",
       "      <td>8.728823e-06</td>\n",
       "      <td>9.196917e-04</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>5.957111e-03</td>\n",
       "      <td>9.998777e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.812086e-01</td>\n",
       "      <td>0.564072</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.116492</td>\n",
       "      <td>1.887775e-06</td>\n",
       "      <td>9.952095e-01</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>9.984468e-01</td>\n",
       "      <td>0.999681</td>\n",
       "      <td>9.998662e-01</td>\n",
       "      <td>9.999949e-01</td>\n",
       "      <td>4.112371e-05</td>\n",
       "      <td>9.999994e-01</td>\n",
       "      <td>9.923676e-01</td>\n",
       "      <td>9.695355e-01</td>\n",
       "      <td>9.997806e-01</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>5.820566e-01</td>\n",
       "      <td>1.634441e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.996334</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.504082</td>\n",
       "      <td>0.992520</td>\n",
       "      <td>2.282194e-01</td>\n",
       "      <td>9.886728e-01</td>\n",
       "      <td>0.994003</td>\n",
       "      <td>9.966336e-01</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>9.993253e-01</td>\n",
       "      <td>9.994853e-01</td>\n",
       "      <td>1.097422e-02</td>\n",
       "      <td>9.998964e-01</td>\n",
       "      <td>4.127295e-04</td>\n",
       "      <td>6.243932e-04</td>\n",
       "      <td>9.998981e-01</td>\n",
       "      <td>0.995208</td>\n",
       "      <td>6.144069e-01</td>\n",
       "      <td>2.557264e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.995290e-01</td>\n",
       "      <td>0.133104</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.996271</td>\n",
       "      <td>0.051228</td>\n",
       "      <td>9.970793e-01</td>\n",
       "      <td>2.451493e-03</td>\n",
       "      <td>0.989910</td>\n",
       "      <td>9.763867e-01</td>\n",
       "      <td>0.046796</td>\n",
       "      <td>9.990502e-01</td>\n",
       "      <td>6.390570e-03</td>\n",
       "      <td>6.596552e-03</td>\n",
       "      <td>2.045522e-06</td>\n",
       "      <td>9.012696e-01</td>\n",
       "      <td>4.779747e-01</td>\n",
       "      <td>4.163624e-07</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>9.997198e-01</td>\n",
       "      <td>8.621904e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.809125</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.982447</td>\n",
       "      <td>7.849649e-05</td>\n",
       "      <td>8.919598e-01</td>\n",
       "      <td>0.999904</td>\n",
       "      <td>9.998344e-01</td>\n",
       "      <td>0.991347</td>\n",
       "      <td>9.999064e-01</td>\n",
       "      <td>9.996842e-01</td>\n",
       "      <td>1.517768e-04</td>\n",
       "      <td>9.999899e-01</td>\n",
       "      <td>7.483673e-04</td>\n",
       "      <td>2.139357e-04</td>\n",
       "      <td>9.998947e-01</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>1.370329e-03</td>\n",
       "      <td>2.224371e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999863</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.751584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.413376e-05</td>\n",
       "      <td>5.347388e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.750275e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600709e-07</td>\n",
       "      <td>9.986628e-01</td>\n",
       "      <td>2.014484e-06</td>\n",
       "      <td>9.999235e-01</td>\n",
       "      <td>1.959178e-01</td>\n",
       "      <td>3.780622e-03</td>\n",
       "      <td>9.999964e-01</td>\n",
       "      <td>0.999053</td>\n",
       "      <td>3.804333e-02</td>\n",
       "      <td>2.570437e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.997407e-01</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>0.841562</td>\n",
       "      <td>0.978401</td>\n",
       "      <td>3.264277e-06</td>\n",
       "      <td>9.080054e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.722379e-08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.774804e-08</td>\n",
       "      <td>9.993616e-01</td>\n",
       "      <td>1.896762e-08</td>\n",
       "      <td>9.978809e-01</td>\n",
       "      <td>9.166709e-01</td>\n",
       "      <td>9.140667e-01</td>\n",
       "      <td>9.999651e-01</td>\n",
       "      <td>0.816317</td>\n",
       "      <td>9.456939e-01</td>\n",
       "      <td>6.941300e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.996756e-01</td>\n",
       "      <td>0.961402</td>\n",
       "      <td>9.999933e-01</td>\n",
       "      <td>0.957463</td>\n",
       "      <td>0.997574</td>\n",
       "      <td>8.018143e-01</td>\n",
       "      <td>8.378164e-01</td>\n",
       "      <td>0.903518</td>\n",
       "      <td>1.236204e-03</td>\n",
       "      <td>0.007377</td>\n",
       "      <td>5.552902e-03</td>\n",
       "      <td>1.163220e-01</td>\n",
       "      <td>8.741810e-01</td>\n",
       "      <td>8.932080e-01</td>\n",
       "      <td>1.407525e-02</td>\n",
       "      <td>3.657069e-02</td>\n",
       "      <td>2.596068e-01</td>\n",
       "      <td>0.191250</td>\n",
       "      <td>1.999731e-01</td>\n",
       "      <td>9.880601e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.357590e-04</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>1.309942e-05</td>\n",
       "      <td>0.999868</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>5.366280e-04</td>\n",
       "      <td>1.939070e-05</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>9.998235e-01</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>6.849265e-04</td>\n",
       "      <td>3.447213e-06</td>\n",
       "      <td>6.536806e-07</td>\n",
       "      <td>1.636066e-06</td>\n",
       "      <td>9.993469e-01</td>\n",
       "      <td>1.056744e-06</td>\n",
       "      <td>3.958464e-06</td>\n",
       "      <td>0.997777</td>\n",
       "      <td>9.571102e-07</td>\n",
       "      <td>9.843995e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.966834e-01</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>6.423675e-01</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.999382</td>\n",
       "      <td>3.252073e-01</td>\n",
       "      <td>3.819732e-06</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>9.953775e-01</td>\n",
       "      <td>0.963407</td>\n",
       "      <td>9.395645e-01</td>\n",
       "      <td>5.354946e-05</td>\n",
       "      <td>5.687503e-01</td>\n",
       "      <td>5.921737e-06</td>\n",
       "      <td>9.194887e-01</td>\n",
       "      <td>1.795016e-04</td>\n",
       "      <td>6.412070e-06</td>\n",
       "      <td>0.818450</td>\n",
       "      <td>1.410119e-02</td>\n",
       "      <td>2.079197e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.976103e-01</td>\n",
       "      <td>4.243368e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>9.880773e-01</td>\n",
       "      <td>0.999090</td>\n",
       "      <td>9.996192e-01</td>\n",
       "      <td>1.085425e-06</td>\n",
       "      <td>9.997811e-01</td>\n",
       "      <td>9.024175e-05</td>\n",
       "      <td>3.737268e-05</td>\n",
       "      <td>5.879258e-08</td>\n",
       "      <td>1.925434e-06</td>\n",
       "      <td>0.999434</td>\n",
       "      <td>2.634638e-04</td>\n",
       "      <td>7.155194e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9.999926e-01</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.521378</td>\n",
       "      <td>0.997482</td>\n",
       "      <td>1.212213e-05</td>\n",
       "      <td>1.623061e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.982784e-07</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>1.571913e-06</td>\n",
       "      <td>9.989116e-01</td>\n",
       "      <td>1.794630e-06</td>\n",
       "      <td>9.994752e-01</td>\n",
       "      <td>9.997467e-01</td>\n",
       "      <td>9.932792e-01</td>\n",
       "      <td>8.784008e-01</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>9.942315e-01</td>\n",
       "      <td>1.421364e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.290200e-07</td>\n",
       "      <td>0.074248</td>\n",
       "      <td>1.633305e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.999992e-01</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>2.049086e-01</td>\n",
       "      <td>0.995819</td>\n",
       "      <td>9.575192e-07</td>\n",
       "      <td>6.270267e-07</td>\n",
       "      <td>9.988130e-01</td>\n",
       "      <td>2.106784e-08</td>\n",
       "      <td>3.456530e-06</td>\n",
       "      <td>9.997488e-01</td>\n",
       "      <td>2.526374e-01</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>6.032673e-06</td>\n",
       "      <td>1.446585e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9.984952e-01</td>\n",
       "      <td>0.996364</td>\n",
       "      <td>9.999988e-01</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.998054</td>\n",
       "      <td>5.185466e-05</td>\n",
       "      <td>9.917101e-01</td>\n",
       "      <td>0.999947</td>\n",
       "      <td>9.992722e-01</td>\n",
       "      <td>0.994933</td>\n",
       "      <td>9.989964e-01</td>\n",
       "      <td>9.999472e-01</td>\n",
       "      <td>3.654560e-05</td>\n",
       "      <td>9.999973e-01</td>\n",
       "      <td>9.992108e-01</td>\n",
       "      <td>9.995689e-01</td>\n",
       "      <td>9.569515e-01</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>9.983876e-01</td>\n",
       "      <td>4.413893e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>9.998910e-01</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.805672</td>\n",
       "      <td>0.482736</td>\n",
       "      <td>3.877808e-06</td>\n",
       "      <td>9.159855e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.310827e-09</td>\n",
       "      <td>0.996079</td>\n",
       "      <td>2.904256e-08</td>\n",
       "      <td>9.909118e-01</td>\n",
       "      <td>2.973403e-10</td>\n",
       "      <td>9.461822e-01</td>\n",
       "      <td>9.003154e-01</td>\n",
       "      <td>5.748441e-01</td>\n",
       "      <td>8.968162e-01</td>\n",
       "      <td>0.106455</td>\n",
       "      <td>9.168677e-01</td>\n",
       "      <td>3.899791e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>9.996731e-01</td>\n",
       "      <td>0.859843</td>\n",
       "      <td>9.997267e-01</td>\n",
       "      <td>0.883014</td>\n",
       "      <td>0.886068</td>\n",
       "      <td>5.279544e-01</td>\n",
       "      <td>9.991554e-01</td>\n",
       "      <td>0.938228</td>\n",
       "      <td>6.531123e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.854854e-01</td>\n",
       "      <td>9.999777e-01</td>\n",
       "      <td>9.516612e-01</td>\n",
       "      <td>7.897650e-01</td>\n",
       "      <td>9.958682e-01</td>\n",
       "      <td>9.851004e-01</td>\n",
       "      <td>9.999923e-01</td>\n",
       "      <td>0.010582</td>\n",
       "      <td>7.781755e-01</td>\n",
       "      <td>2.496092e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>1.244802e-02</td>\n",
       "      <td>0.992493</td>\n",
       "      <td>1.813867e-02</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.585667</td>\n",
       "      <td>9.998382e-01</td>\n",
       "      <td>9.997755e-01</td>\n",
       "      <td>0.082235</td>\n",
       "      <td>5.800074e-02</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>2.697955e-03</td>\n",
       "      <td>5.853162e-02</td>\n",
       "      <td>9.996061e-01</td>\n",
       "      <td>1.145558e-03</td>\n",
       "      <td>7.614732e-03</td>\n",
       "      <td>9.875183e-01</td>\n",
       "      <td>9.999026e-01</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>5.571116e-02</td>\n",
       "      <td>2.419810e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>9.995771e-01</td>\n",
       "      <td>0.997964</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.102224</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>6.285930e-08</td>\n",
       "      <td>9.982022e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.397281e-10</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>1.219459e-10</td>\n",
       "      <td>9.194591e-01</td>\n",
       "      <td>6.604906e-13</td>\n",
       "      <td>9.991876e-01</td>\n",
       "      <td>4.916412e-02</td>\n",
       "      <td>1.357784e-03</td>\n",
       "      <td>9.991697e-01</td>\n",
       "      <td>0.996749</td>\n",
       "      <td>2.150211e-03</td>\n",
       "      <td>2.735774e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.069769</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.998935</td>\n",
       "      <td>0.992937</td>\n",
       "      <td>9.528593e-01</td>\n",
       "      <td>1.127196e-05</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>7.323690e-04</td>\n",
       "      <td>0.994680</td>\n",
       "      <td>9.341871e-01</td>\n",
       "      <td>9.999479e-01</td>\n",
       "      <td>9.999354e-01</td>\n",
       "      <td>9.691881e-06</td>\n",
       "      <td>9.999770e-01</td>\n",
       "      <td>9.999853e-01</td>\n",
       "      <td>3.738622e-05</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>9.999276e-01</td>\n",
       "      <td>7.104409e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>9.999919e-01</td>\n",
       "      <td>0.999857</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.132667</td>\n",
       "      <td>0.713718</td>\n",
       "      <td>9.492947e-06</td>\n",
       "      <td>9.943183e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.949519e-05</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>1.579181e-06</td>\n",
       "      <td>8.556538e-03</td>\n",
       "      <td>6.250071e-05</td>\n",
       "      <td>9.999804e-01</td>\n",
       "      <td>6.206724e-05</td>\n",
       "      <td>5.777020e-05</td>\n",
       "      <td>1.952201e-01</td>\n",
       "      <td>0.999779</td>\n",
       "      <td>1.425272e-05</td>\n",
       "      <td>9.174622e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.443121</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.997710</td>\n",
       "      <td>0.999042</td>\n",
       "      <td>9.966443e-01</td>\n",
       "      <td>5.262723e-06</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>6.644081e-04</td>\n",
       "      <td>0.998826</td>\n",
       "      <td>9.184116e-01</td>\n",
       "      <td>9.619719e-01</td>\n",
       "      <td>9.996790e-01</td>\n",
       "      <td>6.026992e-06</td>\n",
       "      <td>9.720117e-01</td>\n",
       "      <td>9.988384e-01</td>\n",
       "      <td>1.963415e-02</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>9.988311e-01</td>\n",
       "      <td>1.126539e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>9.999567e-01</td>\n",
       "      <td>0.991575</td>\n",
       "      <td>9.997956e-01</td>\n",
       "      <td>0.993017</td>\n",
       "      <td>0.999459</td>\n",
       "      <td>9.907833e-01</td>\n",
       "      <td>8.947784e-01</td>\n",
       "      <td>0.010496</td>\n",
       "      <td>7.641231e-02</td>\n",
       "      <td>0.990980</td>\n",
       "      <td>2.616576e-02</td>\n",
       "      <td>8.328405e-01</td>\n",
       "      <td>9.995334e-01</td>\n",
       "      <td>9.509410e-01</td>\n",
       "      <td>1.745957e-01</td>\n",
       "      <td>4.448363e-01</td>\n",
       "      <td>9.806773e-01</td>\n",
       "      <td>0.531551</td>\n",
       "      <td>6.200535e-01</td>\n",
       "      <td>1.029146e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>9.999980e-01</td>\n",
       "      <td>0.988574</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.986739</td>\n",
       "      <td>9.421500e-04</td>\n",
       "      <td>9.824730e-01</td>\n",
       "      <td>0.995236</td>\n",
       "      <td>9.953833e-01</td>\n",
       "      <td>0.265717</td>\n",
       "      <td>9.966738e-01</td>\n",
       "      <td>7.139744e-01</td>\n",
       "      <td>6.418411e-04</td>\n",
       "      <td>9.999890e-01</td>\n",
       "      <td>6.617320e-03</td>\n",
       "      <td>1.762567e-03</td>\n",
       "      <td>7.551752e-01</td>\n",
       "      <td>0.999513</td>\n",
       "      <td>3.736563e-03</td>\n",
       "      <td>9.415157e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>7.404959e-03</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>2.239620e-01</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>2.614255e-03</td>\n",
       "      <td>8.734784e-04</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>6.445584e-01</td>\n",
       "      <td>0.922678</td>\n",
       "      <td>9.933227e-06</td>\n",
       "      <td>4.853597e-05</td>\n",
       "      <td>1.666934e-05</td>\n",
       "      <td>2.462417e-06</td>\n",
       "      <td>1.905407e-01</td>\n",
       "      <td>8.292890e-06</td>\n",
       "      <td>8.724671e-05</td>\n",
       "      <td>0.099634</td>\n",
       "      <td>7.134842e-06</td>\n",
       "      <td>1.231409e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>9.999950e-01</td>\n",
       "      <td>0.923766</td>\n",
       "      <td>9.999970e-01</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.974799</td>\n",
       "      <td>2.214449e-04</td>\n",
       "      <td>8.968836e-01</td>\n",
       "      <td>0.141608</td>\n",
       "      <td>1.825624e-01</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>2.719495e-01</td>\n",
       "      <td>9.990975e-01</td>\n",
       "      <td>7.168865e-01</td>\n",
       "      <td>9.994874e-01</td>\n",
       "      <td>8.838552e-01</td>\n",
       "      <td>9.267399e-01</td>\n",
       "      <td>9.989949e-01</td>\n",
       "      <td>0.999571</td>\n",
       "      <td>9.292160e-01</td>\n",
       "      <td>1.456978e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.990453</td>\n",
       "      <td>7.210847e-04</td>\n",
       "      <td>9.974165e-01</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>9.723954e-01</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>9.485523e-01</td>\n",
       "      <td>7.265253e-03</td>\n",
       "      <td>3.298223e-05</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>3.002260e-04</td>\n",
       "      <td>2.857350e-04</td>\n",
       "      <td>1.813723e-03</td>\n",
       "      <td>0.999859</td>\n",
       "      <td>2.709757e-02</td>\n",
       "      <td>9.999926e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>9.999881e-01</td>\n",
       "      <td>0.886869</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>0.278497</td>\n",
       "      <td>1.413452e-03</td>\n",
       "      <td>9.985300e-01</td>\n",
       "      <td>0.999946</td>\n",
       "      <td>9.998626e-01</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>9.997835e-01</td>\n",
       "      <td>9.999952e-01</td>\n",
       "      <td>1.304638e-04</td>\n",
       "      <td>9.999970e-01</td>\n",
       "      <td>9.982779e-01</td>\n",
       "      <td>9.987465e-01</td>\n",
       "      <td>9.997171e-01</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>9.877952e-01</td>\n",
       "      <td>2.779374e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>9.999994e-01</td>\n",
       "      <td>0.912460</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>0.984320</td>\n",
       "      <td>2.731469e-01</td>\n",
       "      <td>7.464710e-01</td>\n",
       "      <td>0.802343</td>\n",
       "      <td>7.061719e-01</td>\n",
       "      <td>0.996008</td>\n",
       "      <td>5.509360e-01</td>\n",
       "      <td>9.998620e-01</td>\n",
       "      <td>8.037775e-01</td>\n",
       "      <td>9.568705e-01</td>\n",
       "      <td>9.076113e-01</td>\n",
       "      <td>3.714641e-01</td>\n",
       "      <td>9.835144e-01</td>\n",
       "      <td>0.462684</td>\n",
       "      <td>3.897207e-02</td>\n",
       "      <td>1.726958e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.979875</td>\n",
       "      <td>0.539263</td>\n",
       "      <td>8.673732e-01</td>\n",
       "      <td>9.967802e-01</td>\n",
       "      <td>0.316683</td>\n",
       "      <td>1.841803e-01</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>6.494211e-02</td>\n",
       "      <td>9.881802e-01</td>\n",
       "      <td>9.248307e-01</td>\n",
       "      <td>9.835920e-01</td>\n",
       "      <td>3.185645e-03</td>\n",
       "      <td>2.644372e-04</td>\n",
       "      <td>9.999886e-01</td>\n",
       "      <td>0.225953</td>\n",
       "      <td>4.736198e-05</td>\n",
       "      <td>6.885214e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.934128</td>\n",
       "      <td>0.996983</td>\n",
       "      <td>3.443652e-05</td>\n",
       "      <td>7.327547e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.784814e-07</td>\n",
       "      <td>0.999732</td>\n",
       "      <td>1.390103e-08</td>\n",
       "      <td>5.073905e-01</td>\n",
       "      <td>1.633633e-08</td>\n",
       "      <td>2.217594e-01</td>\n",
       "      <td>9.074172e-01</td>\n",
       "      <td>1.225324e-01</td>\n",
       "      <td>8.958351e-01</td>\n",
       "      <td>0.814906</td>\n",
       "      <td>2.333156e-01</td>\n",
       "      <td>5.851128e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>9.995201e-01</td>\n",
       "      <td>0.901040</td>\n",
       "      <td>9.998468e-01</td>\n",
       "      <td>0.328823</td>\n",
       "      <td>0.998523</td>\n",
       "      <td>2.164039e-01</td>\n",
       "      <td>9.491063e-01</td>\n",
       "      <td>0.055259</td>\n",
       "      <td>3.131562e-01</td>\n",
       "      <td>0.140806</td>\n",
       "      <td>7.690450e-01</td>\n",
       "      <td>7.152768e-01</td>\n",
       "      <td>9.845268e-01</td>\n",
       "      <td>9.979879e-01</td>\n",
       "      <td>2.799568e-01</td>\n",
       "      <td>5.583459e-01</td>\n",
       "      <td>4.055885e-01</td>\n",
       "      <td>0.738170</td>\n",
       "      <td>9.140029e-01</td>\n",
       "      <td>9.546053e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>9.994294e-01</td>\n",
       "      <td>0.979857</td>\n",
       "      <td>9.999942e-01</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.998226</td>\n",
       "      <td>4.839577e-04</td>\n",
       "      <td>9.977396e-01</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>9.998423e-01</td>\n",
       "      <td>0.014088</td>\n",
       "      <td>9.998339e-01</td>\n",
       "      <td>9.209788e-01</td>\n",
       "      <td>1.632371e-05</td>\n",
       "      <td>9.999421e-01</td>\n",
       "      <td>2.611695e-01</td>\n",
       "      <td>6.656475e-01</td>\n",
       "      <td>2.072351e-01</td>\n",
       "      <td>0.999488</td>\n",
       "      <td>9.301651e-01</td>\n",
       "      <td>9.967242e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>9.985013e-01</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.252573</td>\n",
       "      <td>0.270605</td>\n",
       "      <td>4.177345e-07</td>\n",
       "      <td>9.956944e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.715670e-07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.013887e-08</td>\n",
       "      <td>9.999452e-01</td>\n",
       "      <td>1.351947e-09</td>\n",
       "      <td>9.503731e-01</td>\n",
       "      <td>9.989439e-01</td>\n",
       "      <td>9.969585e-01</td>\n",
       "      <td>9.996724e-01</td>\n",
       "      <td>0.510108</td>\n",
       "      <td>9.332535e-01</td>\n",
       "      <td>5.779290e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>5.479691e-01</td>\n",
       "      <td>0.831812</td>\n",
       "      <td>9.738950e-01</td>\n",
       "      <td>0.355249</td>\n",
       "      <td>0.847798</td>\n",
       "      <td>3.346031e-02</td>\n",
       "      <td>8.037581e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>8.600786e-01</td>\n",
       "      <td>0.176204</td>\n",
       "      <td>1.178343e-02</td>\n",
       "      <td>9.917070e-01</td>\n",
       "      <td>6.527213e-04</td>\n",
       "      <td>5.123333e-01</td>\n",
       "      <td>9.255978e-01</td>\n",
       "      <td>9.370759e-01</td>\n",
       "      <td>5.493162e-01</td>\n",
       "      <td>0.338772</td>\n",
       "      <td>5.859932e-01</td>\n",
       "      <td>3.287783e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>0.835811</td>\n",
       "      <td>9.999995e-01</td>\n",
       "      <td>0.490594</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>7.014070e-01</td>\n",
       "      <td>3.254855e-02</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>1.579665e-01</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>1.028931e-01</td>\n",
       "      <td>7.872433e-01</td>\n",
       "      <td>9.921652e-01</td>\n",
       "      <td>4.402867e-01</td>\n",
       "      <td>1.502271e-02</td>\n",
       "      <td>9.750821e-02</td>\n",
       "      <td>9.998955e-01</td>\n",
       "      <td>0.112548</td>\n",
       "      <td>1.571351e-02</td>\n",
       "      <td>1.980414e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.436333</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.012884</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>5.430749e-04</td>\n",
       "      <td>2.175986e-02</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>2.470908e-04</td>\n",
       "      <td>0.021632</td>\n",
       "      <td>5.665510e-01</td>\n",
       "      <td>9.863935e-01</td>\n",
       "      <td>9.986792e-01</td>\n",
       "      <td>9.998912e-01</td>\n",
       "      <td>9.966916e-01</td>\n",
       "      <td>9.937555e-01</td>\n",
       "      <td>5.954945e-04</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>9.915132e-01</td>\n",
       "      <td>9.998317e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>9.999995e-01</td>\n",
       "      <td>0.997660</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.007566</td>\n",
       "      <td>0.993312</td>\n",
       "      <td>1.493474e-04</td>\n",
       "      <td>9.982829e-01</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>9.990189e-01</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>9.950571e-01</td>\n",
       "      <td>9.999948e-01</td>\n",
       "      <td>4.915754e-04</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>9.985310e-01</td>\n",
       "      <td>9.988828e-01</td>\n",
       "      <td>9.994205e-01</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>9.962714e-01</td>\n",
       "      <td>5.054965e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>9.999981e-01</td>\n",
       "      <td>0.967733</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>0.355369</td>\n",
       "      <td>0.979441</td>\n",
       "      <td>1.329465e-01</td>\n",
       "      <td>9.685857e-01</td>\n",
       "      <td>0.800659</td>\n",
       "      <td>9.527197e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.807435e-01</td>\n",
       "      <td>9.999686e-01</td>\n",
       "      <td>9.020782e-01</td>\n",
       "      <td>9.986919e-01</td>\n",
       "      <td>9.932417e-01</td>\n",
       "      <td>9.975096e-01</td>\n",
       "      <td>9.995166e-01</td>\n",
       "      <td>0.644687</td>\n",
       "      <td>9.866655e-01</td>\n",
       "      <td>2.778176e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>5.513748e-08</td>\n",
       "      <td>0.665755</td>\n",
       "      <td>1.174142e-08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034603</td>\n",
       "      <td>9.999605e-01</td>\n",
       "      <td>9.994826e-01</td>\n",
       "      <td>0.135176</td>\n",
       "      <td>9.438224e-01</td>\n",
       "      <td>0.253242</td>\n",
       "      <td>3.213385e-07</td>\n",
       "      <td>4.344605e-08</td>\n",
       "      <td>2.371002e-01</td>\n",
       "      <td>1.256986e-09</td>\n",
       "      <td>1.832637e-06</td>\n",
       "      <td>1.579100e-03</td>\n",
       "      <td>1.061184e-04</td>\n",
       "      <td>0.854558</td>\n",
       "      <td>1.021122e-09</td>\n",
       "      <td>4.038845e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.194353</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.860118</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>9.841690e-01</td>\n",
       "      <td>6.474335e-04</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>5.035093e-04</td>\n",
       "      <td>0.857015</td>\n",
       "      <td>2.203493e-01</td>\n",
       "      <td>9.752623e-01</td>\n",
       "      <td>9.995641e-01</td>\n",
       "      <td>1.156092e-02</td>\n",
       "      <td>8.376536e-01</td>\n",
       "      <td>9.840236e-01</td>\n",
       "      <td>3.246557e-01</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>9.295815e-01</td>\n",
       "      <td>2.493776e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>4.514689e-02</td>\n",
       "      <td>0.999844</td>\n",
       "      <td>9.829219e-02</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.981693</td>\n",
       "      <td>6.149712e-01</td>\n",
       "      <td>8.217149e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.254349e-01</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>1.479901e-05</td>\n",
       "      <td>1.768205e-02</td>\n",
       "      <td>1.036665e-04</td>\n",
       "      <td>8.296981e-05</td>\n",
       "      <td>3.423075e-02</td>\n",
       "      <td>1.112360e-01</td>\n",
       "      <td>9.948124e-01</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>2.918106e-01</td>\n",
       "      <td>3.130289e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>9.999905e-01</td>\n",
       "      <td>0.999808</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.200327</td>\n",
       "      <td>0.050085</td>\n",
       "      <td>2.768207e-07</td>\n",
       "      <td>9.597699e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.420390e-11</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>7.862746e-11</td>\n",
       "      <td>6.743020e-02</td>\n",
       "      <td>1.883981e-10</td>\n",
       "      <td>9.994710e-01</td>\n",
       "      <td>2.139938e-01</td>\n",
       "      <td>6.098756e-03</td>\n",
       "      <td>1.582589e-03</td>\n",
       "      <td>0.928795</td>\n",
       "      <td>4.982027e-02</td>\n",
       "      <td>9.990647e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>9.852912e-01</td>\n",
       "      <td>0.957150</td>\n",
       "      <td>8.460226e-01</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>0.995615</td>\n",
       "      <td>8.212482e-01</td>\n",
       "      <td>9.042652e-05</td>\n",
       "      <td>0.018283</td>\n",
       "      <td>9.902616e-01</td>\n",
       "      <td>0.926492</td>\n",
       "      <td>9.839653e-01</td>\n",
       "      <td>5.238231e-04</td>\n",
       "      <td>8.565332e-01</td>\n",
       "      <td>1.652136e-04</td>\n",
       "      <td>8.808833e-01</td>\n",
       "      <td>1.574640e-02</td>\n",
       "      <td>9.586984e-05</td>\n",
       "      <td>0.225162</td>\n",
       "      <td>9.103803e-01</td>\n",
       "      <td>1.347866e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.999919</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.137513</td>\n",
       "      <td>0.994096</td>\n",
       "      <td>2.740576e-07</td>\n",
       "      <td>9.891027e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.778894e-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.096510e-07</td>\n",
       "      <td>9.999985e-01</td>\n",
       "      <td>7.247409e-04</td>\n",
       "      <td>9.999679e-01</td>\n",
       "      <td>9.965875e-01</td>\n",
       "      <td>9.976386e-01</td>\n",
       "      <td>9.998993e-01</td>\n",
       "      <td>0.979211</td>\n",
       "      <td>9.688367e-01</td>\n",
       "      <td>2.240791e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1297 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              danr   CG14427           dan   CG43394     ImpL2          Nek2  \\\n",
       "0     1.000000e+00  0.952570  1.000000e+00  0.999968  0.999427  9.988462e-01   \n",
       "1     1.000000e+00  0.073005  1.000000e+00  0.260960  0.999972  9.924010e-01   \n",
       "2     1.000000e+00  0.999980  1.000000e+00  0.128240  0.998572  2.026050e-05   \n",
       "3     9.999962e-01  0.788806  9.998572e-01  1.000000  0.999795  9.279311e-01   \n",
       "4     1.000000e+00  0.999593  1.000000e+00  0.004106  0.999833  1.097144e-07   \n",
       "5     9.857821e-03  0.398339  6.580142e-03  0.995009  0.094408  9.904410e-01   \n",
       "6     9.970232e-01  0.999684  9.999852e-01  0.993155  0.999887  6.502574e-01   \n",
       "7     7.972035e-04  0.999992  3.349570e-05  0.999668  0.999975  8.212480e-06   \n",
       "8     2.495757e-03  0.182013  4.974751e-02  0.004361  0.016997  9.591834e-03   \n",
       "9     4.811024e-03  0.999988  2.724231e-04  0.999914  0.999469  4.158520e-04   \n",
       "10    9.999993e-01  0.997766  1.000000e+00  0.668968  0.999614  2.553989e-02   \n",
       "11    1.000000e+00  0.143619  1.000000e+00  0.001303  0.999999  3.550558e-04   \n",
       "12    9.999760e-01  0.994280  9.999995e-01  0.009707  0.996073  6.270727e-04   \n",
       "13    2.865134e-03  0.638284  1.317361e-04  0.999929  0.017298  9.998714e-01   \n",
       "14    4.615304e-03  0.999890  9.990397e-01  0.999935  0.999414  5.893366e-02   \n",
       "15    1.393367e-07  0.005413  2.693647e-08  0.999980  0.001292  9.999983e-01   \n",
       "16    1.000000e+00  0.999933  1.000000e+00  0.036880  0.999848  1.459149e-06   \n",
       "17    9.812086e-01  0.564072  9.999998e-01  0.000162  0.116492  1.887775e-06   \n",
       "18    1.000000e+00  0.996334  1.000000e+00  0.504082  0.992520  2.282194e-01   \n",
       "19    9.995290e-01  0.133104  1.000000e+00  0.996271  0.051228  9.970793e-01   \n",
       "20    1.000000e+00  0.809125  1.000000e+00  0.000241  0.982447  7.849649e-05   \n",
       "21    1.000000e+00  0.999863  1.000000e+00  0.751584  1.000000  5.413376e-05   \n",
       "22    9.997407e-01  0.999762  9.999999e-01  0.841562  0.978401  3.264277e-06   \n",
       "23    9.996756e-01  0.961402  9.999933e-01  0.957463  0.997574  8.018143e-01   \n",
       "24    7.357590e-04  0.999996  1.309942e-05  0.999868  0.999922  5.366280e-04   \n",
       "25    9.966834e-01  0.993407  6.423675e-01  0.999941  0.999382  3.252073e-01   \n",
       "26    1.000000e+00  0.999975  1.000000e+00  0.999989  1.000000  9.976103e-01   \n",
       "27    9.999926e-01  0.999962  1.000000e+00  0.521378  0.997482  1.212213e-05   \n",
       "28    6.290200e-07  0.074248  1.633305e-09  1.000000  0.000110  1.000000e+00   \n",
       "29    9.984952e-01  0.996364  9.999988e-01  0.000521  0.998054  5.185466e-05   \n",
       "...            ...       ...           ...       ...       ...           ...   \n",
       "1267  9.998910e-01  0.996986  1.000000e+00  0.805672  0.482736  3.877808e-06   \n",
       "1268  9.996731e-01  0.859843  9.997267e-01  0.883014  0.886068  5.279544e-01   \n",
       "1269  1.244802e-02  0.992493  1.813867e-02  0.999990  0.585667  9.998382e-01   \n",
       "1270  9.995771e-01  0.997964  1.000000e+00  0.102224  0.010748  6.285930e-08   \n",
       "1271  1.000000e+00  0.069769  1.000000e+00  0.998935  0.992937  9.528593e-01   \n",
       "1272  9.999919e-01  0.999857  1.000000e+00  0.132667  0.713718  9.492947e-06   \n",
       "1273  1.000000e+00  0.443121  1.000000e+00  0.997710  0.999042  9.966443e-01   \n",
       "1274  9.999567e-01  0.991575  9.997956e-01  0.993017  0.999459  9.907833e-01   \n",
       "1275  9.999980e-01  0.988574  1.000000e+00  0.001011  0.986739  9.421500e-04   \n",
       "1276  7.404959e-03  0.999955  2.239620e-01  0.999918  0.999655  2.614255e-03   \n",
       "1277  9.999950e-01  0.923766  9.999970e-01  0.010001  0.974799  2.214449e-04   \n",
       "1278  9.999999e-01  0.997800  1.000000e+00  0.003965  0.990453  7.210847e-04   \n",
       "1279  9.999881e-01  0.886869  1.000000e+00  0.002989  0.278497  1.413452e-03   \n",
       "1280  9.999994e-01  0.912460  1.000000e+00  0.368600  0.984320  2.731469e-01   \n",
       "1281  1.000000e+00  0.996180  1.000000e+00  0.979875  0.539263  8.673732e-01   \n",
       "1282  1.000000e+00  0.999991  1.000000e+00  0.934128  0.996983  3.443652e-05   \n",
       "1283  9.995201e-01  0.901040  9.998468e-01  0.328823  0.998523  2.164039e-01   \n",
       "1284  9.994294e-01  0.979857  9.999942e-01  0.000463  0.998226  4.839577e-04   \n",
       "1285  9.985013e-01  0.997169  1.000000e+00  0.252573  0.270605  4.177345e-07   \n",
       "1286  5.479691e-01  0.831812  9.738950e-01  0.355249  0.847798  3.346031e-02   \n",
       "1287  9.999999e-01  0.835811  9.999995e-01  0.490594  0.999739  7.014070e-01   \n",
       "1288  1.000000e+00  0.436333  1.000000e+00  0.012884  0.999974  5.430749e-04   \n",
       "1289  9.999995e-01  0.997660  1.000000e+00  0.007566  0.993312  1.493474e-04   \n",
       "1290  9.999981e-01  0.967733  9.999999e-01  0.355369  0.979441  1.329465e-01   \n",
       "1291  5.513748e-08  0.665755  1.174142e-08  1.000000  0.034603  9.999605e-01   \n",
       "1292  1.000000e+00  0.194353  1.000000e+00  0.860118  0.999990  9.841690e-01   \n",
       "1293  4.514689e-02  0.999844  9.829219e-02  0.999996  0.981693  6.149712e-01   \n",
       "1294  9.999905e-01  0.999808  1.000000e+00  0.200327  0.050085  2.768207e-07   \n",
       "1295  9.852912e-01  0.957150  8.460226e-01  0.999964  0.995615  8.212482e-01   \n",
       "1296  1.000000e+00  0.999919  1.000000e+00  0.137513  0.994096  2.740576e-07   \n",
       "\n",
       "            CG8147       Ama        Btk29A       trn          numb  \\\n",
       "0     2.225487e-06  0.000144  8.976961e-01  0.910867  9.992010e-01   \n",
       "1     4.919980e-04  0.003902  4.686632e-02  0.999998  3.026228e-01   \n",
       "2     7.032250e-03  0.999997  5.656553e-06  1.000000  2.185221e-05   \n",
       "3     6.623404e-07  0.001171  9.910707e-01  0.174682  9.992439e-01   \n",
       "4     1.467953e-01  0.999978  1.737630e-09  0.986156  4.429802e-06   \n",
       "5     9.999146e-01  0.999999  7.737213e-01  0.999999  1.550446e-04   \n",
       "6     1.507457e-03  0.459570  2.764302e-01  0.912435  6.476845e-01   \n",
       "7     1.815548e-04  0.999999  9.984673e-01  0.999898  2.098095e-04   \n",
       "8     9.999360e-01  0.999893  9.998872e-01  0.000308  9.999392e-01   \n",
       "9     1.917854e-04  0.999983  9.913943e-01  0.999727  3.918419e-04   \n",
       "10    9.212085e-03  0.998669  9.818991e-05  0.957203  1.136290e-03   \n",
       "11    2.620373e-03  0.000313  4.401929e-03  0.685028  9.751267e-01   \n",
       "12    9.107602e-01  0.999934  9.996758e-01  0.999992  9.931347e-01   \n",
       "13    9.981104e-01  0.038774  7.815810e-02  0.244115  2.733906e-03   \n",
       "14    1.338275e-03  1.000000  9.659755e-01  0.243339  1.676398e-03   \n",
       "15    9.999737e-01  0.007292  4.211594e-01  0.000633  4.940522e-06   \n",
       "16    8.769456e-02  1.000000  1.367542e-08  0.000287  7.736443e-08   \n",
       "17    9.952095e-01  0.999705  9.984468e-01  0.999681  9.998662e-01   \n",
       "18    9.886728e-01  0.994003  9.966336e-01  0.999992  9.993253e-01   \n",
       "19    2.451493e-03  0.989910  9.763867e-01  0.046796  9.990502e-01   \n",
       "20    8.919598e-01  0.999904  9.998344e-01  0.991347  9.999064e-01   \n",
       "21    5.347388e-02  1.000000  2.750275e-09  1.000000  1.600709e-07   \n",
       "22    9.080054e-01  1.000000  4.722379e-08  1.000000  1.774804e-08   \n",
       "23    8.378164e-01  0.903518  1.236204e-03  0.007377  5.552902e-03   \n",
       "24    1.939070e-05  0.999997  9.998235e-01  0.999912  6.849265e-04   \n",
       "25    3.819732e-06  0.011503  9.953775e-01  0.963407  9.395645e-01   \n",
       "26    4.243368e-07  0.000007  9.880773e-01  0.999090  9.996192e-01   \n",
       "27    1.623061e-01  1.000000  6.982784e-07  0.991716  1.571913e-06   \n",
       "28    9.999992e-01  0.008219  2.049086e-01  0.995819  9.575192e-07   \n",
       "29    9.917101e-01  0.999947  9.992722e-01  0.994933  9.989964e-01   \n",
       "...            ...       ...           ...       ...           ...   \n",
       "1267  9.159855e-01  1.000000  7.310827e-09  0.996079  2.904256e-08   \n",
       "1268  9.991554e-01  0.938228  6.531123e-01  1.000000  8.854854e-01   \n",
       "1269  9.997755e-01  0.082235  5.800074e-02  0.999999  2.697955e-03   \n",
       "1270  9.982022e-01  1.000000  2.397281e-10  0.999720  1.219459e-10   \n",
       "1271  1.127196e-05  0.000523  7.323690e-04  0.994680  9.341871e-01   \n",
       "1272  9.943183e-01  0.999998  1.949519e-05  0.015029  1.579181e-06   \n",
       "1273  5.262723e-06  0.000066  6.644081e-04  0.998826  9.184116e-01   \n",
       "1274  8.947784e-01  0.010496  7.641231e-02  0.990980  2.616576e-02   \n",
       "1275  9.824730e-01  0.995236  9.953833e-01  0.265717  9.966738e-01   \n",
       "1276  8.734784e-04  0.999997  6.445584e-01  0.922678  9.933227e-06   \n",
       "1277  8.968836e-01  0.141608  1.825624e-01  0.999910  2.719495e-01   \n",
       "1278  9.974165e-01  0.999943  9.723954e-01  0.000271  9.485523e-01   \n",
       "1279  9.985300e-01  0.999946  9.998626e-01  0.999993  9.997835e-01   \n",
       "1280  7.464710e-01  0.802343  7.061719e-01  0.996008  5.509360e-01   \n",
       "1281  9.967802e-01  0.316683  1.841803e-01  0.999999  6.494211e-02   \n",
       "1282  7.327547e-04  1.000000  1.784814e-07  0.999732  1.390103e-08   \n",
       "1283  9.491063e-01  0.055259  3.131562e-01  0.140806  7.690450e-01   \n",
       "1284  9.977396e-01  0.999772  9.998423e-01  0.014088  9.998339e-01   \n",
       "1285  9.956944e-01  1.000000  1.715670e-07  1.000000  1.013887e-08   \n",
       "1286  8.037581e-01  0.999998  8.600786e-01  0.176204  1.178343e-02   \n",
       "1287  3.254855e-02  0.006348  1.579665e-01  0.999986  1.028931e-01   \n",
       "1288  2.175986e-02  0.001911  2.470908e-04  0.021632  5.665510e-01   \n",
       "1289  9.982829e-01  0.999314  9.990189e-01  0.999993  9.950571e-01   \n",
       "1290  9.685857e-01  0.800659  9.527197e-01  1.000000  9.807435e-01   \n",
       "1291  9.994826e-01  0.135176  9.438224e-01  0.253242  3.213385e-07   \n",
       "1292  6.474335e-04  0.006079  5.035093e-04  0.857015  2.203493e-01   \n",
       "1293  8.217149e-01  1.000000  1.254349e-01  0.999526  1.479901e-05   \n",
       "1294  9.597699e-01  1.000000  2.420390e-11  0.001580  7.862746e-11   \n",
       "1295  9.042652e-05  0.018283  9.902616e-01  0.926492  9.839653e-01   \n",
       "1296  9.891027e-01  1.000000  3.778894e-06  1.000000  5.096510e-07   \n",
       "\n",
       "               prd           brk           tsh           pxb           dpn  \\\n",
       "0     4.948604e-06  9.973761e-01  1.085652e-04  4.932669e-04  1.096998e-04   \n",
       "1     5.767848e-01  9.956729e-01  3.016724e-02  6.589219e-04  9.117642e-03   \n",
       "2     9.262726e-01  1.872751e-04  9.973971e-01  9.825932e-01  5.684511e-02   \n",
       "3     1.558326e-05  9.878981e-01  5.819520e-07  5.616950e-01  2.520103e-03   \n",
       "4     9.999733e-01  1.644533e-03  9.999981e-01  9.961914e-01  9.938365e-01   \n",
       "5     8.348986e-03  1.097112e-04  6.478760e-03  4.778898e-04  8.386919e-01   \n",
       "6     8.231743e-04  6.520438e-01  1.139168e-02  2.286469e-03  1.270115e-05   \n",
       "7     5.091575e-05  2.507595e-06  1.720824e-05  9.998615e-01  2.240034e-04   \n",
       "8     7.474542e-01  3.679442e-06  9.718327e-01  1.333877e-01  4.582791e-01   \n",
       "9     1.168152e-04  1.891582e-04  4.295379e-06  9.955487e-01  1.478557e-05   \n",
       "10    8.066000e-01  6.916312e-03  9.018955e-01  9.928236e-01  9.150981e-01   \n",
       "11    9.994123e-01  9.996619e-01  9.993830e-01  9.965026e-01  9.987382e-01   \n",
       "12    9.981224e-01  1.047253e-05  9.999948e-01  9.814327e-01  9.147418e-01   \n",
       "13    1.631580e-02  9.454259e-01  1.690295e-03  2.211262e-02  9.957338e-01   \n",
       "14    3.930203e-06  2.040161e-06  3.323668e-07  6.603202e-03  2.577902e-05   \n",
       "15    6.464802e-09  2.028336e-01  7.572076e-06  5.822034e-07  3.205048e-04   \n",
       "16    9.425172e-04  8.211447e-09  9.999952e-01  9.757390e-04  8.728823e-06   \n",
       "17    9.999949e-01  4.112371e-05  9.999994e-01  9.923676e-01  9.695355e-01   \n",
       "18    9.994853e-01  1.097422e-02  9.998964e-01  4.127295e-04  6.243932e-04   \n",
       "19    6.390570e-03  6.596552e-03  2.045522e-06  9.012696e-01  4.779747e-01   \n",
       "20    9.996842e-01  1.517768e-04  9.999899e-01  7.483673e-04  2.139357e-04   \n",
       "21    9.986628e-01  2.014484e-06  9.999235e-01  1.959178e-01  3.780622e-03   \n",
       "22    9.993616e-01  1.896762e-08  9.978809e-01  9.166709e-01  9.140667e-01   \n",
       "23    1.163220e-01  8.741810e-01  8.932080e-01  1.407525e-02  3.657069e-02   \n",
       "24    3.447213e-06  6.536806e-07  1.636066e-06  9.993469e-01  1.056744e-06   \n",
       "25    5.354946e-05  5.687503e-01  5.921737e-06  9.194887e-01  1.795016e-04   \n",
       "26    1.085425e-06  9.997811e-01  9.024175e-05  3.737268e-05  5.879258e-08   \n",
       "27    9.989116e-01  1.794630e-06  9.994752e-01  9.997467e-01  9.932792e-01   \n",
       "28    6.270267e-07  9.988130e-01  2.106784e-08  3.456530e-06  9.997488e-01   \n",
       "29    9.999472e-01  3.654560e-05  9.999973e-01  9.992108e-01  9.995689e-01   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1267  9.909118e-01  2.973403e-10  9.461822e-01  9.003154e-01  5.748441e-01   \n",
       "1268  9.999777e-01  9.516612e-01  7.897650e-01  9.958682e-01  9.851004e-01   \n",
       "1269  5.853162e-02  9.996061e-01  1.145558e-03  7.614732e-03  9.875183e-01   \n",
       "1270  9.194591e-01  6.604906e-13  9.991876e-01  4.916412e-02  1.357784e-03   \n",
       "1271  9.999479e-01  9.999354e-01  9.691881e-06  9.999770e-01  9.999853e-01   \n",
       "1272  8.556538e-03  6.250071e-05  9.999804e-01  6.206724e-05  5.777020e-05   \n",
       "1273  9.619719e-01  9.996790e-01  6.026992e-06  9.720117e-01  9.988384e-01   \n",
       "1274  8.328405e-01  9.995334e-01  9.509410e-01  1.745957e-01  4.448363e-01   \n",
       "1275  7.139744e-01  6.418411e-04  9.999890e-01  6.617320e-03  1.762567e-03   \n",
       "1276  4.853597e-05  1.666934e-05  2.462417e-06  1.905407e-01  8.292890e-06   \n",
       "1277  9.990975e-01  7.168865e-01  9.994874e-01  8.838552e-01  9.267399e-01   \n",
       "1278  7.265253e-03  3.298223e-05  9.999999e-01  3.002260e-04  2.857350e-04   \n",
       "1279  9.999952e-01  1.304638e-04  9.999970e-01  9.982779e-01  9.987465e-01   \n",
       "1280  9.998620e-01  8.037775e-01  9.568705e-01  9.076113e-01  3.714641e-01   \n",
       "1281  9.881802e-01  9.248307e-01  9.835920e-01  3.185645e-03  2.644372e-04   \n",
       "1282  5.073905e-01  1.633633e-08  2.217594e-01  9.074172e-01  1.225324e-01   \n",
       "1283  7.152768e-01  9.845268e-01  9.979879e-01  2.799568e-01  5.583459e-01   \n",
       "1284  9.209788e-01  1.632371e-05  9.999421e-01  2.611695e-01  6.656475e-01   \n",
       "1285  9.999452e-01  1.351947e-09  9.503731e-01  9.989439e-01  9.969585e-01   \n",
       "1286  9.917070e-01  6.527213e-04  5.123333e-01  9.255978e-01  9.370759e-01   \n",
       "1287  7.872433e-01  9.921652e-01  4.402867e-01  1.502271e-02  9.750821e-02   \n",
       "1288  9.863935e-01  9.986792e-01  9.998912e-01  9.966916e-01  9.937555e-01   \n",
       "1289  9.999948e-01  4.915754e-04  9.999998e-01  9.985310e-01  9.988828e-01   \n",
       "1290  9.999686e-01  9.020782e-01  9.986919e-01  9.932417e-01  9.975096e-01   \n",
       "1291  4.344605e-08  2.371002e-01  1.256986e-09  1.832637e-06  1.579100e-03   \n",
       "1292  9.752623e-01  9.995641e-01  1.156092e-02  8.376536e-01  9.840236e-01   \n",
       "1293  1.768205e-02  1.036665e-04  8.296981e-05  3.423075e-02  1.112360e-01   \n",
       "1294  6.743020e-02  1.883981e-10  9.994710e-01  2.139938e-01  6.098756e-03   \n",
       "1295  5.238231e-04  8.565332e-01  1.652136e-04  8.808833e-01  1.574640e-02   \n",
       "1296  9.999985e-01  7.247409e-04  9.999679e-01  9.965875e-01  9.976386e-01   \n",
       "\n",
       "               ftz        Kr             h           eve  \n",
       "0     3.081364e-06  0.692830  9.230255e-01  1.595170e-04  \n",
       "1     9.999350e-01  0.033744  1.860791e-03  2.594135e-05  \n",
       "2     9.997877e-01  0.999295  1.272992e-02  1.028869e-05  \n",
       "3     8.022946e-08  0.036852  9.923142e-01  4.249081e-05  \n",
       "4     9.665008e-01  0.999558  9.889327e-01  7.089722e-01  \n",
       "5     9.999241e-01  0.009804  3.880746e-03  9.181547e-07  \n",
       "6     1.181825e-03  0.920938  2.793467e-04  2.107628e-02  \n",
       "7     9.414664e-06  0.989408  1.642943e-04  5.226656e-06  \n",
       "8     3.363704e-02  0.946534  9.827011e-01  9.960330e-01  \n",
       "9     1.859974e-04  0.633407  9.499377e-06  4.998521e-07  \n",
       "10    2.341511e-01  0.963215  7.768021e-01  5.764536e-01  \n",
       "11    1.185822e-01  0.995253  9.969323e-01  9.813778e-01  \n",
       "12    9.998261e-01  0.999960  8.847806e-01  3.586725e-03  \n",
       "13    3.480875e-03  0.027029  2.698334e-02  1.070119e-01  \n",
       "14    7.779146e-06  0.004214  2.031399e-03  1.548131e-04  \n",
       "15    3.045832e-06  0.999865  1.738679e-07  9.803601e-05  \n",
       "16    9.196917e-04  0.999866  5.957111e-03  9.998777e-01  \n",
       "17    9.997806e-01  0.999988  5.820566e-01  1.634441e-05  \n",
       "18    9.998981e-01  0.995208  6.144069e-01  2.557264e-04  \n",
       "19    4.163624e-07  0.000225  9.997198e-01  8.621904e-02  \n",
       "20    9.998947e-01  0.999401  1.370329e-03  2.224371e-01  \n",
       "21    9.999964e-01  0.999053  3.804333e-02  2.570437e-04  \n",
       "22    9.999651e-01  0.816317  9.456939e-01  6.941300e-06  \n",
       "23    2.596068e-01  0.191250  1.999731e-01  9.880601e-01  \n",
       "24    3.958464e-06  0.997777  9.571102e-07  9.843995e-07  \n",
       "25    6.412070e-06  0.818450  1.410119e-02  2.079197e-05  \n",
       "26    1.925434e-06  0.999434  2.634638e-04  7.155194e-06  \n",
       "27    8.784008e-01  0.998702  9.942315e-01  1.421364e-01  \n",
       "28    2.526374e-01  0.000324  6.032673e-06  1.446585e-07  \n",
       "29    9.569515e-01  0.999960  9.983876e-01  4.413893e-01  \n",
       "...            ...       ...           ...           ...  \n",
       "1267  8.968162e-01  0.106455  9.168677e-01  3.899791e-02  \n",
       "1268  9.999923e-01  0.010582  7.781755e-01  2.496092e-07  \n",
       "1269  9.999026e-01  0.000037  5.571116e-02  2.419810e-07  \n",
       "1270  9.991697e-01  0.996749  2.150211e-03  2.735774e-04  \n",
       "1271  3.738622e-05  0.000017  9.999276e-01  7.104409e-03  \n",
       "1272  1.952201e-01  0.999779  1.425272e-05  9.174622e-01  \n",
       "1273  1.963415e-02  0.000260  9.988311e-01  1.126539e-03  \n",
       "1274  9.806773e-01  0.531551  6.200535e-01  1.029146e-02  \n",
       "1275  7.551752e-01  0.999513  3.736563e-03  9.415157e-01  \n",
       "1276  8.724671e-05  0.099634  7.134842e-06  1.231409e-04  \n",
       "1277  9.989949e-01  0.999571  9.292160e-01  1.456978e-03  \n",
       "1278  1.813723e-03  0.999859  2.709757e-02  9.999926e-01  \n",
       "1279  9.997171e-01  0.999874  9.877952e-01  2.779374e-04  \n",
       "1280  9.835144e-01  0.462684  3.897207e-02  1.726958e-02  \n",
       "1281  9.999886e-01  0.225953  4.736198e-05  6.885214e-06  \n",
       "1282  8.958351e-01  0.814906  2.333156e-01  5.851128e-03  \n",
       "1283  4.055885e-01  0.738170  9.140029e-01  9.546053e-01  \n",
       "1284  2.072351e-01  0.999488  9.301651e-01  9.967242e-01  \n",
       "1285  9.996724e-01  0.510108  9.332535e-01  5.779290e-06  \n",
       "1286  5.493162e-01  0.338772  5.859932e-01  3.287783e-01  \n",
       "1287  9.998955e-01  0.112548  1.571351e-02  1.980414e-04  \n",
       "1288  5.954945e-04  0.999694  9.915132e-01  9.998317e-01  \n",
       "1289  9.994205e-01  0.999978  9.962714e-01  5.054965e-03  \n",
       "1290  9.995166e-01  0.644687  9.866655e-01  2.778176e-05  \n",
       "1291  1.061184e-04  0.854558  1.021122e-09  4.038845e-10  \n",
       "1292  3.246557e-01  0.020535  9.295815e-01  2.493776e-01  \n",
       "1293  9.948124e-01  0.000095  2.918106e-01  3.130289e-05  \n",
       "1294  1.582589e-03  0.928795  4.982027e-02  9.990647e-01  \n",
       "1295  9.586984e-05  0.225162  9.103803e-01  1.347866e-04  \n",
       "1296  9.998993e-01  0.979211  9.688367e-01  2.240791e-06  \n",
       "\n",
       "[1297 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NNMF to select best 20 genes using clustering over the feature matrix.\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#We partition b to two matrices. H will be the 10 special location characteristics of each in-situ gene (we assume\n",
    "# that b contains some hidden location information).\n",
    "model = NMF(n_components=10)\n",
    "#b.shape is (rows, columns) = (3039, 84)\n",
    "W = model.fit_transform(b)\n",
    "H = model.components_\n",
    "\n",
    "print(H.shape)\n",
    "print(np.shape(np.matmul(W,H)))\n",
    "print(np.matmul(W,H)[0])\n",
    "print(b.iloc[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "#We transpose H in order to get 84 samples (rows), and we use clustering to find the most dominant ones.\n",
    "kmeans = KMeans(n_clusters=20).fit(H.T)\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, H.T)\n",
    "print(closest)\n",
    "print(b.columns[closest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glist_20 = [3,16,80,77,19,52,53,57,78,68,62,0,75,21,66,26,81,51,63,7]\n",
    "glist_60 = [3,16,80,77,19,52,53,57,78,68,62,0,75,21,66,26,81,51,63,7,8,56,35,18,83,6,1,61,65,55,74,22,64,20,59,23,79,48,58,31,69,73,76,24,33,17,47,14,25,15,67,42,54,46,50,28,27,49,43,13]\n",
    "glist_20_knn_ver_1 = [39,64,56,30,3,70,79,14,27,67,16,59,73,19,44,49,83,24,40,35]\n",
    "glist_20_knn_ver_2 = [60,8,38,26,16,79,65,43,83,76,73,2,24,17,3,48,82,36,30,78]\n",
    "print(b.columns[glist_20_knn_ver_2])\n",
    "print(b.columns[list(set(glist_60) & set(glist_20_knn_ver_2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model check on other samples as requested by Avner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 31 22:29:24 2018 Selecting samples from d\n",
      "Wed Oct 31 22:29:24 2018 len(d_list): 100000\n"
     ]
    }
   ],
   "source": [
    "#Select 500000 samples from d.\n",
    "print(time.ctime(), 'Selecting samples from d')\n",
    "indicies = random.sample(range(len(d_false)), 100000)\n",
    "d_list = [d_false[i] for i in indicies]\n",
    "random.shuffle(d_list)\n",
    "len_list = len(d_list)\n",
    "print(time.ctime(), f'len(d_list): {len_list}')\n",
    "\n",
    "# Now need to run cell 'Create train input arrays'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 40s 405us/step\n",
      "Score: 0.31402943283319473, acc: 0.8329299947023392\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#model = load_model('c:\\\\data\\\\Dream\\\\2-84_genes_new_model\\\\model_sav.h5')\n",
    "score, acc = model.evaluate(x=[X1_train, X2_train, X3_train], y=y_train, batch_size=50)\n",
    "print(f'Score: {score}, acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxMCC handle cases with more than ten indices, use ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle rows with more than 10 indices: build data for prediction using ANN.\n",
    "num_situ = 60\n",
    "\n",
    "#Create the pairs. Left: d-array, right: b-array \n",
    "#Labels start from 0 in the original file. They indicate a specific row in b table.\n",
    "print(time.ctime(),'Create list of tuples')\n",
    "#labels.pkl contains a dictionary mapping of all 1270 cells to (possibly few) locations in [0,3038].\n",
    "pkl_file = open(f'data/labels_using_maxcc_{num_situ}.pkl', 'rb')\n",
    "ind_load = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "data_ind = pd.DataFrame(list(ind_load.items()))\n",
    "data_ind.drop([0], axis=1, inplace=True)\n",
    "data_ind[1] = [np.ndarray.flatten(data_ind[1][i]) for i in range(len(data_ind))]\n",
    "model = load_model('data/models/weights-improvement60-102-0.85.hdf5')\n",
    "\n",
    "loop = 0\n",
    "d_true = pd.DataFrame()\n",
    "#i is index in dge.\n",
    "for i in range(len(data_ind)):\n",
    "    if(loop%100 == 0):\n",
    "        print(loop, ' ', end=\"\")\n",
    "    loop = loop + 1\n",
    "    one_row = pd.DataFrame()\n",
    "    #j is index in bdtnp\n",
    "    for j in data_ind.iloc[i].iloc[0]:\n",
    "        pred = model.predict([bdtnp.iloc[j][glist_60_tom][np.newaxis,:],\n",
    "                              d.iloc[i][glist_60_tom][np.newaxis,:],\n",
    "                              d.iloc[i][np.newaxis,:]], batch_size=1)\n",
    "        one_row = pd.concat([one_row, pd.DataFrame([i,j,pred[0][0]]).T])\n",
    "    one_row.columns=['i', 'j', 'pred']\n",
    "    one_row = one_row.nlargest(10, 'pred')\n",
    "    one_row_df = pd.DataFrame(one_row.j).T.reset_index(drop=True)\n",
    "    d_true = pd.concat([d_true, one_row_df])\n",
    "\n",
    "d_true.to_csv('data/maxmcc_10_selected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
